{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to My Notes","text":"<p>This is a collection of personal notes and documentation organized by topics.</p> <p>\u300c\u4e00\u5e74\u592a\u957f\uff0c\u5341\u4e8c\u4e2a\u6708\u592a\u77ed\u300d</p>"},{"location":"#navigation","title":"Navigation","text":"<p>Use the sidebar to navigate through different topics and notes. Feel free to explore the content using the search functionality.</p>"},{"location":"cpp/async-cuda/","title":"Asynchronous CUDA Programming","text":""},{"location":"cpp/async-cuda/#asynchronous-barrier","title":"Asynchronous Barrier","text":"<p>10.26 Asynchronous Barrier</p>"},{"location":"cpp/async-cuda/#simple-synchronization-pattern","title":"Simple Synchronization Pattern","text":"<pre><code>#include &lt;cooperative_groups.h&gt;\n\n__global__ void simple_sync(int iteration_count) {\n    auto block = cooperative_groups::this_thread_block();\n\n    for (int i = 0; i &lt; iteration_count; ++i) {\n        /* code before arrive */\n        block.sync(); /* wait for all threads to arrive here */\n        /* code after wait */\n    }\n}\n</code></pre> <p>\u8fd9\u91cc\u7684 <code>block</code> \u7684\u8303\u56f4\u5c31\u662f\u4e00\u822c\u610f\u4e49\u4e0a\u7684 block\uff0c<code>block.sync()</code> \u7b49\u4ef7\u4e8e <code>__syncthreads()</code>\u3002</p>"},{"location":"cpp/async-cuda/#temporal-splitting-and-five-stages-of-synchronization","title":"Temporal Splitting and Five Stages of Synchronization","text":"<pre><code>#include &lt;cuda/barrier&gt;\n#include &lt;cooperative_groups.h&gt;\n\n__device__ void compute(float* data, int curr_iteration);\n\n__global__ void split_arrive_wait(int iteration_count, float *data) {\n    using barrier = cuda::barrier&lt;cuda::thread_scope_block&gt;;\n    __shared__  barrier bar;\n    auto block = cooperative_groups::this_thread_block();\n\n    if (block.thread_rank() == 0) {\n        init(&amp;bar, block.size()); // Initialize the barrier with expected arrival count\n    }\n    block.sync();\n\n    for (int curr_iter = 0; curr_iter &lt; iteration_count; ++curr_iter) {\n        /* code before arrive */\n       barrier::arrival_token token = bar.arrive(); /* this thread arrives. Arrival does not block a thread */\n       compute(data, curr_iter);\n       bar.wait(std::move(token)); /* wait for all threads participating in the barrier to complete bar.arrive()*/\n        /* code after wait */\n    }\n}\n</code></pre> <p>\u8fd9\u91cc\u7684\u540c\u6b65\u64cd\u4f5c\u5206\u6210\u4e86\u4e24\u6b65\uff1a<code>bar.arrive()</code> \u548c <code>bar.wait()</code>\u3002\u5728 <code>bar.arrive()</code> \u4e4b\u524d\u7684\u5185\u5b58\u66f4\u65b0\u80fd\u786e\u4fdd\u88ab <code>bar.wait()</code> \u4e4b\u540e\u7684\u4ee3\u7801\u770b\u89c1\u3002 \u53ef\u4ee5\u7406\u89e3\u4e3a\uff0c<code>bar.wait()</code> \u4f1a\u4e00\u76f4\u963b\u585e\u5f53\u524d\u7ebf\u7a0b\uff0c\u76f4\u5230 <code>block</code> \u5185\u6240\u6709\u7ebf\u7a0b\uff08<code>block.size</code> \u4e2a\u7ebf\u7a0b\uff09\u5230\u8fbe <code>bar.arrive()</code>\u3002</p>"},{"location":"cpp/crtp/","title":"Curiously Recurring Template Pattern (CRTP)","text":"<p>CRTP\uff1a\u57fa\u7c7b\u6a21\u677f\u4ee5\u6d3e\u751f\u7c7b\u4f5c\u4e3a\u6a21\u677f\u53c2\u6570\uff0c\u4ece\u800c\u5728\u7f16\u8bd1\u671f\u5b9e\u73b0\u9759\u6001\u591a\u6001\uff08static polymorphism\uff09\u3002</p>"},{"location":"cpp/crtp/#_1","title":"\u5b9e\u73b0\u9759\u6001\u591a\u6001","text":"<p>CRTP \u901a\u8fc7\u5728\u57fa\u7c7b\u516c\u5f00\u63a5\u53e3\uff0c\u5e76\u5728\u6d3e\u751f\u7c7b\u4e2d\u5b9e\u73b0\u8be5\u63a5\u53e3\uff0c\u4ece\u800c\u5728\u7f16\u8bd1\u671f\u7ed1\u5b9a\u51fd\u6570\u8c03\u7528\uff0c\u5b9e\u73b0\u9759\u6001\u591a\u6001\uff0c\u907f\u514d\u865a\u51fd\u6570\u7684\u8fd0\u884c\u65f6\u5f00\u9500\u3002</p> <pre><code>template&lt;typename Derived&gt;\nclass Shape {\npublic:\n    float area() const {\n        return static_cast&lt;const Derived*&gt;(this)-&gt;area();\n    }\n};\n\nclass Circle : public Shape&lt;Circle&gt; {\npublic:\n    Circle(float radius) : radius(radius) {}\n    float area() const {\n        return 3.14159f * radius * radius;\n    }\nprivate:\n    float radius;\n};\n\nclass Square : public Shape&lt;Square&gt; {\npublic:\n    Square(float side) : side(side) {}\n    float area() const {\n        return side * side;\n    }\nprivate:\n    float side;\n};\n\n// \u4f7f\u7528\uff1a\nCircle c(5.0f);\nSquare s(4.0f);\nfloat circleArea = c.area(); // 78.5398\nfloat squareArea = s.area(); // 16.0\n</code></pre>"},{"location":"cpp/crtp/#_2","title":"\u81ea\u5b9a\u4e49\u6d3e\u751f\u7c7b\u7684\u884c\u4e3a","text":"<p>CRTP \u53ef\u4ee5\u7ed9\u57fa\u7c7b\u63d0\u4f9b\u901a\u7528\u63a5\u53e3\uff0c\u6d3e\u751f\u7c7b\u5b9e\u73b0\u5177\u4f53\u884c\u4e3a\u3002\u53ef\u4ee5\u628a\u6d3e\u751f\u7c7b\u7684\u516c\u5171\u903b\u8f91\u653e\u5728\u57fa\u7c7b\u4e2d\uff0c\u6d3e\u751f\u7c7b\u53ea\u9700\u5b9e\u73b0\u7279\u5b9a\u884c\u4e3a\uff0c\u51cf\u5c11\u4ee3\u7801\u91cd\u590d\u3002</p> <pre><code>template &lt;typename Derived&gt;\nclass EqualityComparable {\npublic:\n    bool operator==(const Derived&amp; other) const {\n        const Derived&amp; self = static_cast&lt;const Derived&amp;&gt;(*this);\n        return self.is_equal(other);\n    }\n\n    bool operator!=(const Derived&amp; other) const {\n        return !(*this == other);\n    }\n};\n\nclass MyClass : public EqualityComparable&lt;MyClass&gt; {\n    int value;\npublic:\n    MyClass(int v) : value(v) {}\n\n    bool is_equal(const MyClass&amp; other) const {\n        return value == other.value;\n    }\n};\n\n// \u4f7f\u7528\uff1a\nMyClass a(1), b(2);\nif (a == b) { ... } // \u81ea\u52a8\u53ef\u7528\uff01\n</code></pre> <ol> <li>\u5b9e\u73b0\u7f16\u8bd1\u671f\u9759\u6001\u63a5\u53e3\u7ea6\u675f\uff0c\u7c7b\u4f3c C++20 concepts\uff0c\u7528\u4e8e\u8981\u6c42\u6d3e\u751f\u7c7b\u5fc5\u987b\u5b9e\u73b0\u67d0\u4e2a\u51fd\u6570</li> </ol> <pre><code>template &lt;typename Derived&gt;\nclass Drawable {\npublic:\n    void draw() {\n        static_cast&lt;Derived*&gt;(this)-&gt;do_draw();\n    }\n};\n// \u5982\u679c Derived \u6ca1\u6709 do_draw()\uff0c\u7f16\u8bd1\u65f6\u62a5\u9519\n</code></pre> <p>\u5728 C++20 \u540e\u7684\u7248\u672c\u4e2d\uff0c\u53ef\u4ee5\u4f7f\u7528 concepts \u6765\u5b9e\u73b0\u7c7b\u4f3c\u7684\u529f\u80fd\uff1a</p> <pre><code>template &lt;typename T&gt;\nconcept DrawableConcept = requires(T t) {\n    { t.do_draw() };\n};\n\ntemplate &lt;DrawableConcept T&gt;\nclass Drawable {\npublic:\n    void draw() {\n        static_cast&lt;T*&gt;(this)-&gt;do_draw();\n    }\n};\n</code></pre>"},{"location":"cpp/crtp/#_3","title":"\u6ce8\u610f\u4e8b\u9879","text":"<ul> <li>CRTP \u9700\u8981\u6d3e\u751f\u7c7b\u5728\u7f16\u8bd1\u671f\u5df2\u77e5\uff0c\u56e0\u6b64\u4e0d\u80fd\u7528\u4e8e\u8fd0\u884c\u65f6\u591a\u6001\uff1a\u4e0d\u80fd\u4f7f\u7528\u6307\u9488\u6216\u5f15\u7528\u6765\u5b58\u50a8\u57fa\u7c7b\u7c7b\u578b\u3002</li> </ul>"},{"location":"cpp/crtp/#_4","title":"\u7528\u4f8b","text":"<p><code>std::enable_shared_from_this</code> \u662f CRTP \u7684\u4e00\u4e2a\u5e38\u89c1\u7528\u4f8b\uff0c\u5b83\u5141\u8bb8\u7c7b\u5b89\u5168\u5730\u4ece <code>shared_from_this</code> \u83b7\u53d6\u81ea\u8eab\u7684 <code>shared_ptr</code>\u3002</p> <pre><code>class Widget: public std::enable_shared_from_this&lt;Widget&gt; {\npublic:\n\u2026\nvoid process();\n\u2026\n}\n</code></pre> <p>\u7b80\u5316\u7248\u5b9e\u73b0\uff1a</p> <pre><code>template &lt;typename T&gt;\nclass enable_shared_from_this {\nprotected:\n    weak_ptr&lt;T&gt; __weak_this; // \u5185\u90e8\u4fdd\u5b58\u4e00\u4e2a\u5f31\u5f15\u7528\uff0c\u6307\u5411\u7ba1\u7406 this \u7684 shared_ptr\n\npublic:\n    shared_ptr&lt;T&gt; shared_from_this() {\n        return shared_ptr&lt;T&gt;(__weak_this); // \u4ece\u5f31\u5f15\u7528\u5347\u7ea7\u4e3a shared_ptr\n    }\n};\n</code></pre> <p>\u5f53\u5bf9\u8c61\u88ab\u7b2c\u4e00\u4e2a <code>shared_ptr</code> \u6784\u9020\u65f6\uff0c\u6807\u51c6\u5e93\u4f1a\u68c0\u6d4b\u5b83\u662f\u5426\u7ee7\u627f\u81ea<code>enable_shared_from_this&lt;T&gt;</code>\uff0c\u5982\u679c\u662f\uff0c\u5c31\u81ea\u52a8\u8bbe\u7f6e <code>__weak_this</code> \u6307\u5411\u8fd9\u4e2a\u63a7\u5236\u5757\u3002</p>"},{"location":"fire/theory/","title":"Initialization","text":""},{"location":"fire/theory/#permanent-portfolio","title":"Permanent Portfolio","text":"<p>\u6c38\u4e45\u6295\u8d44\u7ec4\u5408\u914d\u7f6e 4 \u79cd\u8d44\u4ea7\uff1a</p> <ul> <li>25% \u80a1\u7968</li> <li>25% \u503a\u5238</li> <li>25% \u73b0\u91d1</li> <li>25% \u9ec4\u91d1</li> </ul> <p>\u901a\u8fc7\u5206\u6563\u5316\u8d44\u4ea7\u7c7b\u522b\u5e94\u5bf9\u4e0d\u540c\u7ecf\u6d4e\u72b6\u51b5\u3002\u88ab\u52a8\u7ba1\u7406\u800c\u975e\u79ef\u6781\u7ba1\u7406\u3002</p>"},{"location":"fire/theory/#4","title":"4 \u79cd\u7ecf\u6d4e\u72b6\u51b5","text":"<ol> <li>\u7e41\u8363\uff1a\u751f\u4ea7\u529b\u4e0e\u6536\u76ca\u63d0\u9ad8\uff0c\u4f4e\u5931\u4e1a\u7387\uff0c\u7a33\u5b9a\u6216\u4e0b\u8dcc\u7684\u5229\u7387\u3002\u5bf9\u7ecf\u6d4e\u72b6\u51b5\u6301\u4e50\u89c2\u6001\u5ea6\uff0c\u80a1\u7968\u8d70\u52bf\u826f\u597d\u3002\u503a\u5238\u8868\u73b0\u4e5f\u4e0d\u9519\uff0c\u4f46\u662f\u4e0d\u5982\u80a1\u7968\u3002\u4e0d\u5229\u597d\u6ca1\u6709\u5229\u606f\u4e0e\u5206\u7ea2\u7684\u9ec4\u91d1\u3002</li> <li>\u901a\u8d27\u7d27\u7f29\uff1a\u4ef7\u683c\u4e0b\u964d\uff0c\u5229\u7387\u4e0b\u8dcc\uff0c\u8d27\u5e01\u4ef7\u503c\u4e0a\u6da8\u3002\u5229\u7387\u4e0b\u8dcc\u5229\u597d\u957f\u671f\u503a\u5238\u3002\u7531\u4e8e\u73b0\u91d1\u8d2d\u4e70\u529b\u589e\u52a0\uff0c\u73b0\u91d1\u7c7b\u8d44\u4ea7\u8868\u73b0\u826f\u597d\u3002\u4ef7\u683c\u4e0b\u964d\u4f7f\u516c\u53f8\u5229\u6da6\u4e0b\u8dcc\uff0c\u80a1\u7968\u8868\u73b0\u7cdf\u7cd5\u3002\u800c\u9ec4\u91d1\u901a\u5e38\u8868\u73b0\u4e0d\u592a\u597d\u3002</li> <li>\u8870\u9000\uff1a\u5728\u6c38\u4e45\u6295\u8d44\u7ec4\u5408\u7684\u8bed\u5883\u4e0b\uff0c\u8870\u9000\u6307\u7684\u662f\u94f6\u6839\u7d27\u7f29\u8870\u9000\u671f\u3002\u5728\u94f6\u6839\u7f29\u7d27\u8870\u9000\u671f\uff0c\u592e\u884c\u63d0\u9ad8\u5229\u7387\u6765\u964d\u4f4e\u9ad8\u901a\u80c0\uff0c\u5e26\u6765\u7684\u4e0d\u826f\u540e\u679c\u5c31\u662f\u7ecf\u6d4e\u8870\u9000\u3002\u80a1\u7968\u4e0e\u503a\u5238\u8868\u73b0\u4e0d\u4f73\u3002\u9ec4\u91d1\u901a\u5e38\u8868\u73b0\u4e5f\u4e0d\u592a\u597d\u3002\u6b64\u65f6\u73b0\u91d1\u53ef\u4ee5\u7528\u6765\u8d2d\u5165\u4ef7\u683c\u8f83\u4f4e\u7684\u5176\u4ed6\u8d44\u4ea7\u3002</li> <li>\u901a\u8d27\u81a8\u80c0\uff1a\u8d27\u5e01\u4ef7\u503c\u4e0b\u8dcc\uff0c\u4ef7\u683c\u4e0a\u6da8\uff0c\u4f34\u968f\u5229\u7387\u4e0a\u6da8\u3002\u9ad8\u901a\u80c0\u4f7f\u8d44\u91d1\u8d2c\u503c\uff0c\u9ec4\u91d1\u53ef\u4ee5\u5bf9\u8d44\u4ea7\u52a0\u4ee5\u4fdd\u62a4\u3002</li> </ol>"},{"location":"fire/theory/#_1","title":"\u80a1\u7968","text":"<ul> <li>\u6807\u666e500 / \u7eb3\u6307100</li> <li>\u6052\u751f\u79d1\u6280\u6307\u6570</li> <li>\u6caa\u6df1300</li> </ul>"},{"location":"fire/theory/#_2","title":"\u503a\u5238","text":"<ul> <li>30\u5e74\u56fd\u503aETF</li> </ul>"},{"location":"fire/theory/#_3","title":"\u73b0\u91d1","text":"<ul> <li>\u77ed\u503a\uff08\u4f59\u989d\u5b9d\uff09</li> </ul>"},{"location":"fire/theory/#_4","title":"\u9ec4\u91d1","text":"<ul> <li>\u9ec4\u91d1ETF</li> </ul>"},{"location":"fire/theory/#target","title":"Target","text":"<ul> <li>20% \u6807\u666e500</li> <li>20% \u4e2d\u6e2f\u80a1\u6307</li> <li>20% 30\u5e74\u56fd\u503a</li> <li>20% \u9ec4\u91d1</li> <li>20% \u73b0\u91d1</li> </ul>"},{"location":"notes/flash-attention/","title":"From Online Softmax to Flash Attention","text":""},{"location":"notes/flash-attention/#online-softmax","title":"Online Softmax","text":""},{"location":"notes/flash-attention/#3-pass-safe-softmax","title":"3-Pass Safe Softmax","text":"<p>Softmax \u7684\u6807\u51c6\u5b9a\u4e49\u662f $$ y_i = \\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)} $$ \u4f46\u662f\u5f53 \\(x_i\\) \u5f88\u5927\u65f6\uff0c\\(\\exp(x_i)\\) \u4f1a\u6ea2\u51fa\u3002\u4e3a\u4e86\u907f\u514d\u8fd9\u79cd\u60c5\u51b5\uff0c\u901a\u5e38\u4f1a\u5148\u8ba1\u7b97\u8f93\u5165\u7684\u6700\u5927\u503c \\(m = \\max_i (x_i)\\)\uff0c\u7136\u540e\u4f7f\u7528 $$ y_i = \\frac{\\exp(x_i - m)}{\\sum_j \\exp(x_j - m)} $$ \u6765\u8ba1\u7b97 softmax\uff0c\u786e\u4fdd \\(x_i - m \\le 0\\)\uff0c\u4ece\u800c\u907f\u514d\u6ea2\u51fa\u3002 \u6734\u7d20\u7684\u5b9e\u73b0\u8fd9\u79cd\u65b9\u6cd5\u9700\u8981\u904d\u5386\u4e09\u6b21\u8f93\u5165\uff1a</p> <ol> <li>\u8ba1\u7b97\u6700\u5927\u503c \\(m_i = \\max(m_{i-1}, x_i)\\)\u3002</li> <li>\u8ba1\u7b97\u5f52\u4e00\u5316\u56e0\u5b50 \\(d_i = d_{i-1} + \\exp(x_j - m_N)\\)\u3002</li> <li>\u8ba1\u7b97\u6700\u7ec8\u7684 softmax \u8f93\u51fa \\(y_i = \\exp(x_i - m_N) / d_N\\)\u3002</li> </ol>"},{"location":"notes/flash-attention/#2-pass-safe-online-softmax","title":"2-Pass Safe Online Softmax","text":"<p>\u6211\u4eec\u53ef\u4ee5\u628a\u524d\u4e24\u6b21\u904d\u5386\u878d\u5408\u6210\u4e00\u6b21\uff0c\u5728\u4e00\u6b21\u904d\u5386\u4e2d\u540c\u65f6\u66f4\u65b0\u6700\u5927\u503c\u548c\u5f52\u4e00\u5316\u56e0\u5b50\uff0c\u5b9e\u73b0 2-Pass Safe Softmax\uff1a</p> <ol> <li>\u8ba1\u7b97\u6700\u5927\u503c\u548c\u5f52\u4e00\u5316\u56e0\u5b50<ul> <li>\\(m_i = \\max(m_{i-1}, x_i)\\)</li> <li>\\(d_i = d_{i-1} \\cdot \\exp(m_{i-1} - m_i) + \\exp(x_i - m_i)\\)</li> </ul> </li> <li>\u8ba1\u7b97\u6700\u7ec8\u7684 softmax \u8f93\u51fa \\(y_i = \\exp(x_i - m_N) / d_N\\)\u3002</li> </ol> <p>\u628a 3-Pass \u4f18\u5316\u6210 2-Pass \u6709\u4ec0\u4e48\u6536\u76ca\u5417\uff1f\u4ece\u8ba1\u7b97\u91cf\u4e0a\u770b\uff0c2-Pass \u751a\u81f3\u8fd8\u8981\u6bd4 3-Pass \u591a\u4e00\u4e9b\u8ba1\u7b97\u3002\u4f46\u4ece\u5168\u5c40\u5185\u5b58\u8bbf\u95ee\u7684\u89d2\u5ea6\u6765\u770b\uff0c2-Pass \u53ea\u9700\u8981\u904d\u5386\u4e24\u6b21\u8f93\u5165\uff0c\u800c 3-Pass \u9700\u8981\u904d\u5386\u4e09\u6b21\uff0c\u51cf\u5c11\u4e86\u4e00\u6b21\u5185\u5b58\u8bbf\u95ee\u3002\u5bf9\u4e8e softmax \u8fd9\u79cd memory-bound \u7684\u64cd\u4f5c\u6765\u8bf4\uff0c\u51cf\u5c11\u5185\u5b58\u8bbf\u95ee\u5f80\u5f80\u80fd\u5e26\u6765\u66f4\u5927\u7684\u6027\u80fd\u63d0\u5347\u3002</p>"},{"location":"notes/flash-attention/#flashattention-v1","title":"FlashAttention V1","text":""},{"location":"notes/flash-attention/#1-pass-attention","title":"1-Pass Attention","text":"<p>\u5ffd\u7565\u7f29\u653e\u5e38\u6570 \\(\\sqrt{d}\\)\uff0c\u6807\u51c6 Attention \u7684\u8ba1\u7b97\u516c\u5f0f\u662f $$ O = \\text{Softmax}(QK^T)V $$ \u5176\u4e2d \\(Q, K, V\\) \u7684\u5f62\u72b6\u5206\u522b\u662f \\((M, D)\\)\uff0c\\((N, D)\\)\uff0c\\((N, D)\\)\u3002 \u90a3\u4e48\u5728 2-Pass Softmax \u7684\u57fa\u7840\u4e0a\uff0c\u6bcf\u4e00\u884c \\(O_i\\) \u7684\u8ba1\u7b97\u53ef\u4ee5\u5206\u89e3\u6210\u4ee5\u4e0b\u4e24\u6b65\uff1a</p> <ol> <li>\u8ba1\u7b97 \\(QK^T\\) \u7684\u6700\u5927\u503c\u548c\u5f52\u4e00\u5316\u56e0\u5b50<ul> <li>\\(x_j = Q_i K_j^T\\)</li> <li>\\(m_j = \\max(m_{j-1}, x_j)\\)</li> <li>\\(d_j = d_j \\cdot \\exp(m_{j-1} - m_j) + \\exp(x_j - m_j)\\)</li> </ul> </li> <li>\u8ba1\u7b97 \\(O\\)<ul> <li>\\(O_j = O_{j-1} + \\frac{\\exp(x_j - m_D)}{d_N} V_j\\)</li> </ul> </li> </ol> <p>\u867d\u7136 Softmax \u4e0d\u80fd\u88ab\u8fdb\u4e00\u6b65\u4f18\u5316\u6210 1-Pass\uff0c\u4f46\u662f Attention \u53ea\u9700\u8981\u5f97\u5230\u6700\u7ec8\u7684\u8f93\u51fa \\(O\\)\uff0c\u800c\u4e0d\u9700\u8981\u4e2d\u95f4\u7684 Softmax \u7ed3\u679c\u3002\u5b9e\u9645\u4e0a Attention \u662f\u53ef\u4ee5\u88ab\u4f18\u5316\u6210 1-Pass \u7684\u3002 \u5b9a\u4e49 \\(O'_j\\)</p> \\[ O'_j = \\sum_{k=1}^{j} \\frac{\\exp(x_k-m_j)}{d_j} V_k \\] <p>\u53ef\u4ee5\u5f97\u5230\u9012\u63a8\u5f0f\uff1a</p> \\[ O'_j = O'_{j-1} \\cdot \\exp(m_{j-1}-m_j) \\cdot \\frac{d_{j-1}}{d_j} + \\frac{\\exp(x_j - m_j)}{d_j} V_j \\] <p>\u7531\u6b64\uff0c\u6211\u4eec\u53ef\u4ee5\u5728\u4e00\u6b21\u904d\u5386\u4e2d\u540c\u65f6\u8ba1\u7b97 \\(m_j, d_j, O'_j\\)\uff0c\u4ece\u800c\u5b9e\u73b0 1-Pass Attention\uff1a</p> <ol> <li>\u8ba1\u7b97 \\(QK^T\\) \u7684\u6700\u5927\u503c\u3001\u5f52\u4e00\u5316\u56e0\u5b50\u548c\u8f93\u51fa<ul> <li>\\(x_j = Q_i K_j^T\\)</li> <li>\\(m_j = \\max(m_{j-1}, x_j)\\)</li> <li>\\(d_j = d_j \\cdot \\exp(m_{j-1} - m_j) + \\exp(x_j - m_j)\\)</li> <li>\\(O'_j = O'_{j-1} \\cdot \\exp(m_{j-1}-m_j) \\cdot \\frac{d_{j-1}}{d_j} + \\frac{\\exp(x_j - m_j)}{d_j} V_j\\)</li> </ul> </li> </ol>"},{"location":"notes/flash-attention/#tiling","title":"Tiling","text":"<p>\u6211\u4eec\u628a K \u548c V \u5206\u5757\uff0c\u6bcf\u5757\u5927\u5c0f\u4e3a \\((n, D)\\)\uff0c\u518d\u628a Q \u6bcf\u5757\u5206\u4e3a \\((m, D)\\)\u3002 \u6211\u4eec\u5148\u5206\u6790\u5bf9\u4e8e \\(Q_i, K_j, V_j\\) \u5206\u5757\u7684\u8ba1\u7b97\u8fc7\u7a0b\uff1a</p> <ol> <li>\u8ba1\u7b97 \\(QK^T\\)<ul> <li>\\(X = Q_i K_j^T \\in \\R^{(m,n)}\\)</li> </ul> </li> <li>\u5bf9\u6bcf\u884c\u6c42<ul> <li>\\(\\tilde{m_i}[k] = \\max_l(X_{k,l}) \\in \\R^m\\)</li> <li>\\(P_{k,l} = \\exp(X_{k,l} - \\tilde{m_i}) \\in \\R^{(m,n)}\\)</li> <li>\\(d_i[k] = \\sum_l P_{k,l} \\in \\R^m\\)</li> </ul> </li> <li>\u5bf9\u6bcf\u884c\u66f4\u65b0\u6700\u5927\u503c\u548c\u5f52\u4e00\u5316\u56e0\u5b50<ul> <li>\\(m_i[k] = \\max(m'_i[k], \\tilde{m_i}[k])\\)</li> <li>\\(d_i[k] = d'_i[k] \\cdot \\exp(m'_i[k] - m_i[k]) + d_i[k] \\cdot \\exp(\\tilde{m_i}[k] - m_i[k])\\)</li> </ul> </li> <li>\u5bf9\u6bcf\u884c\u8ba1\u7b97\u8f93\u51fa<ul> <li>$O_i[k] = d_i^{-1} ((d'_i \\cdot \\exp(m'_i - m_i)) \\cdot O_i[k] + \\exp(\\tilde{m_i} - m_i) \\cdot PV) $</li> </ul> </li> <li>\u7ed9\u4e0b\u4e2a\u5757\u66f4\u65b0\u8fd9\u4e2a\u5757\u7684\u6700\u5927\u503c\u548c\u5f52\u4e00\u5316\u56e0\u5b50<ul> <li>\\(m'_i = m\\)</li> <li>\\(d'_i = d\\)</li> </ul> </li> </ol> <p>FlashAttention V1 \u628a K \u548c V \u7684\u5206\u5757\u904d\u5386\u653e\u5728\u5916\u5c42\u5faa\u73af\uff0c\u628a Q \u7684\u5206\u5757\u904d\u5386\u653e\u5728\u5185\u5c42\u5faa\u73af\u3002</p>"},{"location":"notes/flash-attention/#flashattention-v2","title":"FlashAttention V2","text":"<ol> <li> <p>\u51cf\u5c11 Cuda Core \u8ba1\u7b97</p> <p>\u5728 V1\uff0c\u6bcf\u4e2a tile \u4e2d \\(O_i\\) \u7684\u8ba1\u7b97\u90fd\u5305\u542b\u4e24\u6b21 rescale \u64cd\u4f5c\uff1a\u4e58 \\(d'\\) \u548c\u9664 \\(d\\)\u3002 V2 \u5728\u6bcf\u4e2a tile \u4e2d\u4ec5\u4fdd\u7559\u4e58 \\(d'\\) \u7684 rescale\uff0c\u628a\u6240\u6709\u7684 \\(d\\) \u7684 rescale \u4e00\u8d77\u653e\u5728\u6700\u540e\u3002</p> </li> <li> <p>\u8c03\u6362\u5faa\u73af\u987a\u5e8f</p> <p>V1 \u5148\u5faa\u73af K \u548c V\uff0c\u540e\u5faa\u73af Q \u7684\u505a\u6cd5\u4f7f\u5f97\u6bcf\u6b21\u5185\u5faa\u73af\u90fd\u9700\u8981\u53cd\u590d\u5411 HBM \u8bbf\u5b58 \\(O_i, m'_i, d'_i\\)\u3002 V2 \u8c03\u6362\u4e86\u5185\u5916\u5faa\u73af\u7684\u987a\u5e8f\uff0c\u5148\u5faa\u73af Q\uff0c\u518d\u5faa\u73af K \u548c V\uff0c\u53ea\u5728\u6bcf\u6b21\u5916\u5faa\u73af\u4e2d\u5411 HBM \u8bbf\u5b58 \\(O_i, m'_i, d'_i\\)\u3002</p> </li> <li> <p>\u589e\u52a0 Sequence Length \u7ef4\u5ea6\u5e76\u884c</p> <p>V1 \u53ea\u5728 Batch Size \u548c Head Num \u7ef4\u5ea6\u4e0a\u5e76\u884c\uff0cV2 \u589e\u52a0\u4e86 Sequence Length \u7ef4\u5ea6\u7684\u5e76\u884c</p> </li> </ol>"},{"location":"notes/flash-attention/#flashattention-v3","title":"FlashAttention V3","text":"<p>TODO</p>"},{"location":"notes/flash-attention/#flashattention-vs-sdpa","title":"FlashAttention vs SDPA","text":"<p>FlashAttention \u662f\u5426\u4e00\u5b9a\u6bd4\u6807\u51c6\u7684 SDPA \u5feb\uff1f\u7b54\u6848\u662f\u5426\u5b9a\u7684\u3002 FlashAttention \u5e0c\u671b\u901a\u8fc7\u51cf\u5c11\u5168\u5c40\u5185\u5b58\u8bbf\u95ee\u6765\u63d0\u5347\u6027\u80fd\uff0cFA1 \u8bba\u6587\u4e2d\u5176\u5b9e\u7ed9\u51fa\u4e86 FlashAttention \u548c SDPA \u8bbf\u5b58\u91cf\u7684\u6e10\u8fdb\u5206\u6790\uff1a</p> Method Memory Access SDPA \\(\\Theta(Nd + N^2)\\) FlashAttention \\(\\Theta(N^2 d^2 M^{-1})\\) <p>\u5176\u4e2d \\(N\\) \u662f <code>seq_len</code>\uff0c\\(d\\) \u662f <code>head_dim</code>\uff0c\\(M\\) \u662f GPU \u7684 SRAM \u5927\u5c0f\u3002 \u5bf9\u4e8e\u4e3b\u6d41\u7684\u6a21\u578b\u548c\u63a8\u7406\u6846\u67b6\uff0c<code>seq_len</code> \u4e00\u822c\u5728\u51e0\u5343\u4ee5\u5185\uff088192\uff09\uff0c<code>head_dim</code> \u662f 64/128\uff0cGPU \u7684 SRAM \u5927\u6982\u662f 200kB\uff08A100: 192kB, H100: 228kB\uff09\u3002 \u8fd9\u65f6 \\(d^2 M^{-1}\\) \u662f\u8fdc\u5c0f\u4e8e 1 \u7684\uff0c\u6240\u4ee5 FlashAttention \u7684\u8bbf\u5b58\u91cf\u8981\u8fdc\u5c0f\u4e8e SDPA\u3002</p> <p>\u4f46\u662f\u5982\u679c <code>head_dim</code> \u6bd4\u8f83\u5927\uff0c\u6bd4\u5982 256 \u6216 512\uff0c\u6216\u8005 SRAM \u6bd4\u8f83\u5c0f\uff0c\u6bd4\u5982\u5728\u65e7\u67b6\u6784\u7684 GPU \u4e0a\u8fd0\u884c\uff0cFlashAttention \u7684\u8bbf\u5b58\u91cf\u53ef\u80fd\u4f1a\u5927\u4e8e SDPA\u3002</p>"},{"location":"notes/flash-attention/#flashattention-v4","title":"FlashAttention V4","text":"<p>Tri Dao \u5df2\u7ecf\u9884\u544a FlashAttention V4 \u4e86\uff0c\u4f46\u8fd8\u672a\u6b63\u5f0f\u53d1\u5e03\u3002</p>"},{"location":"notes/mla/","title":"Multi-head Latent Attention (MLA)","text":""},{"location":"notes/mla/#transformers","title":"Transformers","text":""},{"location":"notes/mla/#deepseekv3-configuration","title":"DeepSeekV3 Configuration","text":"<pre><code>hidden_size = 7168\nq_lora_rank = 1536\nnum_heads   = 128\nqk_nope_head_dim = 128\nqk_rope_head_dim = 64\nqk_head_dim = qk_nope_head_dim + qk_rope_head_dim = 192\nv_head_dim = 128\n</code></pre>"},{"location":"notes/mla/#deepseekv3-attention","title":"DeepSeekV3 Attention","text":"<pre><code>query_shape = (batch_size, seq_length, -1, self.qk_head_dim)\nkey_shape = (batch_size, seq_length, -1, self.qk_nope_head_dim + self.v_head_dim)\n\nif self.q_lora_rank is None:\n    q_states = self.q_proj(hidden_states)  # [b, s, h * qk_head_dim]\nelse:\n    q_states = self.q_b_proj(self.q_a_layernorm(self.q_a_proj(hidden_states)))\n        # q_a_proj: [b, s, d] -&gt; [b, s, q_lora_rank]   (16x down_proj)\n        # q_b_proj: [b, s, q_lora_rank] -&gt; [b, s, h * qk_head_dim]  (16x up_proj)\n\nq_states = q_states.view(query_shape).transpose(1, 2)  # [b, h, s, qk_head_dim]\nq_pass, q_rot = torch.split(q_states, [self.qk_nope_head_dim, self.qk_rope_head_dim], dim=-1)\n    # q_pass: [b, h, s, qk_nope_head_dim]\n    # q_rot:  [b, h, s, qk_rope_head_dim]\n\ncompressed_kv = self.kv_a_proj_with_mqa(hidden_states)  # [b, s, kv_lora_rank + qk_rope_head_dim]\nk_pass, k_rot = torch.split(compressed_kv, [self.kv_lora_rank, self.qk_rope_head_dim], dim=-1)\n    # k_pass: [b, s, kv_lora_rank]\n    # k_rot:  [b, s, qk_rope_head_dim]\n\nk_pass = self.kv_b_proj(self.kv_a_layernorm(k_pass)).view(key_shape).transpose(1, 2)\n    # k_pass: [b, h, s, qk_nope_head_dim + v_head_dim]\nk_pass, value_states = torch.split(k_pass, [self.qk_nope_head_dim, self.v_head_dim], dim=-1)\n    # k_pass:       [b, h, s, qk_nope_head_dim]\n    # value_states: [b, h, s, v_head_dim]\n\nk_rot = k_rot.view(batch_size, 1, seq_length, self.qk_rope_head_dim)  # [b, 1, s, qk_rope_head_dim]\n\ncos, sin = position_embeddings\nif self.config.rope_interleave:  # support using interleaved weights for efficiency\n    q_rot, k_rot = apply_rotary_pos_emb_interleave(q_rot, k_rot, cos, sin)\nelse:\n    q_rot, k_rot = apply_rotary_pos_emb(q_rot, k_rot, cos, sin)\nk_rot = k_rot.expand(*k_pass.shape[:-1], -1)  # [b, h, s, qk_rope_head_dim]\n\nquery_states = torch.cat((q_pass, q_rot), dim=-1)  # [b, h, s, qk_head_dim]\nkey_states = torch.cat((k_pass, k_rot), dim=-1)  # [b, h, s, qk_head_dim]\n\n# Normal attention\nattn_output, attn_weights = attention_interface(\n    self,\n    query_states,\n    key_states,\n    value_states,\n    attention_mask,\n    dropout=0.0 if not self.training else self.attention_dropout,\n    scaling=self.scaling,\n    **kwargs,\n)\n</code></pre> <p>MLA \u7684 KV Cache \u91cc\u5b58\u7684\u662f <code>compressed_kv</code> \u548c <code>k_rot</code>\uff0c\u5927\u5927\u51cf\u5c11\u4e86 KV Cache \u7684\u6240\u9700\u7a7a\u95f4\u3002 \u4ee5 DeepSeekV3 \u4e3a\u4f8b\uff0c\u6bcf\u4e2a layer \u6bcf\u4e2a token \u4ec5\u9700\u4e00\u4e2a\u957f\u5ea6\u4e3a 192 \u7684 Cache\u3002</p>"},{"location":"notes/mla/#_1","title":"\u77e9\u9635\u5438\u6536","text":"<p>\u8fd9\u91cc\u77e9\u9635\u5438\u6536\u5e76\u975e\u76f4\u63a5\u5c06\u4e24\u4e2a\u8fde\u7eed\u7684\u77e9\u9635\u4e58\u5408\u5e76\u6210\u4e00\u4e2a\u77e9\u9635\u4e58\uff08\u8fd9\u6837\u505a\u4e27\u5931\u4e86 LoRA \u7684\u610f\u4e49\uff09\uff0c\u800c\u662f\u4ea4\u6362\u77e9\u9635\u4e58\u7684\u8ba1\u7b97\u987a\u5e8f\u3002</p>"},{"location":"notes/mla/#wuk-w_uq","title":"\u5438\u6536 \\(W^{UK}\\) \u548c \\(W_{UQ}\\)","text":"\\[ q^\\top k = (W^{UQ} c_t^Q)^\\top (W^{UK} c_t^{KV}) = \\left({c^Q}^\\top {W^{UQ}}^\\top W^{UK}\\right) c^{KV} = \\left({W^{UK}}^\\top W^{UQ} c^Q \\right)^\\top c^{KV} \\] <p>\u5176\u4e2d \\(W^{UQ}\\) \u7684\u5f62\u72b6\u662f <code>[h * qk_head_dim (24576), q_lora_rank (1536)]</code>\uff0c \\(W^{UK}\\) \u7684\u5f62\u72b6\u662f <code>[h * qk_head_dim (24576), kv_lora_rank (512)]</code>\u3002</p> <p>\u77e9\u9635\u5438\u6536\u540e\uff0c\u53ef\u4ee5\u76f4\u63a5\u628a \\(c_t^{KV}\\) \u770b\u4f5c\u662f \\(K\\) \u8fdb\u884c Attention \u8ba1\u7b97\uff0c\u800c \\(c^{KV}\\) \u53c8\u662f\u6bcf\u4e2a\u5934\u5171\u7528\u7684\u3002 \u56e0\u6b64\u539f\u5148\u7684 128 heads 128+64 head_dim \u7684 MHA \u8f6c\u5316\u4e3a\u4e86 128 heads 512+64 head_dim \u7684 MQA\uff0c\u51cf\u5c0f\u4e86\u8bbf\u5b58\u91cf\uff0c\u4f46\u589e\u52a0\u4e86\u8ba1\u7b97\u91cf\u3002</p> <p>\u6211\u4eec\u5728 prefill \u65f6\u4f7f\u7528\u8ba1\u7b97\u5f3a\u5ea6\u8f83\u5c0f\uff0c\u8bbf\u5b58\u91cf\u66f4\u5927\u7684 MHA\u3002\u5728 decode \u65f6\u4f7f\u7528\u8ba1\u7b97\u5f3a\u5ea6\u8f83\u5927\uff0c\u8bbf\u5b58\u91cf\u66f4\u5c0f\u7684 MQA\u3002</p>"},{"location":"notes/mla/#wuv-wo","title":"\u5438\u6536 \\(W^{UV}\\) \u548c \\(W^O\\)","text":"<pre><code>v_t = einsum('hdc,blc-&gt;blhd', W_UV, c_t_KV) # (1)\no   = einsum('bqhl,blhd-&gt;bqhd', attn_weights, v_t)     # (2)\nu   = einsum('hdD,bhqd-&gt;bhD', W_o, o)       # (3)\n\n# \u5c06\u4e0a\u8ff0\u4e09\u5f0f\u5408\u5e76\uff0c\u5f97\u5230\u603b\u7684\u8ba1\u7b97\u8fc7\u7a0b\nu   = einsum('hdc,blc,bqhl,hdD-&gt;bhD', W_UV, c_t_KV, attn_weights, W_o)\n\n# \u5229\u7528\u7ed3\u5408\u5f8b\u6539\u53d8\u8ba1\u7b97\u987a\u5e8f\no_  = einsum('bhql,blc-&gt;bhqc', attn_weights, c_t_KV) # (4)\no   = einsum('bhqc,hdc-&gt;bhqd', o_, W_UV)  # (5)\nu   = einsum('hdD,bhqd-&gt;bqD', W_o, o)     # (6)\n</code></pre> <p>https://github.com/flashinfer-ai/flashinfer/pull/551</p> <p>https://zhuanlan.zhihu.com/p/700214123</p>"},{"location":"notes/mla/#rope","title":"RoPE","text":"<p>RoPE \u4f5c\u7528\u5728 \\(c^{KV}\\) \u548c \\(c^Q\\) \u4e0a\uff0c\u4f7f\u5f97 \\(W^{UK}\\) \u548c \\(W_{UQ}\\) \u4e0d\u80fd\u518d\u88ab\u5438\u6536\u3002 MLA \u7684\u65b9\u6848\u662f\u628a K \u5207\u6210\u4e24\u90e8\u5206\uff0c <code>pass</code> \u90e8\u5206\u4e0d\u7ecf\u8fc7 RoPE\uff0c\u4f7f\u4e24\u4e2a\u77e9\u9635\u80fd\u591f\u88ab\u5438\u6536\uff1b\u8ba9 <code>rot</code> \u90e8\u5206\u4e0d\u53c2\u4e0e\u77e9\u9635\u4e58\uff0c\u7ecf\u8fc7 RoPE \u540e\u76f4\u63a5\u4e0e <code>pass</code> \u90e8\u5206 concat\u3002</p>"},{"location":"notes/mla/#mla-in-vllm","title":"MLA in vLLM","text":"<pre><code>class MultiHeadLatentAttention(CustomOp):\n    ...\n\n    def forward_native(\n        self,\n        positions: torch.Tensor,\n        hidden_states: torch.Tensor,\n    ) -&gt; torch.Tensor:\n        q_c = None\n        kv_lora = None\n\n        if self.q_lora_rank is not None:\n            assert self.fused_qkv_a_proj is not None, \\\n                \"fused_qkv_a_proj is required when q_lora_rank is not None\"\n            assert self.q_a_layernorm is not None, \\\n                \"q_a_layernorm is required when q_lora_rank is not None\"\n            assert self.q_b_proj is not None, \\\n                \"q_b_proj is required when q_lora_rank is not None\"\n            qkv_lora = self.fused_qkv_a_proj(hidden_states)[0]\n            q_c, kv_lora = qkv_lora.split(\n                [self.q_lora_rank, self.kv_lora_rank + self.qk_rope_head_dim],\n                dim=-1,\n            )\n            q_c = self.q_a_layernorm(q_c)\n            q = self.q_b_proj(q_c)[0]\n        else:\n            assert self.kv_a_proj_with_mqa is not None, \\\n                \"kv_a_proj_with_mqa is required when q_lora_rank is None\"\n            assert self.q_proj is not None, \\\n                \"q_proj is required when q_lora_rank is None\"\n            kv_lora = self.kv_a_proj_with_mqa(hidden_states)[0]\n            q = self.q_proj(hidden_states)[0]\n\n        kv_c, k_pe = kv_lora.split([self.kv_lora_rank, self.qk_rope_head_dim],\n                                   dim=-1)\n        kv_c_normed = self.kv_a_layernorm(kv_c)\n\n        q = q.view(-1, self.num_heads, self.qk_head_dim)\n        # Add head dim of 1 to k_pe\n        k_pe = k_pe.unsqueeze(1)\n\n        q[..., self.qk_nope_head_dim:], k_pe = self.rotary_emb(\n            positions, q[..., self.qk_nope_head_dim:], k_pe)\n\n        attn_out = self.mla_attn(\n            q,\n            kv_c_normed,\n            k_pe,\n            output_shape=(hidden_states.shape[0],\n                          self.num_heads * self.v_head_dim))\n        return self.o_proj(attn_out)[0]\n</code></pre> <p>vLLM \u628a <code>q_a_proj</code> \u548c <code>kv_a_proj_with_mqa</code> \u4e24\u4e2a\u5bf9 <code>hidden_states</code> \u7684\u77e9\u9635\u4e58\u878d\u5408\u6210 <code>fused_qkv_a_proj</code>\u3002 \u5b9e\u73b0\u77e9\u9635\u5438\u6536\uff0c\u628a <code>kv_c_norm</code> \u4f20\u8fdb <code>mla_attn</code>\u3002 KV cache \u5b58\u653e\u7684\u662f <code>kv_c_normed</code> \u548c <code>k_pe</code>\u3002</p> <p>\u8c03\u7528\u94fe\uff1a</p> <pre><code>DeepseekV2MLAAttention.mla_attn = MultiHeadLatentAttention(...)\n-&gt;\nMultiHeadLatentAttention.mla_attn = Attention(..., use_mla=True, ...)\n-&gt;\nselector.py dispatch attention backend\n</code></pre> <p>\u5728 vLLM v1 \u4e2d\u6709 CutlassMLA, FlashattnMLA, FlashinferMLA, FlashMLA, TritonMLA \u540e\u7aef\uff08for CUDA\uff09\uff0c\u800c v0 \u53ea\u652f\u6301 FlashMLA \u548c Triton \u540e\u7aef\u3002 vLLM \u5b9a\u4e49\u4e86\u4e00\u4e2a\u901a\u7528\u63a5\u53e3 <code>MLACommonImpl</code>\uff0c\u5b9e\u73b0\u4e86 <code>forward</code> \u548c <code>_forward_prefill</code> \u7b49\u65b9\u6cd5\uff0c\u4f46\u662f\u6ca1\u6709\u5b9e\u73b0 <code>_forward_decode</code>\u3002 \u5404\u4e2a\u540e\u7aef\u7684 MLA \u5b9e\u73b0\u90fd\u7ee7\u627f <code>MLACommonImpl</code>\uff0c\u5404\u81ea\u5b9e\u73b0 <code>_forward_decode</code>\u3002</p> <p>\u6211\u4eec\u5173\u6ce8 <code>TritonMLAImpl._forward_decode</code> \u5b9e\u73b0\uff0c\u5176\u8c03\u7528\u4e86 <code>decode_attention_fwd</code>\u3002 \u7531\u4e8e\u77e9\u9635\u5438\u6536\u540e\u7b49\u4ef7\u4e8e MQA\uff0c\u63a5\u7740\u8c03\u7528 <code>decode_attention_fwd_grouped</code>\uff1a</p> <ol> <li><code>_decode_att_m_fwd</code></li> <li><code>_decode_softmax_reducev_fwd</code></li> </ol>"},{"location":"notes/nano-vllm/","title":"Nano-vLLM","text":"<p>https://github.com/GeeeekExplorer/nano-vllm</p>"},{"location":"notes/nano-vllm/#llm","title":"LLM","text":"<p><pre><code>class LLM(LLMEngine):\n    pass\n</code></pre> \u8fd9\u91cc LLM \u7c7b\u76f4\u63a5\u7a7f\u900f\u5230 LLMEngine \u7c7b\u3002 vLLM \u4e2d\u540c\u65f6\u6709\u540c\u6b65\u7684 LLM \u7c7b\u548c\u4e3a\u5f02\u6b65\u7684 AsyncLLMEngine \u7c7b\uff0c\u5b9e\u73b0\u66f4\u590d\u6742\u3002</p>"},{"location":"notes/nano-vllm/#llmengine","title":"LLMEngine","text":"<pre><code>class LLMEngine:\n\n    def __init__(self, model, **kwargs):\n        config_fields = {field.name for field in fields(Config)}\n        config_kwargs = {k: v for k, v in kwargs.items() if k in config_fields}\n        config = Config(model, **config_kwargs)\n        self.ps = []\n        self.events = []\n        ctx = mp.get_context(\"spawn\")\n        for i in range(1, config.tensor_parallel_size):\n            event = ctx.Event()\n            process = ctx.Process(target=ModelRunner, args=(config, i, event))\n            process.start()\n            self.ps.append(process)\n            self.events.append(event)\n        self.model_runner = ModelRunner(config, 0, self.events)\n        self.tokenizer = AutoTokenizer.from_pretrained(config.model, use_fast=True)\n        config.eos = self.tokenizer.eos_token_id\n        self.scheduler = Scheduler(config)\n        atexit.register(self.exit)\n</code></pre> <p>LLMEngine \u4e2d\u5305\u62ec\u4e86 model_runner, tokenizer, scheduler\uff0c\u8fd9\u4e09\u4e2a\u7c7b\u4e4b\u540e\u4f1a\u5c55\u5f00\u5206\u6790\u3002 \u6211\u4eec\u5148\u770b LLMEngine \u5bf9\u5916\u66b4\u9732\u7684\u63a5\u53e3 <code>LLMEngine.generate()</code>\u3002</p> <pre><code>def add_request(self, prompt: str | list[int], sampling_params: SamplingParams):\n    if isinstance(prompt, str):\n        prompt = self.tokenizer.encode(prompt)\n    seq = Sequence(prompt, sampling_params)\n    self.scheduler.add(seq)\n\ndef step(self):\n    seqs, is_prefill = self.scheduler.schedule()\n    token_ids = self.model_runner.call(\"run\", seqs, is_prefill)\n    self.scheduler.postprocess(seqs, token_ids)\n    outputs = [(seq.seq_id, seq.completion_token_ids) for seq in seqs if seq.is_finished]\n    num_tokens = sum(len(seq) for seq in seqs) if is_prefill else -len(seqs)\n    return outputs, num_tokens\n\ndef is_finished(self):\n    return self.scheduler.is_finished()\n\ndef generate(\n    self,\n    prompts: list[str] | list[list[int]],\n    sampling_params: SamplingParams | list[SamplingParams],\n    use_tqdm: bool = True,\n) -&gt; list[str]:\n    if use_tqdm:\n        pbar = tqdm(total=len(prompts), desc=\"Generating\", dynamic_ncols=True)\n    if not isinstance(sampling_params, list):\n        sampling_params = [sampling_params] * len(prompts)\n    for prompt, sp in zip(prompts, sampling_params):\n        self.add_request(prompt, sp)\n    outputs = {}\n    prefill_throughput = decode_throughput = 0.\n    while not self.is_finished():\n        t = perf_counter()\n        output, num_tokens = self.step()\n        if use_tqdm:\n            if num_tokens &gt; 0:\n                prefill_throughput = num_tokens / (perf_counter() - t)\n            else:\n                decode_throughput = -num_tokens / (perf_counter() - t)\n            pbar.set_postfix({\n                \"Prefill\": f\"{int(prefill_throughput)}tok/s\",\n                \"Decode\": f\"{int(decode_throughput)}tok/s\",\n            })\n        for seq_id, token_ids in output:\n            outputs[seq_id] = token_ids\n            if use_tqdm:\n                pbar.update(1)\n    outputs = [outputs[seq_id] for seq_id in sorted(outputs.keys())]\n    outputs = [{\"text\": self.tokenizer.decode(token_ids), \"token_ids\": token_ids} for token_ids in outputs]\n    if use_tqdm:\n        pbar.close()\n    return outputs\n</code></pre> <p>\u9996\u5148\uff0c<code>add_request</code> \u65b9\u6cd5\u4f1a\u5c06 prompt \u4f9d\u6b21\u52a0\u5165 scheduler\u3002 \u968f\u540e\u4e0d\u65ad\u8c03\u7528 <code>step</code> \u65b9\u6cd5\uff0c\u76f4\u5230 scheduler \u4e2d\u6ca1\u6709\u5c1a\u672a\u5b8c\u6210\u7684 sequence\u3002 \u5728 <code>step</code> \u65b9\u6cd5\u4e2d\uff0cscheduler \u51b3\u5b9a\u5f53\u524d\u8fdb\u884c prefill \u8fd8\u662f decode\uff0c\u5e76\u8fd4\u56de\u9700\u8981\u5904\u7406\u7684 sequence\uff0c\u4ea4\u7ed9 model_runner \u5904\u7406\u3002</p>"},{"location":"notes/nano-vllm/#scheduler","title":"Scheduler","text":"<p>\u5728\u6df1\u5165 Scheduler \u4e4b\u524d\uff0c\u6211\u4eec\u5148\u4ecb\u7ecd\u4e00\u4e0b Block \u7c7b\u548c BlockManager \u7c7b\u3002</p>"},{"location":"notes/nano-vllm/#block","title":"Block","text":"<pre><code>class Block:\n\n    def __init__(self, block_id):\n        self.block_id = block_id\n        self.ref_count = 0\n        self.hash = -1\n        self.token_ids = []\n\n    def update(self, hash: int, token_ids: list[int]):\n        self.hash = hash\n        self.token_ids = token_ids\n\n    def reset(self):\n        self.ref_count = 1\n        self.hash = -1\n        self.token_ids = []\n</code></pre> <p>Block \u7c7b\u5728\u521b\u5efa\u65f6\u65f6\u62ff\u5230\u4e00\u4e2a block_id\uff0c\u521d\u59cb\u5316\u5f15\u7528\u8ba1\u6570\uff0c\u54c8\u5e0c\uff0c\u548c\u5b58\u653e\u7684token Id\u3002 update \u65b9\u6cd5\u66f4\u65b0\u54c8\u5e0c\u503c\u548ctoken Id\u3002 reset \u65b9\u6cd5\u5c06\u5f15\u7528\u8ba1\u6570\u8bbe\u62101\uff0c\u54c8\u5e0c\u548c token Id \u90fd\u53d8\u4e3a\u521d\u59cb\u72b6\u6001\u3002</p>"},{"location":"notes/nano-vllm/#blockmanager","title":"BlockManager","text":"<pre><code>class BlockManager:\n\n    def __init__(self, num_blocks: int, block_size: int):\n        self.block_size = block_size\n        self.blocks: list[Block] = [Block(i) for i in range(num_blocks)]\n        self.hash_to_block_id: dict[int, int] = dict()\n        self.free_block_ids: deque[int] = deque(range(num_blocks))\n        self.used_block_ids: set[int] = set()\n</code></pre>"},{"location":"notes/nano-vllm/#allocate","title":"Allocate","text":"<pre><code>def allocate(self, seq: Sequence):\n    assert not seq.block_table\n    h = -1\n    cache_miss = False\n    for i in range(seq.num_blocks):\n        token_ids = seq.block(i)\n        h = self.compute_hash(token_ids, h) if len(token_ids) == self.block_size else -1\n        block_id = self.hash_to_block_id.get(h, -1)\n        if block_id == -1 or self.blocks[block_id].token_ids != token_ids:\n            cache_miss = True\n        if cache_miss:\n            block_id = self.free_block_ids[0]\n            block = self._allocate_block(block_id)\n        else:\n            seq.num_cached_tokens += self.block_size\n            if block_id in self.used_block_ids:\n                block = self.blocks[block_id]\n                block.ref_count += 1\n            else:\n                block = self._allocate_block(block_id)\n        if h != -1:\n            block.update(h, token_ids)\n            self.hash_to_block_id[h] = block_id\n        seq.block_table.append(block_id)\n</code></pre>"}]}