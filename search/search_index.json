{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to My Notes","text":"<p>This is a collection of personal notes and documentation organized by topics.</p> <p>\u300c\u4e00\u5e74\u592a\u957f\uff0c\u5341\u4e8c\u4e2a\u6708\u592a\u77ed\u300d</p>"},{"location":"#navigation","title":"Navigation","text":"<p>Use the sidebar to navigate through different topics and notes. Feel free to explore the content using the search functionality.</p>"},{"location":"cpp/async-cuda/","title":"Asynchronous CUDA Programming","text":""},{"location":"cpp/async-cuda/#asynchronous-barrier","title":"Asynchronous Barrier","text":"<p>10.26 Asynchronous Barrier</p>"},{"location":"cpp/async-cuda/#simple-synchronization-pattern","title":"Simple Synchronization Pattern","text":"<pre><code>#include &lt;cooperative_groups.h&gt;\n\n__global__ void simple_sync(int iteration_count) {\n    auto block = cooperative_groups::this_thread_block();\n\n    for (int i = 0; i &lt; iteration_count; ++i) {\n        /* code before arrive */\n        block.sync(); /* wait for all threads to arrive here */\n        /* code after wait */\n    }\n}\n</code></pre> <p>\u8fd9\u91cc\u7684 <code>block</code> \u7684\u8303\u56f4\u5c31\u662f\u4e00\u822c\u610f\u4e49\u4e0a\u7684 block\uff0c<code>block.sync()</code> \u7b49\u4ef7\u4e8e <code>__syncthreads()</code>\u3002</p>"},{"location":"cpp/async-cuda/#temporal-splitting-and-five-stages-of-synchronization","title":"Temporal Splitting and Five Stages of Synchronization","text":"<pre><code>#include &lt;cuda/barrier&gt;\n#include &lt;cooperative_groups.h&gt;\n\n__device__ void compute(float* data, int curr_iteration);\n\n__global__ void split_arrive_wait(int iteration_count, float *data) {\n    using barrier = cuda::barrier&lt;cuda::thread_scope_block&gt;;\n    __shared__  barrier bar;\n    auto block = cooperative_groups::this_thread_block();\n\n    if (block.thread_rank() == 0) {\n        init(&amp;bar, block.size()); // Initialize the barrier with expected arrival count\n    }\n    block.sync();\n\n    for (int curr_iter = 0; curr_iter &lt; iteration_count; ++curr_iter) {\n        /* code before arrive */\n       barrier::arrival_token token = bar.arrive(); /* this thread arrives. Arrival does not block a thread */\n       compute(data, curr_iter);\n       bar.wait(std::move(token)); /* wait for all threads participating in the barrier to complete bar.arrive()*/\n        /* code after wait */\n    }\n}\n</code></pre> <p>\u8fd9\u91cc\u7684\u540c\u6b65\u64cd\u4f5c\u5206\u6210\u4e86\u4e24\u6b65\uff1a<code>bar.arrive()</code> \u548c <code>bar.wait()</code>\u3002\u5728 <code>bar.arrive()</code> \u4e4b\u524d\u7684\u5185\u5b58\u66f4\u65b0\u80fd\u786e\u4fdd\u88ab <code>bar.wait()</code> \u4e4b\u540e\u7684\u4ee3\u7801\u770b\u89c1\u3002 \u53ef\u4ee5\u7406\u89e3\u4e3a\uff0c<code>bar.wait()</code> \u4f1a\u4e00\u76f4\u963b\u585e\u5f53\u524d\u7ebf\u7a0b\uff0c\u76f4\u5230 <code>block</code> \u5185\u6240\u6709\u7ebf\u7a0b\uff08<code>block.size</code> \u4e2a\u7ebf\u7a0b\uff09\u5230\u8fbe <code>bar.arrive()</code>\u3002</p>"},{"location":"cpp/crtp/","title":"Curiously Recurring Template Pattern (CRTP)","text":"<p>CRTP\uff1a\u57fa\u7c7b\u6a21\u677f\u4ee5\u6d3e\u751f\u7c7b\u4f5c\u4e3a\u6a21\u677f\u53c2\u6570\uff0c\u4ece\u800c\u5728\u7f16\u8bd1\u671f\u5b9e\u73b0\u9759\u6001\u591a\u6001\uff08static polymorphism\uff09\u3002</p>"},{"location":"cpp/crtp/#_1","title":"\u5b9e\u73b0\u9759\u6001\u591a\u6001","text":"<p>CRTP \u901a\u8fc7\u5728\u57fa\u7c7b\u516c\u5f00\u63a5\u53e3\uff0c\u5e76\u5728\u6d3e\u751f\u7c7b\u4e2d\u5b9e\u73b0\u8be5\u63a5\u53e3\uff0c\u4ece\u800c\u5728\u7f16\u8bd1\u671f\u7ed1\u5b9a\u51fd\u6570\u8c03\u7528\uff0c\u5b9e\u73b0\u9759\u6001\u591a\u6001\uff0c\u907f\u514d\u865a\u51fd\u6570\u7684\u8fd0\u884c\u65f6\u5f00\u9500\u3002</p> <pre><code>template&lt;typename Derived&gt;\nclass Shape {\npublic:\n    float area() const {\n        return static_cast&lt;const Derived*&gt;(this)-&gt;area();\n    }\n};\n\nclass Circle : public Shape&lt;Circle&gt; {\npublic:\n    Circle(float radius) : radius(radius) {}\n    float area() const {\n        return 3.14159f * radius * radius;\n    }\nprivate:\n    float radius;\n};\n\nclass Square : public Shape&lt;Square&gt; {\npublic:\n    Square(float side) : side(side) {}\n    float area() const {\n        return side * side;\n    }\nprivate:\n    float side;\n};\n\n// \u4f7f\u7528\uff1a\nCircle c(5.0f);\nSquare s(4.0f);\nfloat circleArea = c.area(); // 78.5398\nfloat squareArea = s.area(); // 16.0\n</code></pre>"},{"location":"cpp/crtp/#_2","title":"\u81ea\u5b9a\u4e49\u6d3e\u751f\u7c7b\u7684\u884c\u4e3a","text":"<p>CRTP \u53ef\u4ee5\u7ed9\u57fa\u7c7b\u63d0\u4f9b\u901a\u7528\u63a5\u53e3\uff0c\u6d3e\u751f\u7c7b\u5b9e\u73b0\u5177\u4f53\u884c\u4e3a\u3002\u53ef\u4ee5\u628a\u6d3e\u751f\u7c7b\u7684\u516c\u5171\u903b\u8f91\u653e\u5728\u57fa\u7c7b\u4e2d\uff0c\u6d3e\u751f\u7c7b\u53ea\u9700\u5b9e\u73b0\u7279\u5b9a\u884c\u4e3a\uff0c\u51cf\u5c11\u4ee3\u7801\u91cd\u590d\u3002</p> <pre><code>template &lt;typename Derived&gt;\nclass EqualityComparable {\npublic:\n    bool operator==(const Derived&amp; other) const {\n        const Derived&amp; self = static_cast&lt;const Derived&amp;&gt;(*this);\n        return self.is_equal(other);\n    }\n\n    bool operator!=(const Derived&amp; other) const {\n        return !(*this == other);\n    }\n};\n\nclass MyClass : public EqualityComparable&lt;MyClass&gt; {\n    int value;\npublic:\n    MyClass(int v) : value(v) {}\n\n    bool is_equal(const MyClass&amp; other) const {\n        return value == other.value;\n    }\n};\n\n// \u4f7f\u7528\uff1a\nMyClass a(1), b(2);\nif (a == b) { ... } // \u81ea\u52a8\u53ef\u7528\uff01\n</code></pre> <ol> <li>\u5b9e\u73b0\u7f16\u8bd1\u671f\u9759\u6001\u63a5\u53e3\u7ea6\u675f\uff0c\u7c7b\u4f3c C++20 concepts\uff0c\u7528\u4e8e\u8981\u6c42\u6d3e\u751f\u7c7b\u5fc5\u987b\u5b9e\u73b0\u67d0\u4e2a\u51fd\u6570</li> </ol> <pre><code>template &lt;typename Derived&gt;\nclass Drawable {\npublic:\n    void draw() {\n        static_cast&lt;Derived*&gt;(this)-&gt;do_draw();\n    }\n};\n// \u5982\u679c Derived \u6ca1\u6709 do_draw()\uff0c\u7f16\u8bd1\u65f6\u62a5\u9519\n</code></pre> <p>\u5728 C++20 \u540e\u7684\u7248\u672c\u4e2d\uff0c\u53ef\u4ee5\u4f7f\u7528 concepts \u6765\u5b9e\u73b0\u7c7b\u4f3c\u7684\u529f\u80fd\uff1a</p> <pre><code>template &lt;typename T&gt;\nconcept DrawableConcept = requires(T t) {\n    { t.do_draw() };\n};\n\ntemplate &lt;DrawableConcept T&gt;\nclass Drawable {\npublic:\n    void draw() {\n        static_cast&lt;T*&gt;(this)-&gt;do_draw();\n    }\n};\n</code></pre>"},{"location":"cpp/crtp/#_3","title":"\u6ce8\u610f\u4e8b\u9879","text":"<ul> <li>CRTP \u9700\u8981\u6d3e\u751f\u7c7b\u5728\u7f16\u8bd1\u671f\u5df2\u77e5\uff0c\u56e0\u6b64\u4e0d\u80fd\u7528\u4e8e\u8fd0\u884c\u65f6\u591a\u6001\uff1a\u4e0d\u80fd\u4f7f\u7528\u6307\u9488\u6216\u5f15\u7528\u6765\u5b58\u50a8\u57fa\u7c7b\u7c7b\u578b\u3002</li> </ul>"},{"location":"cpp/crtp/#_4","title":"\u7528\u4f8b","text":"<p><code>std::enable_shared_from_this</code> \u662f CRTP \u7684\u4e00\u4e2a\u5e38\u89c1\u7528\u4f8b\uff0c\u5b83\u5141\u8bb8\u7c7b\u5b89\u5168\u5730\u4ece <code>shared_from_this</code> \u83b7\u53d6\u81ea\u8eab\u7684 <code>shared_ptr</code>\u3002</p> <pre><code>class Widget: public std::enable_shared_from_this&lt;Widget&gt; {\npublic:\n\u2026\nvoid process();\n\u2026\n}\n</code></pre> <p>\u7b80\u5316\u7248\u5b9e\u73b0\uff1a</p> <pre><code>template &lt;typename T&gt;\nclass enable_shared_from_this {\nprotected:\n    weak_ptr&lt;T&gt; __weak_this; // \u5185\u90e8\u4fdd\u5b58\u4e00\u4e2a\u5f31\u5f15\u7528\uff0c\u6307\u5411\u7ba1\u7406 this \u7684 shared_ptr\n\npublic:\n    shared_ptr&lt;T&gt; shared_from_this() {\n        return shared_ptr&lt;T&gt;(__weak_this); // \u4ece\u5f31\u5f15\u7528\u5347\u7ea7\u4e3a shared_ptr\n    }\n};\n</code></pre> <p>\u5f53\u5bf9\u8c61\u88ab\u7b2c\u4e00\u4e2a <code>shared_ptr</code> \u6784\u9020\u65f6\uff0c\u6807\u51c6\u5e93\u4f1a\u68c0\u6d4b\u5b83\u662f\u5426\u7ee7\u627f\u81ea<code>enable_shared_from_this&lt;T&gt;</code>\uff0c\u5982\u679c\u662f\uff0c\u5c31\u81ea\u52a8\u8bbe\u7f6e <code>__weak_this</code> \u6307\u5411\u8fd9\u4e2a\u63a7\u5236\u5757\u3002</p>"},{"location":"cpp/ptx/","title":"Parallel Thread Execution (PTX)","text":""},{"location":"cpp/ptx/#9712-synchronization-and-communication","title":"9.7.12 Synchronization and Communication","text":""},{"location":"cpp/ptx/#971213-griddepcontrol","title":"9.7.12.13 griddepcontrol","text":"<pre><code>griddepcontrol.action;\n\n.action   = { .launch_dependents, .wait }\n</code></pre> <p>The griddepcontrol instruction allows the dependent grids and prerequisite grids as defined by the runtime, to control execution in the following way:</p> <p><code>.launch_dependents</code> modifier signals that specific dependents the runtime system designated to react to this instruction can be scheduled as soon as all other CTAs in the grid issue the same instruction or have completed. The dependent may launch before the completion of the current grid. There is no guarantee that the dependent will launch before the completion of the current grid. Repeated invocations of this instruction by threads in the current CTA will have no additional side effects past that of the first invocation.</p> <p><code>.wait</code> modifier causes the executing thread to wait until all prerequisite grids in flight have completed and all the memory operations from the prerequisite grids are performed and made visible to the current grid.</p> <p>\u4f7f\u7528\u8fd9\u4e24\u6761 ptx \u6307\u4ee4\u7684\u76ee\u7684\u662f\u542f\u7528 PDL (Programmatic Dependent Kernel Launch)\u3002 \u4f7f\u5f97\u540c\u4e00\u4e2a stream \u4e2d\uff0c\u524d\u540e\u4e24\u4e2a\u6709\u4f9d\u8d56\u5173\u7cfb\uff0c\u4f46\u4ecd\u6709\u53ef\u91cd\u53e0\u90e8\u5206\u7684 kernel\uff0c\u53ef\u4ee5\u4e00\u90e8\u5206\u91cd\u53e0\u3002 \u9700\u8981 SM90 \u53ca\u4ee5\u4e0a CC\u3002 \u4f8b\u5982\u4ee5\u4e0b\u4f8b\u5b50\uff1a</p> <pre><code>__global__ void primary_kernel() {\n    // Initial work that should finish before starting secondary kernel\n\n    // Trigger the secondary kernel\n    cudaTriggerProgrammaticLaunchCompletion();\n\n    // Work that can coincide with the secondary kernel\n}\n\n__global__ void secondary_kernel()\n{\n    // Initialization, Independent work, etc.\n\n    // Will block until all primary kernels the secondary kernel is dependent on have\n    // completed and flushed results to global memory\n    cudaGridDependencySynchronize();\n\n    // Dependent work\n}\n\n// Launch the secondary kernel with the special attribute\n\n// Set Up the attribute\ncudaLaunchAttribute attribute[1];\nattribute[0].id = cudaLaunchAttributeProgrammaticStreamSerialization;\nattribute[0].val.programmaticStreamSerializationAllowed = 1;\n\n// Set the attribute in a kernel launch configuration\n\u00a0cudaLaunchConfig_t config = {0};\n\n// Base launch configuration\nconfig.gridDim = grid_dim;\nconfig.blockDim = block_dim;\nconfig.dynamicSmemBytes= 0;\nconfig.stream = stream;\n\n// Add special attribute for PDL\nconfig.attrs = attribute;\nconfig.numAttrs = 1;\n\n// Launch primary kernel\nprimary_kernel&lt;&lt;&lt;grid_dim, block_dim, 0, stream&gt;&gt;&gt;();\n\n// Launch secondary (dependent) kernel using the configuration with\n// the attribute\ncudaLaunchKernelEx(&amp;config, secondary_kernel);\n</code></pre> <p><code>cudaGridDependencySynchronize()</code> \u51fd\u6570\u5c01\u88c5\u7684\u5c31\u662f <code>griddepcontrol.wait</code> \u6307\u4ee4\uff0c\u7b49\u5f85\u7b2c\u4e00\u4e2a kernel \u88ab\u4f9d\u8d56\u7684\u90e8\u5206\u6267\u884c\u5b8c\u6210\u3002 <code>cudaTriggerProgrammaticLaunchCompletion()</code> \u51fd\u6570\u5c01\u88c5\u7684\u662f <code>griddepcontrol.launch_dependents</code> \u6307\u4ee4\uff0c\u63d0\u793a\u7b2c\u4e8c\u4e2a kernel \u53ef\u4ee5\u5f80\u4e0b\u8fd0\u884c\u3002</p>"},{"location":"fire/theory/","title":"Initialization","text":""},{"location":"fire/theory/#permanent-portfolio","title":"Permanent Portfolio","text":"<p>\u6c38\u4e45\u6295\u8d44\u7ec4\u5408\u914d\u7f6e 4 \u79cd\u8d44\u4ea7\uff1a</p> <ul> <li>25% \u80a1\u7968</li> <li>25% \u503a\u5238</li> <li>25% \u73b0\u91d1</li> <li>25% \u9ec4\u91d1</li> </ul> <p>\u901a\u8fc7\u5206\u6563\u5316\u8d44\u4ea7\u7c7b\u522b\u5e94\u5bf9\u4e0d\u540c\u7ecf\u6d4e\u72b6\u51b5\u3002\u88ab\u52a8\u7ba1\u7406\u800c\u975e\u79ef\u6781\u7ba1\u7406\u3002</p>"},{"location":"fire/theory/#4","title":"4 \u79cd\u7ecf\u6d4e\u72b6\u51b5","text":"<ol> <li>\u7e41\u8363\uff1a\u751f\u4ea7\u529b\u4e0e\u6536\u76ca\u63d0\u9ad8\uff0c\u4f4e\u5931\u4e1a\u7387\uff0c\u7a33\u5b9a\u6216\u4e0b\u8dcc\u7684\u5229\u7387\u3002\u5bf9\u7ecf\u6d4e\u72b6\u51b5\u6301\u4e50\u89c2\u6001\u5ea6\uff0c\u80a1\u7968\u8d70\u52bf\u826f\u597d\u3002\u503a\u5238\u8868\u73b0\u4e5f\u4e0d\u9519\uff0c\u4f46\u662f\u4e0d\u5982\u80a1\u7968\u3002\u4e0d\u5229\u597d\u6ca1\u6709\u5229\u606f\u4e0e\u5206\u7ea2\u7684\u9ec4\u91d1\u3002</li> <li>\u901a\u8d27\u7d27\u7f29\uff1a\u4ef7\u683c\u4e0b\u964d\uff0c\u5229\u7387\u4e0b\u8dcc\uff0c\u8d27\u5e01\u4ef7\u503c\u4e0a\u6da8\u3002\u5229\u7387\u4e0b\u8dcc\u5229\u597d\u957f\u671f\u503a\u5238\u3002\u7531\u4e8e\u73b0\u91d1\u8d2d\u4e70\u529b\u589e\u52a0\uff0c\u73b0\u91d1\u7c7b\u8d44\u4ea7\u8868\u73b0\u826f\u597d\u3002\u4ef7\u683c\u4e0b\u964d\u4f7f\u516c\u53f8\u5229\u6da6\u4e0b\u8dcc\uff0c\u80a1\u7968\u8868\u73b0\u7cdf\u7cd5\u3002\u800c\u9ec4\u91d1\u901a\u5e38\u8868\u73b0\u4e0d\u592a\u597d\u3002</li> <li>\u8870\u9000\uff1a\u5728\u6c38\u4e45\u6295\u8d44\u7ec4\u5408\u7684\u8bed\u5883\u4e0b\uff0c\u8870\u9000\u6307\u7684\u662f\u94f6\u6839\u7d27\u7f29\u8870\u9000\u671f\u3002\u5728\u94f6\u6839\u7f29\u7d27\u8870\u9000\u671f\uff0c\u592e\u884c\u63d0\u9ad8\u5229\u7387\u6765\u964d\u4f4e\u9ad8\u901a\u80c0\uff0c\u5e26\u6765\u7684\u4e0d\u826f\u540e\u679c\u5c31\u662f\u7ecf\u6d4e\u8870\u9000\u3002\u80a1\u7968\u4e0e\u503a\u5238\u8868\u73b0\u4e0d\u4f73\u3002\u9ec4\u91d1\u901a\u5e38\u8868\u73b0\u4e5f\u4e0d\u592a\u597d\u3002\u6b64\u65f6\u73b0\u91d1\u53ef\u4ee5\u7528\u6765\u8d2d\u5165\u4ef7\u683c\u8f83\u4f4e\u7684\u5176\u4ed6\u8d44\u4ea7\u3002</li> <li>\u901a\u8d27\u81a8\u80c0\uff1a\u8d27\u5e01\u4ef7\u503c\u4e0b\u8dcc\uff0c\u4ef7\u683c\u4e0a\u6da8\uff0c\u4f34\u968f\u5229\u7387\u4e0a\u6da8\u3002\u9ad8\u901a\u80c0\u4f7f\u8d44\u91d1\u8d2c\u503c\uff0c\u9ec4\u91d1\u53ef\u4ee5\u5bf9\u8d44\u4ea7\u52a0\u4ee5\u4fdd\u62a4\u3002</li> </ol>"},{"location":"fire/theory/#_1","title":"\u80a1\u7968","text":"<ul> <li>\u6807\u666e500 / \u7eb3\u6307100</li> <li>\u6052\u751f\u79d1\u6280\u6307\u6570</li> <li>\u6caa\u6df1300</li> </ul>"},{"location":"fire/theory/#_2","title":"\u503a\u5238","text":"<ul> <li>30\u5e74\u56fd\u503aETF</li> </ul>"},{"location":"fire/theory/#_3","title":"\u73b0\u91d1","text":"<ul> <li>\u77ed\u503a\uff08\u4f59\u989d\u5b9d\uff09</li> </ul>"},{"location":"fire/theory/#_4","title":"\u9ec4\u91d1","text":"<ul> <li>\u9ec4\u91d1ETF</li> </ul>"},{"location":"fire/theory/#target","title":"Target","text":"<ul> <li>20% \u6807\u666e500</li> <li>20% \u4e2d\u6e2f\u80a1\u6307</li> <li>20% 30\u5e74\u56fd\u503a</li> <li>20% \u9ec4\u91d1</li> <li>20% \u73b0\u91d1</li> </ul>"},{"location":"notes/cute/","title":"CuTe Basic","text":""},{"location":"notes/cute/#cute-layout","title":"CuTe Layout","text":"<p>Layout \u7528\u4e8e\u63cf\u8ff0 Tensor \u5728\u5185\u5b58\u4e2d\u7684\u6392\u5e03\uff0c\u63d0\u4f9b\u4e86\u4ece\u903b\u8f91\u5750\u6807\u5230\u7269\u7406\u5750\u6807\u7684\u6620\u5c04\u3002 \u5bf9\u4e8e\u4e8c\u7ef4\u77e9\u9635\uff0c\u6211\u4eec\u53ef\u4ee5\u7b80\u5355\u7684\u7528 row-major \u6216 column-major \u6765\u63cf\u8ff0\u5176\u7269\u7406\u6620\u5c04\u3002 \u4f46\u662f\u5bf9\u4e8e\u66f4\u9ad8\u7ef4\u7684\u5f20\u91cf\uff0c\u8fd9\u6837\u7684\u63cf\u8ff0\u65b9\u5f0f\u663e\u7136\u4e0d\u591f\u7528\u4e86\u3002 \u4e8e\u662f\uff0c\u6211\u4eec\u6709\u4e86\u4f7f\u7528 Shape \u548c Stride \u6765\u63cf\u8ff0\u7269\u7406\u6620\u5c04\u7684\u65b9\u5f0f\u3002</p>"},{"location":"notes/cute/#layout-in-pytorch","title":"Layout in PyTorch","text":"<p>PyTorch \u4e2d\uff0cTensor \u7c7b\u5305\u542b\u6709 <code>shape</code> \u5c5e\u6027 \u548c <code>stride()</code> \u65b9\u6cd5\u3002 shape \u63cf\u8ff0\u4e86 tensor \u5728\u903b\u8f91\u4e0a\u5404\u4e2a\u7ef4\u5ea6\u7684\u5927\u5c0f\u3002 stride \u63cf\u8ff0\u4e86 tensor \u5728\u5404\u4e2a\u7ef4\u5ea6\u4e0a\u7684\u76f8\u90bb\u5143\u7d20\u5728\u7269\u7406\u5750\u6807\u4e0a\u7684\u5dee\u503c\u3002 \u5bf9\u4e8e\u4e00\u4e2a\u5927\u5c0f\uff08shape\uff09\u4e3a <code>[3, 4]</code> \u7684\u77e9\u9635\uff0crow-major \u6392\u5e03\u6240\u5bf9\u5e94\u7684 stride \u4e3a <code>[4, 1]</code>\uff0c \u8868\u793a\u540c\u4e00\u884c\u7684\u76f8\u90bb\u4e24\u4e2a\u5143\u7d20\u5728\u7269\u7406\u5750\u6807\u4e0a\u76f8\u5dee 1\uff0c\u540c\u4e00\u5217\u7684\u76f8\u90bb\u4e24\u4e2a\u5143\u7d20\u5728\u7269\u7406\u5750\u6807\u4e0a\u76f8\u5dee 4\u3002 \u5982\u679c\u8fd9\u4e2a\u77e9\u9635\u662f column-major \u6392\u5e03\u7684\uff0c\u5219 stride \u4e3a <code>[1, 3]</code>\u3002 \u8fd9\u91cc\u7684 shape \u548c stride \u53ef\u4ee5\u81ea\u7136\u7684\u62d3\u5c55\u81f3\u9ad8\u7ef4 tensor\u3002 \u6bcf\u4e2a\u903b\u8f91\u5750\u6807\u5bf9\u5e94\u7684\u7269\u7406\u5750\u6807\u4e5f\u5f88\u5bb9\u6613\u5f97\u5230\uff1a</p> \\[ Id_{physical} = \\sum_i Coord_{i} \\times Stride_{i}. \\] <p>\u7528 shape \u548c stride \u6765\u63cf\u8ff0 tensor \u5f88\u597d\u7684\u523b\u753b\u4e86\u4e00\u4f4d\u5b58\u50a8\u7ed3\u6784\u548c\u9ad8\u7ef4\u903b\u8f91\u7ed3\u6784\u7684\u5173\u7cfb\uff0c \u540c\u65f6\u4f7f\u5f97\u5728\u4e00\u4e9b reshape \u64cd\u4f5c\u65f6\uff0c\u907f\u514d\u4e86\u5bf9\u6570\u636e\u672c\u8eab\u8fdb\u884c\u64cd\u4f5c\uff0c\u4ec5\u9700\u901a\u8fc7\u6539\u5199 shape \u548c stride \u5c5e\u6027\uff08PyTorch \u4e2d\u7684 <code>view</code>\uff09\u3002 \u7136\u800c\uff0c\u5728\u8fd9\u5957\u6392\u5e03\u63cf\u8ff0\u4e2d\uff0c\u6bcf\u4e00\u4e2a\u7ef4\u5ea6\u53ea\u80fd\u6709\u4e00\u4e2a stride \u503c\uff0c\u4f7f\u5f97\u66f4\u590d\u6742\u7684\u591a\u5c42\u6b21\u6392\u5e03\u65e0\u6cd5\u88ab\u63cf\u8ff0\u3002</p>"},{"location":"notes/cute/#hierarchical-layout-in-cute","title":"Hierarchical Layout in CuTe","text":"<p>\u6709\u65f6\uff0c\u6211\u4eec\u4f1a\u5e0c\u671b\u4e00\u4e2a\u5927 tensor \u6309\u67d0\u79cd\u6392\u5e03\u88ab\u5206\u5757\u6210\u82e5\u5e72\u5c0f tensor\uff0c\u800c\u6bcf\u4e2a\u5c0f tensor \u5185\u90e8\u53c8\u9075\u5faa\u67d0\u79cd\u6392\u5e03\u3002 \u62ff GEMM \u4f5c\u4e3a\u4f8b\u5b50\uff0c\u6211\u4eec\u4f1a\u5728\u8f93\u51fa\u77e9\u9635 C \u4e0a\u505a\u5206\u5757\uff0c\u4f7f\u5f97\u6bcf\u4e2a SM \u6709\u8db3\u591f\u8d44\u6e90\u8ba1\u7b97\u4e00\u4e2a\u5757\u3002 \u4e3a\u4e86\u63d0\u5347 L2 Cache \u547d\u4e2d\u7387\uff0c\u5feb\u4e0e\u5757\u4e4b\u95f4\u7684\u6392\u5e03\u53ef\u80fd\u5e76\u4e0d\u662f\u5355\u7eaf\u7684 row-major \u6216\u8005 column-major\uff0c\u800c\u662f\u91c7\u7528 Threadblock Swizzling\uff1a \u82e5\u5e72\u4e2a\u5757\u5148\u6309 row-major \u7684\u6392\u5e03\u7ec4\u6210\u4e00\u4e2a\u8f83\u5927\u7684 block\uff0c\u7136\u540e\u6240\u6709\u7684 block \u518d\u901a\u8fc7 row-major \u7684\u6392\u5e03\u7ec4\u6210\u6574\u4e2a\u5927 tensor\u3002 \u8fd9\u79cd\u590d\u5408\u7684\u6392\u5e03\u65e0\u6cd5\u518d\u88ab shape \u548c stride \u7684\u8bed\u8a00\u63cf\u8ff0\u4e86\uff0cCuTe \u6240\u91c7\u7528\u7684\u591a\u5c42\u7ea7\u6392\u5e03\u63cf\u8ff0\u5e94\u8fd0\u800c\u751f\u3002</p> <p></p> <p>\u5728 CuTe Layout \u4e2d\uff0c\u6bcf\u4e2a\u7ef4\u5ea6\u5bf9\u5e94\u7684 shape \u4e0d\u518d\u5fc5\u987b\u4e3a\u4e00\u4e2a\u6574\u6570\uff0c\u4e5f\u53ef\u4ee5\u5305\u542b\u591a\u4e2a\u6574\u6570\u3002 \u5728\u4e0a\u56fe c \u7684\u4f8b\u5b50\u4e2d\uff0c\u884c\u7ef4\u5ea6\u4e0a shape \u662f 4\uff0cstride \u662f 2\uff0c\u884c\u4e3a\u4e0e\u4e0a\u8282\u6240\u4ecb\u7ecd\u7684\u4e00\u81f4\u3002 \u5217\u7ef4\u5ea6\u4e0a\uff0cshape \u662f (2, 4)\uff0cstride \u662f (1, 8)\uff0c\u8fd9\u91cc\u7684 shape \u548c stride \u662f\u6309\u7167\u4ece\u91cc\u5f80\u5916\u7684\u987a\u5e8f\uff1a \u8fde\u7eed\u7684 2 \u5217\u5148\u6784\u6210\u4e00\u4e2a\u5757\uff0c\u8fde\u7eed 4 \u4e2a\u5757 \u518d\u6784\u6210\u6240\u6709\u5217\u3002 \u5728\u6bcf\u4e2a\u5757\u4e2d\uff0c\u540c\u4e00\u884c\u76f8\u90bb\u4e24\u5217\u7684\u7269\u7406\u5750\u6807\u76f8\u5dee 1\u3002 \u5bf9\u4e8e\u5728\u5217\u4e0a\u76f8\u90bb\u7684\u4e24\u4e2a\u5757\uff0c\u76f8\u540c\u4f4d\u7f6e\u7684\u5143\u7d20\u7269\u7406\u5750\u6807\u76f8\u5dee 8\u3002 \u7c7b\u4f3c\u7684\uff0c\u4e0a\u56fe d \u7684\u4f8b\u5b50\u5728\u884c\u7ef4\u5ea6\u4e0a\u5207\u6210\u4e86 2 \u4e2a\u7531\u8fde\u7eed 2 \u884c\u6784\u6210\u7684\u5757\u3002 \u5757\u5185\u7684 stride \u4e3a 1\uff0c\u5757\u95f4\u7684 stride \u4e3a 4\u3002 \u5728\u5217\u7ef4\u5ea6\u4e0a\u5207\u6210\u4e86 4 \u4e2a\u7531\u8fde\u7eed 2 \u5217\u6784\u6210\u7684\u5757\u3002 \u5757\u5185\u7684 stride \u4e3a 2\uff0c\u5757\u95f4\u7684 stride \u4e3a 8\u3002</p> <p>\u63a5\u4e0b\u6765\uff0c\u6211\u4eec\u6765\u5206\u6790 CuTe \u4e2d Layout \u7684\u5b9e\u73b0\uff0c\u7ed9\u51fa Layout \u8f83\u4e3a\u4e25\u8c28\u7684\u5b9a\u4e49\u3002</p>"},{"location":"notes/cute/#shape-stride-layout","title":"Shape, Stride, Layout","text":"<p>\u524d\u7f6e\u77e5\u8bc6\uff1a\u9759\u6001\u6574\u6570 <code>Int&lt;2&gt;{}</code> (<code>_2</code>) \u548c\u52a8\u6001\u6574\u6570 <code>int{2}</code> (<code>2</code>)\uff0c\u7edf\u79f0\u6574\u6570 (Integer)\u3002</p> <p>\u5b9a\u4e49\uff1a<code>IntTuple</code> \u662f\u4e00\u4e2a\u6574\u6570\uff0c\u6216\u8005\u662f\u7531 IntTuple \u7ec4\u6210\u7684\u5143\u7ec4\u3002</p> <p>\u4f8b\u5982\uff0c<code>2</code>, <code>_3</code>, <code>make_tuple(2, _3)</code>, <code>make_tuple(42, make_tuple(_1, 3), _17)</code> \u90fd\u662f IntTuple\u3002</p> <p>\u5728 CuTe \u4e2d\uff0c<code>Shape</code> \u548c <code>Stride</code> \u90fd\u662f IntTuple\u3002 <code>Layout</code> \u662f\u4e00\u4e2a <code>Shape</code> \u548c <code>Stride</code> \u7ec4\u6210\u7684\u5143\u7ec4\uff0c\u63cf\u8ff0\u4e86\u903b\u8f91\u5750\u6807 <code>Coord</code> \u5230\u4e00\u7ef4\u7269\u7406\u5750\u6807\u7684\u6620\u5c04\u3002 \u8fd9\u4e2a\u6620\u5c04\u7531\u4e0b\u9762\u7684\u4ee3\u7801\u5b9e\u73b0\uff0c\u603b\u7ed3\u6210\u4e09\u6761\u89c4\u5219\uff1a</p> <ol> <li>\u5f53 Coord, Shape, Stride \u90fd\u662f\u6574\u6570\u65f6\uff0c\u7269\u7406\u5750\u6807\u4e3a <code>Coord x Stride</code>\u3002</li> <li>\u5f53 Coord \u662f\u6574\u6570\uff0cShape \u548c Stride \u662f\u5143\u7ec4\u65f6\uff0c\u5219\u5f53\u524d\u7ef4\u5ea6\u4e0a\u9700\u8981\u5206\u5757\u3002\u6309\u7167\u6700\u4f4e\u7ef4\u5728\u5148\u7684\u89c4\u5219\uff0c\u4ece\u6700\u91cc\u9762\u7684\u5757\u5f00\u59cb\uff0c\u9700\u8981\u4ece\u6574\u6570 Coord \u8ba1\u7b97\u51fa\u5757\u5185\u504f\u79fb\u548c\u5757\u7684\u5750\u6807\uff08\u7b2c\u51e0\u5757\uff09\uff0c\u5411\u5916\u8fed\u4ee3\uff0c\u7d2f\u52a0\u7269\u7406\u5750\u6807\u3002</li> <li>\u5f53 Coord, Shape, Stride \u90fd\u662f\u5143\u7ec4\u65f6\uff0c\u5219\u8fed\u4ee3\u5143\u7ec4\u5185\u7684\u6bcf\u4e00\u9879\uff0c\u7d2f\u52a0\u7269\u7406\u5750\u6807\u3002</li> </ol> <p>\u4e0b\u9762\u4ee3\u7801\u7684\u6ce8\u91ca\u89e3\u91ca\u7684\u6bd4\u8f83\u6e05\u695a\uff1a</p> <pre><code>/** crd2idx(c,s,d) maps a coordinate within &lt;Shape,Stride&gt; to an index\n *\n * This is computed as follows:\n *  [coord, shape, and stride are all integers =&gt; step forward by stride]\n * op(c, s, d)             =&gt; c * d\n *  [coord is integer, shape and stride are tuple =&gt; divmod coord for each mode]\n * op(c, (s,S), (d,D))     =&gt; op(c % prod(s), s, d) + op(c / prod(s), (S), (D))\n *  [coord, shape, and stride are all tuples =&gt; consider each mode independently]\n * op((c,C), (s,S), (d,D)) =&gt; op(c, s, d) + op((C), (S), (D))\n */\ntemplate &lt;class Coord, class Shape, class Stride&gt;\nCUTE_HOST_DEVICE constexpr\nauto\ncrd2idx(Coord  const&amp; coord,\n        Shape  const&amp; shape,\n        Stride const&amp; stride);\n\nnamespace detail {\n\ntemplate &lt;class Coord, class Shape, class Stride, int... Is&gt;\nCUTE_HOST_DEVICE constexpr\nauto\ncrd2idx_ttt(Coord  const&amp; coord,\n            Shape  const&amp; shape,\n            Stride const&amp; stride, seq&lt;Is...&gt;)\n{\n  return (... + crd2idx(get&lt;Is&gt;(coord), get&lt;Is&gt;(shape), get&lt;Is&gt;(stride)));\n}\n\ntemplate &lt;class CInt, class STuple, class DTuple, int I0, int... Is&gt;\nCUTE_HOST_DEVICE constexpr\nauto\ncrd2idx_itt(CInt   const&amp; coord,\n            STuple const&amp; shape,\n            DTuple const&amp; stride, seq&lt;I0,Is...&gt;)\n{\n  if constexpr (sizeof...(Is) == 0) {  // Avoid recursion and mod on single/last iter\n    return crd2idx(coord, get&lt;I0&gt;(shape), get&lt;I0&gt;(stride));\n  } else if constexpr (is_constant&lt;0, CInt&gt;::value) {\n    return crd2idx(_0{}, get&lt;I0&gt;(shape), get&lt;I0&gt;(stride))\n         + (_0{} + ... + crd2idx(_0{}, get&lt;Is&gt;(shape), get&lt;Is&gt;(stride)));\n  } else {                             // General case\n    auto [div, mod] = divmod(coord, product(get&lt;I0&gt;(shape)));\n    return crd2idx(mod, get&lt;I0&gt;(shape), get&lt;I0&gt;(stride))\n         + crd2idx_itt(div, shape, stride, seq&lt;Is...&gt;{});\n  }\n\n  CUTE_GCC_UNREACHABLE;\n}\n\n} // end namespace detail\n\ntemplate &lt;class Coord, class Shape, class Stride&gt;\nCUTE_HOST_DEVICE constexpr\nauto\ncrd2idx(Coord  const&amp; coord,\n        Shape  const&amp; shape,\n        Stride const&amp; stride)\n{\n  if constexpr (is_tuple&lt;Coord&gt;::value) {\n    if constexpr (is_tuple&lt;Shape&gt;::value) {      // tuple tuple tuple\n      static_assert(tuple_size&lt;Coord&gt;::value == tuple_size&lt; Shape&gt;::value, \"Mismatched Ranks\");\n      static_assert(tuple_size&lt;Coord&gt;::value == tuple_size&lt;Stride&gt;::value, \"Mismatched Ranks\");\n      return detail::crd2idx_ttt(coord, shape, stride, tuple_seq&lt;Coord&gt;{});\n    } else {                                     // tuple \"int\" \"int\"\n      static_assert(sizeof(Coord) == 0, \"Invalid parameters\");\n    }\n  } else {\n    if constexpr (is_tuple&lt;Shape&gt;::value) {      // \"int\" tuple tuple\n      static_assert(tuple_size&lt;Shape&gt;::value == tuple_size&lt;Stride&gt;::value, \"Mismatched Ranks\");\n      return detail::crd2idx_itt(coord, shape, stride, tuple_seq&lt;Shape&gt;{});\n    } else {                                     // \"int\" \"int\" \"int\"\n      return coord * stride;\n    }\n  }\n\n  CUTE_GCC_UNREACHABLE;\n}\n</code></pre> <p>\u76f8\u53cd\uff0c\u4e5f\u6709\u4ece\u4e00\u7ef4\u7269\u7406\u5750\u6807\u5230\u903b\u8f91\u5750\u6807\u5230\u6620\u5c04\uff0c\u540c\u6837\u6309\u7167\u6700\u4f4e\u7ef4\u5728\u5148\u7684\u89c4\u5219\uff1a</p> <pre><code>/** idx2crd(i,s,d) splits an index into a coordinate within &lt;Shape,Stride&gt;.\n *\n * This is computed as follows:\n *  [index, shape, and stride are all integers =&gt; determine 1D coord]\n * op(i, s, d)             =&gt; (i / d) % s\n *  [index is integer, shape and stride are tuple =&gt; determine component for each mode]\n * op(i, (s,S), (d,D))     =&gt; (op(i, s, d), op(i, S, D)...)\n *  [index, shape, and stride are all tuples =&gt; consider each mode independently]\n * op((i,I), (s,S), (d,D)) =&gt; (op(i, s, d), op((I), (S), (D)))\n *\n * NOTE: This only works for compact shape+stride layouts. A more general version would\n *       apply to all surjective layouts\n */\ntemplate &lt;class Index, class Shape, class Stride&gt;\nCUTE_HOST_DEVICE constexpr\nauto\nidx2crd(Index  const&amp; idx,\n        Shape  const&amp; shape,\n        Stride const&amp; stride)\n{\n  if constexpr (is_tuple&lt;Index&gt;::value) {\n    if constexpr (is_tuple&lt;Shape&gt;::value) {      // tuple tuple tuple\n      static_assert(tuple_size&lt;Index&gt;::value == tuple_size&lt; Shape&gt;::value, \"Mismatched Ranks\");\n      static_assert(tuple_size&lt;Index&gt;::value == tuple_size&lt;Stride&gt;::value, \"Mismatched Ranks\");\n      return transform(idx, shape, stride, [](auto const&amp; i, auto const&amp; s, auto const&amp; d){ return idx2crd(i,s,d); });\n    } else {                                     // tuple \"int\" \"int\"\n      static_assert(sizeof(Index) == 0, \"Invalid parameters\");\n    }\n  } else {\n    if constexpr (is_tuple&lt;Shape&gt;::value) {\n      if constexpr (is_tuple&lt;Stride&gt;::value) {   // \"int\" tuple tuple\n        static_assert(tuple_size&lt;Shape&gt;::value == tuple_size&lt;Stride&gt;::value, \"Mismatched Ranks\");\n        return transform(shape, stride, [&amp;](auto const&amp; s, auto const&amp; d){ return idx2crd(idx,s,d); });\n      } else {                                   // \"int\" tuple \"int\"\n        return transform(shape, compact_col_major(shape, stride), [&amp;](auto const&amp; s, auto const&amp; d){ return idx2crd(idx,s,d); });\n      }\n    } else {                                     // \"int\" \"int\" \"int\"\n      if constexpr (is_constant&lt;1, Shape&gt;::value) {\n        // Skip potential stride-0 division\n        return Int&lt;0&gt;{};\n      } else {\n        return (idx / stride) % shape;\n      }\n    }\n  }\n\n  CUTE_GCC_UNREACHABLE;\n}\n\n/** idx2crd(i,s) splits an index into a coordinate within Shape\n * via a colexicographical enumeration of coordinates in Shape.\n * c0 = (idx / 1) % s0\n * c1 = (idx / s0) % s1\n * c2 = (idx / (s0 * s1)) % s2\n * ...\n */\ntemplate &lt;class Index, class Shape&gt;\nCUTE_HOST_DEVICE constexpr\nauto\nidx2crd(Index const&amp; idx,\n        Shape const&amp; shape)\n{\n  if constexpr (is_tuple&lt;Index&gt;::value) {\n    if constexpr (is_tuple&lt;Shape&gt;::value) {      // tuple tuple\n      static_assert(tuple_size&lt;Index&gt;::value == tuple_size&lt;Shape&gt;::value, \"Mismatched Ranks\");\n      return transform(idx, shape, [](auto const&amp; i, auto const&amp; s) { return idx2crd(i,s); });\n    } else {                                     // tuple \"int\"\n      static_assert(sizeof(Index) == 0, \"Invalid parameters\");\n    }\n  } else {\n    if constexpr (is_tuple&lt;Shape&gt;::value) {      // \"int\" tuple\n      return transform_leaf(as_arithmetic_tuple(crd2idx(idx, shape, make_basis_like(shape))), identity{});\n    } else {                                     // \"int\" \"int\"\n      return idx;\n    }\n  }\n}\n</code></pre> <p>Layout \u7684\u6784\u9020\u4e5f\u53ef\u4ee5\u53ea\u63d0\u4f9b Shape\uff0c\u4e0d\u63d0\u4f9b Stride\u3002 \u8fd9\u65f6\u4f1a\u6309\u7167 LayoutLeft \uff08\u6700\u91cc\u7684\u7ef4\u5ea6\u5728\u5148\uff0c\u5373 Column major\uff09\u6784\u9020 Stride\u3002 \u5177\u4f53\u5b9e\u4f8b\u53ef\u89c1\u4e0b\u8282\u3002</p>"},{"location":"notes/cute/#layout-manipulation","title":"Layout Manipulation","text":""},{"location":"notes/cute/#sublayouts","title":"Sublayouts","text":"<p>\u591a\u5c42\u5355\u70b9\u9009\u53d6\uff1a</p> <pre><code>Layout a   = Layout&lt;Shape&lt;_4,Shape&lt;_3,_6&gt;&gt;&gt;{}; // (4,(3,6)):(1,(4,12))\nLayout a0  = layout&lt;0&gt;(a);                     // 4:1\nLayout a1  = layout&lt;1&gt;(a);                     // (3,6):(4,12)\nLayout a10 = layout&lt;1,0&gt;(a);                   // 3:4\nLayout a11 = layout&lt;1,1&gt;(a);                   // 6:12\n</code></pre> <p>\u5355\u5c42\u591a\u70b9\u9009\u53d6\uff1a</p> <pre><code>Layout a   = Layout&lt;Shape&lt;_2,_3,_5,_7&gt;&gt;{};     // (2,3,5,7):(1,2,6,30)\nLayout a13 = select&lt;1,3&gt;(a);                   // (3,7):(2,30)\nLayout a01 = select&lt;0,1,3&gt;(a);                 // (2,3,7):(1,2,30)\nLayout a2  = select&lt;2&gt;(a);                     // (5):(6)\n</code></pre> <p>\u5355\u5c42\u8303\u56f4\u9009\u53d6\uff1a</p> <pre><code>Layout a   = Layout&lt;Shape&lt;_2,_3,_5,_7&gt;&gt;{};     // (2,3,5,7):(1,2,6,30)\nLayout a13 = take&lt;1,3&gt;(a);                     // (3,5):(2,6)\nLayout a14 = take&lt;1,4&gt;(a);                     // (3,5,7):(2,6,30)\n// take&lt;1,1&gt; not allowed. Empty layouts not allowed.\n</code></pre>"},{"location":"notes/cute/#concatenation","title":"Concatenation","text":"<pre><code>Layout a = Layout&lt;_3,_1&gt;{};                     // 3:1\nLayout b = Layout&lt;_4,_3&gt;{};                     // 4:3\nLayout row = make_layout(a, b);                 // (3,4):(1,3)\nLayout col = make_layout(b, a);                 // (4,3):(3,1)\nLayout q   = make_layout(row, col);             // ((3,4),(4,3)):((1,3),(3,1))\nLayout aa  = make_layout(a);                    // (3):(1)\nLayout aaa = make_layout(aa);                   // ((3)):((1))\nLayout d   = make_layout(a, make_layout(a), a); // (3,(3),3):(1,(1),1)\n</code></pre>"},{"location":"notes/cute/#grouping-and-flattening","title":"Grouping and Flattening","text":"<pre><code>Layout a = Layout&lt;Shape&lt;_2,_3,_5,_7&gt;&gt;{};  // (_2,_3,_5,_7):(_1,_2,_6,_30)\nLayout b = group&lt;0,2&gt;(a);                 // ((_2,_3),_5,_7):((_1,_2),_6,_30)\nLayout c = group&lt;1,3&gt;(b);                 // ((_2,_3),(_5,_7)):((_1,_2),(_6,_30))\nLayout f = flatten(b);                    // (_2,_3,_5,_7):(_1,_2,_6,_30)\nLayout e = flatten(c);                    // (_2,_3,_5,_7):(_1,_2,_6,_30)\n</code></pre>"},{"location":"notes/cute/#layout-algebra","title":"Layout Algebra","text":""},{"location":"notes/cute/#coalesce","title":"Coalesce","text":"<p>Coalesce \u64cd\u4f5c\u662f\u5bf9 Layout \u7684\u7b80\u5316\u3002\u4f8b\u5982\uff1a</p> <pre><code>auto layout = Layout&lt;Shape &lt;_2,Shape &lt;_1,_6&gt;&gt;,\n                     Stride&lt;_1,Stride&lt;_6,_2&gt;&gt;&gt;{};\nauto result = coalesce(layout);    // _12:_1\n</code></pre> <p>\u8003\u8651 Coalesce \u4e00\u4e2a\u4e24\u7ef4\u7684 Layout <code>(s0, s1):(d0, d1)</code>\uff0c\u5b58\u5728\u56db\u79cd\u60c5\u51b5\uff1a</p> <ol> <li><code>(s0, _1):(d0, d1) =&gt; s0:d0</code>\uff0c\u79fb\u9664\u5927\u5c0f\u4e3a 1 \u7684\u7ef4\u5ea6</li> <li><code>(_1, s1):(d0, d1) =&gt; s1:d1</code>\uff0c\u79fb\u9664\u5927\u5c0f\u4e3a 1 \u7684\u7ef4\u5ea6</li> <li><code>(s0, s1):(d0, s0*d0) =&gt; s0*s1:d0</code></li> <li>\u5176\u4ed6\u60c5\u51b5\u5219\u65e0\u6cd5\u5408\u5e76</li> </ol> <p>\u652f\u6301\u6309\u7ef4\u5ea6 Coalesce:</p> <pre><code>auto a = Layout&lt;Shape &lt;_2,Shape &lt;_1,_6&gt;&gt;,\n                Stride&lt;_1,Stride&lt;_6,_2&gt;&gt;&gt;{};\nauto result = coalesce(a, Step&lt;_1,_1&gt;{});   // (_2,_6):(_1,_2)\n// Identical to\nauto same_r = make_layout(coalesce(layout&lt;0&gt;(a)),\n                          coalesce(layout&lt;1&gt;(a)));\n</code></pre>"},{"location":"notes/cute/#composition","title":"Composition","text":"<p>\u65e2\u7136 Layout \u662f\u4ece\u6574\u6570\u5230\u6574\u6570\u7684\u6620\u5c04\uff0c\u6211\u4eec\u53ef\u4ee5\u590d\u5408\u4e24\u4e2a Layout \u6210\u4e00\u4e2a\u65b0\u7684 Layout\uff0c\u5c31\u50cf\u590d\u5408\u51fd\u6570\u3002</p> <pre><code>// @post compatible(@a layout_b, @a result)\n// @post for all i, 0 &lt;= i &lt; size(@a layout_b), @a result(i) == @a layout_a(@a layout_b(i)))\nLayout composition(LayoutA const&amp; layout_a, LayoutB const&amp; layout_b)\n</code></pre> <p>\u4e5f\u53ef\u4ee5\u6309\u7ef4\u5ea6\u590d\u5408\uff1a</p> <pre><code>// (12,(4,8)):(59,(13,1))\nauto a = make_layout(make_shape (12,make_shape ( 4,8)),\n                     make_stride(59,make_stride(13,1)));\n// &lt;3:4, 8:2&gt;\nauto tiler = make_tile(Layout&lt;_3,_4&gt;{},  // Apply 3:4 to mode-0\n                       Layout&lt;_8,_2&gt;{}); // Apply 8:2 to mode-1\n\n// (_3,(2,4)):(236,(26,1))\nauto result = composition(a, tiler);\n// Identical to\nauto same_r = make_layout(composition(layout&lt;0&gt;(a), get&lt;0&gt;(tiler)),\n                          composition(layout&lt;1&gt;(a), get&lt;1&gt;(tiler)));\n</code></pre>"},{"location":"notes/cute/#complement","title":"Complement","text":"<p>\u4e00\u4e2a Layout \u53ef\u4ee5\u4e0d\u662f\u5355\u5c04\u7684\uff0c\u8fd9\u65f6\u7269\u7406\u5750\u6807\u7684\u8303\u56f4\uff08codomain\uff09\u4f1a\u5927\u4e8e\u903b\u8f91\u5750\u6807\u7684\u8303\u56f4\uff08domain\uff09\u3002 Layouot \u7684\u8865\u96c6 Complement \u7528\u6765\u63cf\u8ff0 codomain \u4e2d\u6ca1\u88ab\u6620\u5c04\u5230\u7684\u4f4d\u7f6e\u3002</p> <ul> <li><code>complement(4:1, 24) =&gt; 6:4</code>\uff0c\u56e0\u4e3a <code>(4, 6):(1, 4)</code> \u6709 codomain \u5927\u5c0f\u4e3a 24</li> <li><code>complement(6:4, 24) =&gt; 6:4</code>\uff0c\u56e0\u4e3a <code>(6, 4):(4, 1)</code> \u6709 codomain \u5927\u5c0f\u4e3a 24</li> </ul> <p></p>"},{"location":"notes/cute/#division-tiling","title":"Division (Tiling)","text":"<p>Layout \u7684\u9664\u6cd5\u8fd0\u7b97\u8868\u793a\u6309\u7167\u9664\u6570 Layout \u5bf9\u88ab\u9664\u6570 Layout \u7684\u5212\u5206\u3002 \u9664\u6cd5\u7684\u7ed3\u679c\u662f\u4e00\u4e2a\u4e24\u7ef4\u7684 Layout\u3002 \u53ef\u4ee5\u628a\u9664\u6570 Layout \u7406\u89e3\u6210 Tile \u7684 Layout\uff0c\u9664\u6cd5\u7ed3\u679c\u7684\u7b2c\u4e00\u7ef4\u662f Tile \u5185\u7684 Layout\uff0c\u7b2c\u4e8c\u7ef4\u662f\u88ab\u9664\u6570 Layout \u4e2d Tile \u95f4\u7684 Layout\u3002</p> <p>Layout \u9664\u6cd5\u5171\u6709\u56db\u79cd\uff1alogical, zipped, tiled, flat\u3002\u4e0b\u9762\u5148\u4ecb\u7ecd\u7684\u9664\u6cd5\u6307\u7684\u662f logical divide\u3002</p> <p>\u5177\u4f53\u7684\uff0c\u5148\u628a\u88ab\u9664\u6570\u548c\u9664\u6570 Layout \u90fd\u5c55\u5e73\u6210\u4e00\u7ef4\uff0c\u9664\u6cd5\u7ed3\u679c Layout \u7684\u6bcf\u4e2a Tile \u6309\u7167\u9664\u6570 Layout \u7684\u7d22\u5f15\uff0c\u5728\u88ab\u9664\u6570\u4e2d\u53d6\u503c\u3002 \u53ef\u4ee5\u53c2\u8003\u4e0b\u56fe\u4e2d\u7684\u4f8b\u5b50\uff1a</p> <p></p> <p>\u5b9e\u9645\u4e0a\uff0c\u6211\u4eec\u53ef\u4ee5\u7ed9\u51fa\u4e0a\u8ff0\u9664\u6cd5\u7684\u4e25\u683c\u5b9a\u4e49\uff1a</p> \\[ A / B := A \\circ (B, B^*) \\] <p>A \u4e0e B \u7684 logical divide\uff0c\u662f B \u4e0e B \u7684\u8865\u96c6 (complement) \u7684\u62fc\u63a5 (concatenation)\uff0c\u518d\u548c A \u7684\u590d\u5408 (composition)\u3002</p> <p>\u8003\u8651\u5f53 B \u662f\u4e00\u4e2a\u5355\u5c04\u65f6\uff0c\u6709</p> \\[ A / B = A \\circ (B, B^*) = (A \\circ B, A \\circ B^*) \\] <p>\u53ef\u4ee5\u6ce8\u610f\u5230\u7ed3\u679c\u7684\u7b2c\u4e00\u7ef4\u662f\u5355\u4e2a Tile \u5185\u7684 Layout\uff0c\u800c\u7b2c\u4e8c\u7ef4\u662f\u88ab\u9664\u6570 A \u4e2d\u5404\u4e2a Tile \u7684 Layout\u3002</p> <p>\u4e0a\u8ff0\u7684 logical divide \u4e5f\u53ef\u4ee5\u6269\u5c55\u81f3\u591a\u7ef4\uff0c\u5bf9\u6bcf\u4e00\u7ef4\u5206\u522b\u505a\u4e00\u7ef4\u7684 logical divide\u3002\u89c1\u4e0b\u56fe\uff1a</p> <p>\u8fd9\u91cc\u51fa\u73b0\u7684\u5c16\u62ec\u53f7 <code>B = &lt;3:3, (2,4):(1,8)&gt;</code> \u8868\u793a B \u662f\u4e00\u4e2a Tiler\uff0c\u7528\u6765\u4ee3\u8868\u8fd0\u7b97\u662f\u5bf9\u6bcf\u4e2a\u7ef4\u5ea6\u5206\u522b\u64cd\u4f5c\u7684\u3002</p> <p></p> <p>\u524d\u6587\u63d0\u5230\uff0c\u9664\u4e86 logical divide\uff0c\u8fd8\u6709 zipped\uff0ctiled\uff0cflat divide \u4e09\u79cd\u9664\u6cd5\u8fd0\u7b97\u3002 \u8fd9\u4e09\u79cd\u9664\u6cd5\u4e0e logical divide \u7684\u533a\u522b\u4ec5\u5728\u4e8e\u7ed3\u679c layout \u7684\u6392\u5e03\uff1a</p> <pre><code>Layout Shape : (M, N, L, ...)\nTiler Shape  : &lt;TileM, TileN&gt;\n\nlogical_divide : ((TileM,RestM), (TileN,RestN), L, ...)\nzipped_divide  : ((TileM,TileN), (RestM,RestN,L,...))\ntiled_divide   : ((TileM,TileN), RestM, RestN, L, ...)\nflat_divide    : (TileM, TileN, RestM, RestN, L, ...)\n</code></pre>"},{"location":"notes/cute/#product-tiling","title":"Product (Tiling)","text":"<p>\u6709\u4e86\u9664\u6cd5\u4e4b\u540e\uff0c\u6211\u4eec\u53ef\u4ee5\u628a\u4e58\u6cd5\u60f3\u8c61\u6210\u9664\u6cd5\u7684\u9006\u8fd0\u7b97\u3002 \u9664\u6cd5\u662f\u5c06\u4e00\u4e2a\u5927\u7684 Layout \u6309\u7167\u67d0\u79cd Tiling \u5206\u5757\uff0c\u4e58\u6cd5\u5219\u662f\u5c06\u5c0f\u7684\u5206\u5757 Layout \u7ec4\u5408\u6210\u6574\u4e2a Layout\u3002 Logical product \u7684\u4e25\u683c\u5b9a\u4e49\u4e3a\uff1a</p> \\[ A \\otimes B = (A, A^* \\circ B) \\] <p>A \u4e0e B \u7684\u4e58\u79ef\u6709\u4e24\u7ef4\uff0c\u7b2c\u4e00\u7ef4\u662f A \u672c\u8eab\uff0c\u7b2c\u4e8c\u7ef4\u662f A \u7684\u8865\u96c6\u548c B \u7684\u590d\u5408\u3002 \u4ee5\u4e0b\u662f\u4e24\u4e2a\u4e00\u7ef4\u4e58\u6cd5\u7684\u4f8b\u5b50\uff1a</p> <p></p> <p></p> <p>\u4ece\u4e8c\u7ef4\u5f00\u59cb\uff0clogical product \u53d8\u5f97\u53cd\u76f4\u89c9\u4e86\u3002 \u5728\u4e0b\u56fe\u4e2d\uff0c\u6211\u4eec\u5e0c\u671b\u628a 2x5 \u7684 Tile \u5728\u5217\u65b9\u5411\u4e0a\u91cd\u590d 3 \u6b21\uff0c\u5728\u884c\u65b9\u5411\u4e0a\u91cd\u590d 4 \u6b21\u3002 \u7136\u800c\uff0c\u8fd9\u4e2a\u64cd\u4f5c\u6240\u9700\u7684 B Layout \u662f <code>&lt;3:5, 4:6&gt;</code>\uff0c\u8fd9\u9700\u8981\u4ece A \u7684 Layout \u4e2d\u63a8\u5bfc\u51fa\u6765\uff0c\u4e14\u5e76\u4e0d\u76f4\u63a5\u3002</p> <p></p> <p>CuTe \u63d0\u4f9b\u4e86 <code>blocked_product</code> \u548c <code>raked_product</code>\uff0c\u6765\u7b80\u5316\u8fd9\u6837\u7684\u4e58\u6cd5\u64cd\u4f5c\u3002</p> <p></p> <p>Blocked product \u628a Layout A \u8fde\u7eed\u7684\u6309\u7167 Layout B \u8fdb\u884c\u6392\u5e03\u3002</p> <p></p> <p>Raked product \u628a Layout A \u4e2d\u6bcf\u4e2a\u5143\u7d20\u6309 Layout B \u8fdb\u884c\u6392\u5e03\uff0c\u518d\u6309 Layout A \u8fdb\u884c\u6392\u5e03\u3002 </p>"},{"location":"notes/cute/#tensor","title":"Tensor","text":""},{"location":"notes/cute/#tensor-creation","title":"Tensor Creation","text":"<p>Tensor \u7684\u521b\u5efa\u5206\u4e3a\u4e24\u79cd\uff1a</p> <ol> <li>\u5728\u5df2\u6709\u7684\u6808\u4e0a\u6570\u636e\uff08memory\uff09\u4e0a\u521b\u5efa\uff0cTensor \u4e0d\u62e5\u6709 memory\u3002</li> <li>\u5728\u5806\u4e0a\u521b\u5efa\u9759\u6001\u5927\u5c0f\u7684 memory\uff0cTensor \u62e5\u6709\u8be5 memory\u3002</li> </ol> <pre><code>// \u6808\u4e0a\u5bf9\u8c61\uff1a\u9700\u540c\u65f6\u6307\u5b9a\u7c7b\u578b\u548cLayout\uff0clayout\u5fc5\u987b\u662f\u9759\u6001shape\nTensor make_tensor&lt;T&gt;(Layout layout);\n\n// \u5806\u4e0a\u5bf9\u8c61\uff1a\u9700\u6307\u5b9apointer\u548cLayout\uff0clayout\u53ef\u52a8\u53ef\u9759\nTensor make_tensor(Pointer pointer, Layout layout);\n\n// \u6808\u4e0a\u5bf9\u8c61\uff0ctensor\u7684layout\u5fc5\u987b\u662f\u9759\u6001\u7684\nTensor make_tensor_like(Tensor tensor); \n\n// \u6808\u4e0a\u5bf9\u8c61\uff0ctensor\u7684layout\u5fc5\u987b\u662f\u9759\u6001\u7684\nTensor make_fragment_like(Tensor tensor);\n</code></pre>"},{"location":"notes/cute/#tensor-partition","title":"Tensor Partition","text":"<p>\u5728 GPU \u4e0a\uff0c\u6211\u4eec\u9700\u8981\u5c06\u4e00\u4e2a Tensor \u5212\u5206\u6210\u591a\u4e2a\u5757\uff0c\u8ba9\u6bcf\u4e2a SM \u6bcf\u6b21\u5904\u7406\u4e00\u4e2a\u5757\u3002 \u4f8b\u5982\uff0c\u6211\u4eec\u628a\u4e00\u4e2a\u5927\u5c0f\u4e3a [8, 24] \u7684 Tensor \u5212\u5206\u6210\u82e5\u5e72\u4e2a\u5927\u5c0f\u4e3a [4, 8] \u7684\u5757\uff1a</p> <pre><code>Tensor A = make_tensor(ptr, make_shape(8,24));  // (8,24)\nauto tiler = Shape&lt;_4,_8&gt;{};                    // (_4,_8)\n\nTensor tiled_a = zipped_divide(A, tiler);       // ((_4,_8),(2,3))\n</code></pre> <p>\u5728 kernel \u5185\u53ef\u4ee5\u6839\u636e block ID \u7d22\u5f15\u5f53\u524d SM \u7684\u6570\u636e\uff1a</p> <pre><code>Tensor cta_a = tiled_a(make_coord(_,_), make_coord(blockIdx.x, blockIdx.y));  // (_4,_8)\n</code></pre> <p>\u8fd9\u79cd\u7d22\u5f15\u65b9\u5f0f\u4e5f\u88ab\u5c01\u88c5\u8fdb\u4e86 <code>inner_partition(Tensor, Tiler, Coord)</code> \u6216\u8005 <code>local_tile(Tensor, Tiler, Coord)</code>\u3002</p> <p>\u53e6\u5916\u4e00\u79cd\u60c5\u51b5\u662f\uff0c\u5047\u8bbe\u6709 32 \u4e2a\u7ebf\u7a0b\uff0c\u6bcf\u4e2a\u7ebf\u7a0b\u5904\u7406\u7b2c\u4e00\u7ef4\u4e2d (4, 8) \u5bf9\u5e94\u7684\u67d0\u4e00\u5757\uff1a</p> <pre><code>Tensor thr_a = tiled_a(threadIdx.x, make_coord(_,_)); // (2,3)\n</code></pre> <p>\u4e5f\u53ef\u4ee5\u5199\u6210 <code>outer_partition(Tensor, Layout, Idx)</code> \u6216 <code>local_partition(Tensor, Layout, Idx)</code>\u3002</p>"},{"location":"notes/cute/#thread-value-partitioning","title":"Thread-Value partitioning","text":"<p>\u5047\u8bbe\u6211\u4eec\u6709\u4e00\u4e2a\u5c06\u7ebf\u7a0b\u6620\u5c04\u5230\u5750\u6807\u7684 Layout\uff0c \u6211\u4eec\u53ef\u4ee5\u5c06\u4e00\u4e2a Tensor \u4e0e\u8fd9\u4e2a Layout \u590d\u5408\uff0c\u6784\u6210\u80fd\u5c06\u7ebf\u7a0b\u6620\u5c04\u5230\u503c\u7684 Tensor\u3002</p> <pre><code>// Construct a TV-layout that maps 8 thread indices and 4 value indices\n//   to 1D coordinates within a 4x8 tensor\n// (T8,V4) -&gt; (M4,N8)\nauto tv_layout = Layout&lt;Shape &lt;Shape &lt;_2,_4&gt;,Shape &lt;_2, _2&gt;&gt;,\n                        Stride&lt;Stride&lt;_8,_1&gt;,Stride&lt;_4,_16&gt;&gt;&gt;{}; // (8,4)\n\n// Construct a 4x8 tensor with any layout\nTensor A = make_tensor&lt;float&gt;(Shape&lt;_4,_8&gt;{}, LayoutRight{});    // (4,8)\n// Compose A with the tv_layout to transform its shape and order\nTensor tv = composition(A, tv_layout);                           // (8,4)\n// Slice so each thread has 4 values in the shape and order that the tv_layout prescribes\nTensor  v = tv(threadIdx.x, _);                                  // (4)\n</code></pre>"},{"location":"notes/cute/#references","title":"References","text":"<p>https://github.com/NVIDIA/cutlass/blob/main/media/docs/cpp/cute/</p> <p>https://www.zhihu.com/people/reed-84-49/posts</p>"},{"location":"notes/cute_detail/","title":"CuTe Detail","text":""},{"location":"notes/cute_detail/#cute-mma","title":"CuTe MMA","text":""},{"location":"notes/cute_detail/#mma-operation","title":"MMA Operation","text":"<p>MMA Operation \u4f4d\u7f6e\u5728 <code>cute/arch/mma_sm*</code> \u6587\u4ef6\u4e2d\uff0c\u662f PTX \u6307\u4ee4\u7684\u5c01\u88c5\u3002 \u7c7b\u7684\u540d\u79f0\u91cc\u6307\u5b9a\u4e86 SM \u8ba1\u7b97\u80fd\u529b\uff0cMNK \u7684\u5f62\u72b6\uff0c<code>D = A x B + C</code> \u7684\u6570\u636e\u7c7b\u578b\uff0c\u548c AB \u77e9\u9635\u662f\u5426\u8f6c\u7f6e\u3002 N (normal) \u8868\u793a Col-major\uff0cT (transpose) \u8868\u793a Row-major\u3002 <code>fma</code> \u63a5\u53e3\u4e2d\u8c03\u7528 PTX \u6307\u4ee4\u8fd0\u7b97 <code>D = A x B + C</code>\u3002</p> <pre><code>// MMA 16x8x8 TN\nstruct SM80_16x8x8_F32F16F16F32_TN\n{\n  using DRegisters = float[4];\n  using ARegisters = uint32_t[2];\n  using BRegisters = uint32_t[1];\n  using CRegisters = float[4];\n\n  CUTE_HOST_DEVICE static void\n  fma(float         &amp; d0, float         &amp; d1, float         &amp; d2, float         &amp; d3,\n      uint32_t const&amp; a0, uint32_t const&amp; a1,\n      uint32_t const&amp; b0,\n      float const   &amp; c0, float const   &amp; c1, float const   &amp; c2, float const   &amp; c3)\n  {\n#if defined(CUTE_ARCH_MMA_SM80_ENABLED)\n    asm volatile(\n      \"mma.sync.aligned.m16n8k8.row.col.f32.f16.f16.f32 \"\n      \"{%0,  %1,  %2,  %3},\"\n      \"{%4,  %5},\"\n      \"{%6},\"\n      \"{%7,  %8,  %9,  %10};\\n\"\n      : \"=f\"(d0), \"=f\"(d1), \"=f\"(d2), \"=f\"(d3)\n      :  \"r\"(a0),  \"r\"(a1),\n         \"r\"(b0),\n         \"f\"(c0),  \"f\"(c1),  \"f\"(c2),  \"f\"(c3));\n#else\n    CUTE_INVALID_CONTROL_PATH(\"Attempting to use SM80_16x8x8_F32F16F16F32_TN without CUTE_ARCH_MMA_SM80_ENABLED\");\n#endif\n  }\n};\n</code></pre> <p>\u5bf9\u4e8e SM90 wgmma\uff0cAB \u77e9\u9635\u7684\u8f6c\u7f6e\u60c5\u51b5\u53d8\u4e3a\u5728\u6a21\u7248\u53c2\u6570\u4e2d\u4f20\u5165\u3002 \u7c7b\u7684\u540d\u79f0\u4e2d\u589e\u52a0\u4e86 AB \u77e9\u9635\u662f\u5728\u5bc4\u5b58\u5668 (R) \u6216 SMEM (S) \u7684\u6807\u8bc6\u3002 \u53ef\u4ee5\u6ce8\u610f\u5230\u4ec5\u6709 RS \u548c SS \u4e24\u79cd\u60c5\u51b5\u3002 \u5982\u679c A \u77e9\u9635\u5728\u5bc4\u5b58\u5668\u4e2d\uff0c\u5219\u53ea\u80fd\u662f K-major \u6392\u5e03 (Row-major)\u3002</p> <pre><code>// GMMA 64x8x16 F32+=F16*F16\ntemplate &lt;\n  GMMA::Major tnspA,\n  GMMA::Major tnspB,\n  GMMA::ScaleIn  scaleA = GMMA::ScaleIn::One,\n  GMMA::ScaleIn  scaleB = GMMA::ScaleIn::One\n&gt;\nstruct MMA_64x8x16_F32F16F16_RS\n{\n  using DRegisters = void;\n  using ARegisters = uint32_t[4];\n  using BRegisters = uint64_t[1];\n  using CRegisters = float[4];\n\n  static_assert(tnspA == GMMA::Major::K,\n      \"Register source operand A must have K major layout.\");\n\n  CUTE_HOST_DEVICE static void\n  fma(uint32_t const&amp; a0, uint32_t const&amp; a1, uint32_t const&amp; a2, uint32_t const&amp; a3,\n      uint64_t const&amp; desc_b,\n      float         &amp; d0, float         &amp; d1, float         &amp; d2, float         &amp; d3,\n      GMMA::ScaleOut const scale_D = GMMA::ScaleOut::One)\n  {\n#if defined(CUTE_ARCH_MMA_SM90A_ENABLED)\n    cutlass::arch::synclog_emit_wgmma_reg_smem(__LINE__, desc_b);\n    asm volatile(\n    \"{\\n\"\n      \".reg .pred p;\\n\"\n      \"setp.ne.b32 p, %9, 0;\\n\"\n      \"wgmma.mma_async.sync.aligned.m64n8k16.f32.f16.f16 \"\n      \"{%0,  %1,  %2,  %3},\"\n      \"{%4,  %5,  %6,  %7},\"\n      \" %8,\"\n      \" p,   %10, %11, %12;\\n\"\n    \"}\\n\"\n      : \"+f\"(d0), \"+f\"(d1), \"+f\"(d2), \"+f\"(d3)\n      :  \"r\"(a0),  \"r\"(a1),  \"r\"(a2),  \"r\"(a3),\n         \"l\"(desc_b),\n         \"r\"(int32_t(scale_D)), \"n\"(int32_t(scaleA)), \"n\"(int32_t(scaleB)), \"n\"(int32_t(tnspB)));\n#else\n    CUTE_INVALID_CONTROL_PATH(\"Attempting to use MMA_64x8x16_F32F16F16_RS without CUTE_ARCH_MMA_SM90A_ENABLED\");\n#endif\n  }\n};\n</code></pre>"},{"location":"notes/cute_detail/#mma-traits","title":"MMA Traits","text":"<p>MMA Traits \u4e2d\u63d0\u4f9b\u6bcf\u79cd MMA Operation \u7684\u57fa\u672c\u4fe1\u606f\uff1a</p> <pre><code>using ElementDVal =  // Logical D-value type\nusing ElementAVal =  // Logical A-value type\nusing ElementBVal =  // Logical B-value type\nusing ElementCVal =  // Logical C-value type\n\nusing Shape_MNK =    // Logical MxNxK shape of the MMA\n\nusing ThrID     =    // Logical thread id (tid) -&gt; tidx\n\nusing ALayout =      // (Logical thread id (tid), Logical value id (vid)) -&gt; Flat MK-coord\nusing BLayout =      // (Logical thread id (tid), Logical value id (vid)) -&gt; Flat NK-coord\nusing CLayout =      // (Logical thread id (tid), Logical value id (vid)) -&gt; Flat MN-coord\n</code></pre> <p>ThrID \u8868\u793a\u4e86\u9700\u8981\u53c2\u4e0e MMA \u6307\u4ee4\u7684\u7ebf\u7a0b\u7684 Layout\u3002 \u4f8b\u5982\uff0c\u5728 SM70 \u4e0a\uff0c\u6709 <code>ThrID = (_4, _2):(_1, _16)</code>\uff0c\u5373 8 \u4e2a\u7ebf\u7a0b\u53d1\u8d77\u4e00\u4e2a MMA \u6307\u4ee4\u3002 \u800c\u5728 SM90 WGMMA \u4e0a\uff0c\u6709 <code>ThrID = (_128):(_1)</code>\uff0c\u5373\u4e00\u4e2a warp group 128 \u4e2a\u7ebf\u7a0b\u53d1\u8d77\u4e00\u4e2a WGMMA \u6307\u4ee4\u3002 ABC \u7684 Layout \u662f\u4e00\u4e2a\u6620\u5c04 <code>(T, V) -&gt; Idx</code>\uff0c\u5c06\u7b2c T \u4e2a\u903b\u8f91\u7ebf\u7a0b\u4e2d\u7684\u7b2c V \u4e2a\u503c\u7684\u903b\u8f91\u5750\u6807\u6620\u5c04\u81f3\u6309 Col-major \u6392\u5e03\u5c55\u5e73\u7684\u7269\u7406\u5750\u6807\u3002</p> <pre><code>// (T32,V2) -&gt; (M8,N8)\nusing SM80_8x8_Row  = Layout&lt;Shape &lt;Shape &lt; _4,_8&gt;,_2&gt;,\n                             Stride&lt;Stride&lt;_16,_1&gt;,_8&gt;&gt;;\n// (T32,V4) -&gt; (M16,N8)\nusing SM80_16x8_Row = Layout&lt;Shape &lt;Shape &lt; _4,_8&gt;,Shape &lt; _2,_2&gt;&gt;,\n                             Stride&lt;Stride&lt;_32,_1&gt;,Stride&lt;_16,_8&gt;&gt;&gt;;\ntemplate &lt;&gt;\nstruct MMA_Traits&lt;SM80_16x8x8_F16F16F16F16_TN&gt;\n{\n  using ValTypeD = half_t;\n  using ValTypeA = half_t;\n  using ValTypeB = half_t;\n  using ValTypeC = half_t;\n\n  using Shape_MNK = Shape&lt;_16,_8,_8&gt;;\n  using ThrID   = Layout&lt;_32&gt;;\n  using ALayout = SM80_16x8_Row;\n  using BLayout = SM80_8x8_Row;\n  using CLayout = SM80_16x8_Row;\n};\n</code></pre>"},{"location":"notes/cute_detail/#mma-atom","title":"MMA Atom","text":"<p>MMA Atom \u7ee7\u627f MMA Traits\uff0c\u4f5c\u4e3a\u4e00\u6761 MMA \u6307\u4ee4\u7684\u6700\u7ec8\u5c01\u88c5\u3002</p> <pre><code>template &lt;class MMAOperation, class... Args&gt;\nstruct MMA_Atom&lt;MMA_Traits&lt;MMAOperation, Args...&gt;&gt;\n  : MMA_Traits&lt;MMAOperation, Args...&gt;\n{\n  using MMA_Op = MMAOperation;\n  using Traits = MMA_Traits&lt;MMAOperation, Args...&gt;;\n\n  // Element value types from the MMA_Traits\n  using ValTypeD = typename Traits::ValTypeD;\n  using ValTypeA = typename Traits::ValTypeA;\n  using ValTypeB = typename Traits::ValTypeB;\n  using ValTypeC = typename Traits::ValTypeC;\n\n  // Thr-Val layouts from the MMA_Traits\n  using Shape_MNK  = typename Traits::Shape_MNK;\n  using ThrID      = typename Traits::ThrID;\n  using LayoutC_TV = typename Traits::CLayout;\n  using LayoutA_TV = typename Traits::ALayout;\n  using LayoutB_TV = typename Traits::BLayout;\n\n  // Fragment value types from the MMA_Traits (optional, defaults to Val type)\n  using FrgTypeD = typename detail::FrgTypeC_or_Default&lt;Traits&gt;::type;\n  using FrgTypeA = typename detail::FrgTypeA_or_Default&lt;Traits&gt;::type;\n  using FrgTypeB = typename detail::FrgTypeB_or_Default&lt;Traits&gt;::type;\n  using FrgTypeC = typename detail::FrgTypeC_or_Default&lt;Traits&gt;::type;\n...\n</code></pre>"},{"location":"notes/cute_detail/#tiledmma","title":"TiledMMA","text":"<p>TiledMMA \u5c06\u591a\u4e2a MMA Atom \u7ec4\u5408\u8d77\u6765\uff0c\u8868\u8fbe\u4f7f\u7528\u591a\u4e2a MMA \u6307\u4ee4\u6765\u8ba1\u7b97\u66f4\u5927\u7684\u4e00\u5757 GEMM\u3002</p> <pre><code>// @tparam MMA_Atom The MMA_Atom to use in the TiledMMA\n// @tparam AtomLayoutMNK The MNK-tiling of the Atom to be performed.\n// @tparam PermuationsMNK Permutations to apply to each MNK-mode before tiling for the Atom.\ntemplate &lt;class MMA_Atom,\n          class AtomLayoutMNK,\n          class PermutationMNK = Tile&lt;Underscore,Underscore,Underscore&gt;&gt;\nstruct TiledMMA : MMA_Atom\n{\n  using Atom           = MMA_Atom;\n  using AtomShape_MNK  = typename MMA_Atom::Shape_MNK;\n  using AtomThrID      = typename MMA_Atom::ThrID;\n  using AtomLayoutC_TV = typename MMA_Atom::LayoutC_TV;\n  using AtomLayoutA_TV = typename MMA_Atom::LayoutA_TV;\n  using AtomLayoutB_TV = typename MMA_Atom::LayoutB_TV;\n...\n</code></pre> <p>\u53ea\u4f20\u5165 MMA Atom \u6a21\u677f\u53c2\u6570\u65f6\uff0c\u9ed8\u8ba4 AtomLayoutMNK \u662f <code>(_1, _1, _1)</code>\uff0c\u5373\u53ea\u6709\u4e00\u4e2a MMA Atom\u3002</p> <pre><code>MMA_Atom mma = MMA_Atom&lt;SM70_8x8x4_F32F16F16F32_NT&gt;{};\n</code></pre> <p>\u7b49\u4ef7\u4e8e MNK \u5927\u5c0f\u4e3a <code>(_8, _8, _4)</code> \u7684\u4e00\u4e2a MMA \u6307\u4ee4\uff1a</p> <pre><code>TiledMMA mma = make_tiled_mma(SM70_8x8x4_F32F16F16F32_NT{},\n                              Layout&lt;Shape&lt;_1,_1,_1&gt;&gt;{},   // Layout of Atoms\n                              Tile&lt;_8,_8,_4&gt;{});           // Tiler\n</code></pre> <p>\u4e5f\u53ef\u4ee5\u5c06\u591a\u4e2a MMA Atom \u7ec4\u6210\u4e00\u4e2a\u66f4\u5927\u7684 Tiled MMA\u3002 \u7528\u56db\u4e2a 8 \u7ebf\u7a0b 8x8x4 \u7684 MMA Atom \u7ec4\u6210\u4e00\u4e2a 32 \u7ebf\u7a0b 16x16x4 \u7684 Tiled Atom\uff1a</p> <pre><code>TiledMMA mma = make_tiled_mma(SM70_8x8x4_F32F16F16F32_NT{},\n                              Layout&lt;Shape &lt;_2,_2&gt;,\n                                      Stride&lt;_2,_1&gt;&gt;{});   // 2x2 n-major layout of Atoms\nprint_latex(mma);\n</code></pre> <p>\u540c\u6837\u4f7f\u7528 32 \u7ebf\u7a0b\uff0c\u8fd8\u53ef\u4ee5\u8fdb\u4e00\u6b65\u6269\u5f20\u5230 32x32x4 \u7684 Tiled Atom\uff1a</p> <pre><code>TiledMMA mma = make_tiled_mma(SM70_8x8x4_F32F16F16F32_NT{},\n                              Layout&lt;Shape &lt;_2,_2&gt;,\n                                      Stride&lt;_2,_1&gt;&gt;{},  // 2x2 n-major layout of Atoms\n                              Tile&lt;_32,_32,_4&gt;{});      // 32x32x4 tiler\nprint_latex(mma);\n</code></pre> <p>make_tiled_mma \u7684\u7b2c\u4e8c\u4e2a\u53c2\u6570\u8868\u793a\u53c2\u4e0e MMA Atom \u7ebf\u7a0b/\u7ebf\u7a0b\u5757\u7684 Layout\u3002 \u7b2c\u4e09\u4e2a\u53c2\u6570 PermuteMNK \u8868\u793a\u6574\u4e2a MMA \u7684\u5927\u5c0f\u548c\u6392\u5e03\uff0c\u8fd9\u91cc 32x32 \u7684 C \u77e9\u9635\u4e2d\uff0c\u6bcf\u4e2a 16x16 \u7684\u5757\u90fd\u662f\u4e0a\u9762\u7684 16x16 Tiled MMA \u7684\u6392\u5e03\u3002 \u4f46\u662f\uff0c\u8fd9\u4f1a\u5bfc\u81f4 AB \u77e9\u9635\u7684\u8bfb\u53d6\u4e0d\u662f\u8fde\u7eed\u6392\u5e03\u7684\u3002 M \u7ef4\u5ea6\u4e0a\uff0c\u7b2c 0 \u4e2a MMA Atom \u5904\u7406 M \u7ef4\u5ea6\u4e0a\u7684\u7b2c 0\u30012 \u4e2a 8x8x4 \u7684 MMA\u3002 \u6211\u4eec\u4f1a\u5e0c\u671b\u8fd9\u4e2a MMA Atom \u8fde\u7eed\u5904\u7406 M \u7ef4\u5ea6\u4e0a\u7684\u7b2c 0\u30011 \u4e2a 8x8x4 \u7684 MMA\u3002 \u56e0\u6b64\uff0c\u7b2c\u4e09\u4e2a\u53c2\u6570 PermuteMNK \u5728 M \u7ef4\u5ea6\u4e0a\u8fdb\u884c\u4e86\u53d8\u5316\uff1a</p> <pre><code>TiledMMA mma = make_tiled_mma(SM70_8x8x4_F32F16F16F32_NT{},\n                              Layout&lt;Shape &lt;_2,_2&gt;,\n                                      Stride&lt;_2,_1&gt;&gt;{},       // 2x2 n-major layout of Atoms\n                              Tile&lt;Layout&lt;Shape &lt;_4,_4,_2&gt;,\n                                          Stride&lt;_1,_8,_4&gt;&gt;, // Permutation on M, size 32\n                                    _32,                      // Permutation on N, size 32 identity\n                                    _4&gt;{});                   // Permutation on K, size 4 identity\nprint_latex(mma);\n</code></pre>"},{"location":"notes/flash-attention/","title":"From Online Softmax to Flash Attention","text":""},{"location":"notes/flash-attention/#online-softmax","title":"Online Softmax","text":""},{"location":"notes/flash-attention/#3-pass-safe-softmax","title":"3-Pass Safe Softmax","text":"<p>Softmax \u7684\u6807\u51c6\u5b9a\u4e49\u662f $$ y_i = \\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)} $$ \u4f46\u662f\u5f53 \\(x_i\\) \u5f88\u5927\u65f6\uff0c\\(\\exp(x_i)\\) \u4f1a\u6ea2\u51fa\u3002\u4e3a\u4e86\u907f\u514d\u8fd9\u79cd\u60c5\u51b5\uff0c\u901a\u5e38\u4f1a\u5148\u8ba1\u7b97\u8f93\u5165\u7684\u6700\u5927\u503c \\(m = \\max_i (x_i)\\)\uff0c\u7136\u540e\u4f7f\u7528 $$ y_i = \\frac{\\exp(x_i - m)}{\\sum_j \\exp(x_j - m)} $$ \u6765\u8ba1\u7b97 softmax\uff0c\u786e\u4fdd \\(x_i - m \\le 0\\)\uff0c\u4ece\u800c\u907f\u514d\u6ea2\u51fa\u3002 \u6734\u7d20\u7684\u5b9e\u73b0\u8fd9\u79cd\u65b9\u6cd5\u9700\u8981\u904d\u5386\u4e09\u6b21\u8f93\u5165\uff1a</p> <ol> <li>\u8ba1\u7b97\u6700\u5927\u503c \\(m_i = \\max(m_{i-1}, x_i)\\)\u3002</li> <li>\u8ba1\u7b97\u5f52\u4e00\u5316\u56e0\u5b50 \\(d_i = d_{i-1} + \\exp(x_j - m_N)\\)\u3002</li> <li>\u8ba1\u7b97\u6700\u7ec8\u7684 softmax \u8f93\u51fa \\(y_i = \\exp(x_i - m_N) / d_N\\)\u3002</li> </ol>"},{"location":"notes/flash-attention/#2-pass-safe-online-softmax","title":"2-Pass Safe Online Softmax","text":"<p>\u6211\u4eec\u53ef\u4ee5\u628a\u524d\u4e24\u6b21\u904d\u5386\u878d\u5408\u6210\u4e00\u6b21\uff0c\u5728\u4e00\u6b21\u904d\u5386\u4e2d\u540c\u65f6\u66f4\u65b0\u6700\u5927\u503c\u548c\u5f52\u4e00\u5316\u56e0\u5b50\uff0c\u5b9e\u73b0 2-Pass Safe Softmax\uff1a</p> <ol> <li>\u8ba1\u7b97\u6700\u5927\u503c\u548c\u5f52\u4e00\u5316\u56e0\u5b50<ul> <li>\\(m_i = \\max(m_{i-1}, x_i)\\)</li> <li>\\(d_i = d_{i-1} \\cdot \\exp(m_{i-1} - m_i) + \\exp(x_i - m_i)\\)</li> </ul> </li> <li>\u8ba1\u7b97\u6700\u7ec8\u7684 softmax \u8f93\u51fa \\(y_i = \\exp(x_i - m_N) / d_N\\)\u3002</li> </ol> <p>\u628a 3-Pass \u4f18\u5316\u6210 2-Pass \u6709\u4ec0\u4e48\u6536\u76ca\u5417\uff1f\u4ece\u8ba1\u7b97\u91cf\u4e0a\u770b\uff0c2-Pass \u751a\u81f3\u8fd8\u8981\u6bd4 3-Pass \u591a\u4e00\u4e9b\u8ba1\u7b97\u3002\u4f46\u4ece\u5168\u5c40\u5185\u5b58\u8bbf\u95ee\u7684\u89d2\u5ea6\u6765\u770b\uff0c2-Pass \u53ea\u9700\u8981\u904d\u5386\u4e24\u6b21\u8f93\u5165\uff0c\u800c 3-Pass \u9700\u8981\u904d\u5386\u4e09\u6b21\uff0c\u51cf\u5c11\u4e86\u4e00\u6b21\u5185\u5b58\u8bbf\u95ee\u3002\u5bf9\u4e8e softmax \u8fd9\u79cd memory-bound \u7684\u64cd\u4f5c\u6765\u8bf4\uff0c\u51cf\u5c11\u5185\u5b58\u8bbf\u95ee\u5f80\u5f80\u80fd\u5e26\u6765\u66f4\u5927\u7684\u6027\u80fd\u63d0\u5347\u3002</p>"},{"location":"notes/flash-attention/#flashattention-v1","title":"FlashAttention V1","text":""},{"location":"notes/flash-attention/#1-pass-attention","title":"1-Pass Attention","text":"<p>\u5ffd\u7565\u7f29\u653e\u5e38\u6570 \\(\\sqrt{d}\\)\uff0c\u6807\u51c6 Attention \u7684\u8ba1\u7b97\u516c\u5f0f\u662f $$ O = \\text{Softmax}(QK^T)V $$ \u5176\u4e2d \\(Q, K, V\\) \u7684\u5f62\u72b6\u5206\u522b\u662f \\((M, D)\\)\uff0c\\((N, D)\\)\uff0c\\((N, D)\\)\u3002 \u90a3\u4e48\u5728 2-Pass Softmax \u7684\u57fa\u7840\u4e0a\uff0c\u6bcf\u4e00\u884c \\(O_i\\) \u7684\u8ba1\u7b97\u53ef\u4ee5\u5206\u89e3\u6210\u4ee5\u4e0b\u4e24\u6b65\uff1a</p> <ol> <li>\u8ba1\u7b97 \\(QK^T\\) \u7684\u6700\u5927\u503c\u548c\u5f52\u4e00\u5316\u56e0\u5b50<ul> <li>\\(x_j = Q_i K_j^T\\)</li> <li>\\(m_j = \\max(m_{j-1}, x_j)\\)</li> <li>\\(d_j = d_j \\cdot \\exp(m_{j-1} - m_j) + \\exp(x_j - m_j)\\)</li> </ul> </li> <li>\u8ba1\u7b97 \\(O\\)<ul> <li>\\(O_j = O_{j-1} + \\frac{\\exp(x_j - m_D)}{d_N} V_j\\)</li> </ul> </li> </ol> <p>\u867d\u7136 Softmax \u4e0d\u80fd\u88ab\u8fdb\u4e00\u6b65\u4f18\u5316\u6210 1-Pass\uff0c\u4f46\u662f Attention \u53ea\u9700\u8981\u5f97\u5230\u6700\u7ec8\u7684\u8f93\u51fa \\(O\\)\uff0c\u800c\u4e0d\u9700\u8981\u4e2d\u95f4\u7684 Softmax \u7ed3\u679c\u3002\u5b9e\u9645\u4e0a Attention \u662f\u53ef\u4ee5\u88ab\u4f18\u5316\u6210 1-Pass \u7684\u3002 \u5b9a\u4e49 \\(O'_j\\)</p> \\[ O'_j = \\sum_{k=1}^{j} \\frac{\\exp(x_k-m_j)}{d_j} V_k \\] <p>\u53ef\u4ee5\u5f97\u5230\u9012\u63a8\u5f0f\uff1a</p> \\[ O'_j = O'_{j-1} \\cdot \\exp(m_{j-1}-m_j) \\cdot \\frac{d_{j-1}}{d_j} + \\frac{\\exp(x_j - m_j)}{d_j} V_j \\] <p>\u7531\u6b64\uff0c\u6211\u4eec\u53ef\u4ee5\u5728\u4e00\u6b21\u904d\u5386\u4e2d\u540c\u65f6\u8ba1\u7b97 \\(m_j, d_j, O'_j\\)\uff0c\u4ece\u800c\u5b9e\u73b0 1-Pass Attention\uff1a</p> <ol> <li>\u8ba1\u7b97 \\(QK^T\\) \u7684\u6700\u5927\u503c\u3001\u5f52\u4e00\u5316\u56e0\u5b50\u548c\u8f93\u51fa<ul> <li>\\(x_j = Q_i K_j^T\\)</li> <li>\\(m_j = \\max(m_{j-1}, x_j)\\)</li> <li>\\(d_j = d_j \\cdot \\exp(m_{j-1} - m_j) + \\exp(x_j - m_j)\\)</li> <li>\\(O'_j = O'_{j-1} \\cdot \\exp(m_{j-1}-m_j) \\cdot \\frac{d_{j-1}}{d_j} + \\frac{\\exp(x_j - m_j)}{d_j} V_j\\)</li> </ul> </li> </ol>"},{"location":"notes/flash-attention/#tiling","title":"Tiling","text":"<p>\u6211\u4eec\u628a K \u548c V \u5206\u5757\uff0c\u6bcf\u5757\u5927\u5c0f\u4e3a \\((n, D)\\)\uff0c\u518d\u628a Q \u6bcf\u5757\u5206\u4e3a \\((m, D)\\)\u3002 \u6211\u4eec\u5148\u5206\u6790\u5bf9\u4e8e \\(Q_i, K_j, V_j\\) \u5206\u5757\u7684\u8ba1\u7b97\u8fc7\u7a0b\uff1a</p> <ol> <li>\u8ba1\u7b97 \\(QK^T\\)<ul> <li>\\(X = Q_i K_j^T \\in \\R^{(m,n)}\\)</li> </ul> </li> <li>\u5bf9\u6bcf\u884c\u6c42<ul> <li>\\(\\tilde{m_i}[k] = \\max_l(X_{k,l}) \\in \\R^m\\)</li> <li>\\(P_{k,l} = \\exp(X_{k,l} - \\tilde{m_i}) \\in \\R^{(m,n)}\\)</li> <li>\\(d_i[k] = \\sum_l P_{k,l} \\in \\R^m\\)</li> </ul> </li> <li>\u5bf9\u6bcf\u884c\u66f4\u65b0\u6700\u5927\u503c\u548c\u5f52\u4e00\u5316\u56e0\u5b50<ul> <li>\\(m_i[k] = \\max(m'_i[k], \\tilde{m_i}[k])\\)</li> <li>\\(d_i[k] = d'_i[k] \\cdot \\exp(m'_i[k] - m_i[k]) + d_i[k] \\cdot \\exp(\\tilde{m_i}[k] - m_i[k])\\)</li> </ul> </li> <li>\u5bf9\u6bcf\u884c\u8ba1\u7b97\u8f93\u51fa<ul> <li>$O_i[k] = d_i^{-1} ((d'_i \\cdot \\exp(m'_i - m_i)) \\cdot O_i[k] + \\exp(\\tilde{m_i} - m_i) \\cdot PV) $</li> </ul> </li> <li>\u7ed9\u4e0b\u4e2a\u5757\u66f4\u65b0\u8fd9\u4e2a\u5757\u7684\u6700\u5927\u503c\u548c\u5f52\u4e00\u5316\u56e0\u5b50<ul> <li>\\(m'_i = m\\)</li> <li>\\(d'_i = d\\)</li> </ul> </li> </ol> <p>FlashAttention V1 \u628a K \u548c V \u7684\u5206\u5757\u904d\u5386\u653e\u5728\u5916\u5c42\u5faa\u73af\uff0c\u628a Q \u7684\u5206\u5757\u904d\u5386\u653e\u5728\u5185\u5c42\u5faa\u73af\u3002</p>"},{"location":"notes/flash-attention/#flashattention-v2","title":"FlashAttention V2","text":"<ol> <li> <p>\u51cf\u5c11 Cuda Core \u8ba1\u7b97</p> <p>\u5728 V1\uff0c\u6bcf\u4e2a tile \u4e2d \\(O_i\\) \u7684\u8ba1\u7b97\u90fd\u5305\u542b\u4e24\u6b21 rescale \u64cd\u4f5c\uff1a\u4e58 \\(d'\\) \u548c\u9664 \\(d\\)\u3002 V2 \u5728\u6bcf\u4e2a tile \u4e2d\u4ec5\u4fdd\u7559\u4e58 \\(d'\\) \u7684 rescale\uff0c\u628a\u6240\u6709\u7684 \\(d\\) \u7684 rescale \u4e00\u8d77\u653e\u5728\u6700\u540e\u3002</p> </li> <li> <p>\u8c03\u6362\u5faa\u73af\u987a\u5e8f</p> <p>V1 \u5148\u5faa\u73af K \u548c V\uff0c\u540e\u5faa\u73af Q \u7684\u505a\u6cd5\u4f7f\u5f97\u6bcf\u6b21\u5185\u5faa\u73af\u90fd\u9700\u8981\u53cd\u590d\u5411 HBM \u8bbf\u5b58 \\(O_i, m'_i, d'_i\\)\u3002 V2 \u8c03\u6362\u4e86\u5185\u5916\u5faa\u73af\u7684\u987a\u5e8f\uff0c\u5148\u5faa\u73af Q\uff0c\u518d\u5faa\u73af K \u548c V\uff0c\u53ea\u5728\u6bcf\u6b21\u5916\u5faa\u73af\u4e2d\u5411 HBM \u8bbf\u5b58 \\(O_i, m'_i, d'_i\\)\u3002</p> </li> <li> <p>\u589e\u52a0 Sequence Length \u7ef4\u5ea6\u5e76\u884c</p> <p>V1 \u53ea\u5728 Batch Size \u548c Head Num \u7ef4\u5ea6\u4e0a\u5e76\u884c\uff0cV2 \u589e\u52a0\u4e86 Sequence Length \u7ef4\u5ea6\u7684\u5e76\u884c</p> </li> </ol>"},{"location":"notes/flash-attention/#flashattention-v3","title":"FlashAttention V3","text":"<p>TODO</p>"},{"location":"notes/flash-attention/#flashattention-vs-sdpa","title":"FlashAttention vs SDPA","text":"<p>FlashAttention \u662f\u5426\u4e00\u5b9a\u6bd4\u6807\u51c6\u7684 SDPA \u5feb\uff1f\u7b54\u6848\u662f\u5426\u5b9a\u7684\u3002 FlashAttention \u5e0c\u671b\u901a\u8fc7\u51cf\u5c11\u5168\u5c40\u5185\u5b58\u8bbf\u95ee\u6765\u63d0\u5347\u6027\u80fd\uff0cFA1 \u8bba\u6587\u4e2d\u5176\u5b9e\u7ed9\u51fa\u4e86 FlashAttention \u548c SDPA \u8bbf\u5b58\u91cf\u7684\u6e10\u8fdb\u5206\u6790\uff1a</p> Method Memory Access SDPA \\(\\Theta(Nd + N^2)\\) FlashAttention \\(\\Theta(N^2 d^2 M^{-1})\\) <p>\u5176\u4e2d \\(N\\) \u662f <code>seq_len</code>\uff0c\\(d\\) \u662f <code>head_dim</code>\uff0c\\(M\\) \u662f GPU \u7684 SRAM \u5927\u5c0f\u3002 \u5bf9\u4e8e\u4e3b\u6d41\u7684\u6a21\u578b\u548c\u63a8\u7406\u6846\u67b6\uff0c<code>seq_len</code> \u4e00\u822c\u5728\u51e0\u5343\u4ee5\u5185\uff088192\uff09\uff0c<code>head_dim</code> \u662f 64/128\uff0cGPU \u7684 SRAM \u5927\u6982\u662f 200kB\uff08A100: 192kB, H100: 228kB\uff09\u3002 \u8fd9\u65f6 \\(d^2 M^{-1}\\) \u662f\u8fdc\u5c0f\u4e8e 1 \u7684\uff0c\u6240\u4ee5 FlashAttention \u7684\u8bbf\u5b58\u91cf\u8981\u8fdc\u5c0f\u4e8e SDPA\u3002</p> <p>\u4f46\u662f\u5982\u679c <code>head_dim</code> \u6bd4\u8f83\u5927\uff0c\u6bd4\u5982 256 \u6216 512\uff0c\u6216\u8005 SRAM \u6bd4\u8f83\u5c0f\uff0c\u6bd4\u5982\u5728\u65e7\u67b6\u6784\u7684 GPU \u4e0a\u8fd0\u884c\uff0cFlashAttention \u7684\u8bbf\u5b58\u91cf\u53ef\u80fd\u4f1a\u5927\u4e8e SDPA\u3002</p>"},{"location":"notes/flash-attention/#flashattention-v4","title":"FlashAttention V4","text":"<p>Tri Dao \u5df2\u7ecf\u9884\u544a FlashAttention V4 \u4e86\uff0c\u4f46\u8fd8\u672a\u6b63\u5f0f\u53d1\u5e03\u3002</p>"},{"location":"notes/mla/","title":"Multi-head Latent Attention (MLA)","text":""},{"location":"notes/mla/#transformers","title":"Transformers","text":""},{"location":"notes/mla/#deepseekv3-configuration","title":"DeepSeekV3 Configuration","text":"<pre><code>hidden_size = 7168\nq_lora_rank = 1536\nnum_heads   = 128\nqk_nope_head_dim = 128\nqk_rope_head_dim = 64\nqk_head_dim = qk_nope_head_dim + qk_rope_head_dim = 192\nv_head_dim = 128\n</code></pre>"},{"location":"notes/mla/#deepseekv3-attention","title":"DeepSeekV3 Attention","text":"<pre><code>query_shape = (batch_size, seq_length, -1, self.qk_head_dim)\nkey_shape = (batch_size, seq_length, -1, self.qk_nope_head_dim + self.v_head_dim)\n\nif self.q_lora_rank is None:\n    q_states = self.q_proj(hidden_states)  # [b, s, h * qk_head_dim]\nelse:\n    q_states = self.q_b_proj(self.q_a_layernorm(self.q_a_proj(hidden_states)))\n        # q_a_proj: [b, s, d] -&gt; [b, s, q_lora_rank]   (16x down_proj)\n        # q_b_proj: [b, s, q_lora_rank] -&gt; [b, s, h * qk_head_dim]  (16x up_proj)\n\nq_states = q_states.view(query_shape).transpose(1, 2)  # [b, h, s, qk_head_dim]\nq_pass, q_rot = torch.split(q_states, [self.qk_nope_head_dim, self.qk_rope_head_dim], dim=-1)\n    # q_pass: [b, h, s, qk_nope_head_dim]\n    # q_rot:  [b, h, s, qk_rope_head_dim]\n\ncompressed_kv = self.kv_a_proj_with_mqa(hidden_states)  # [b, s, kv_lora_rank + qk_rope_head_dim]\nk_pass, k_rot = torch.split(compressed_kv, [self.kv_lora_rank, self.qk_rope_head_dim], dim=-1)\n    # k_pass: [b, s, kv_lora_rank]\n    # k_rot:  [b, s, qk_rope_head_dim]\n\nk_pass = self.kv_b_proj(self.kv_a_layernorm(k_pass)).view(key_shape).transpose(1, 2)\n    # k_pass: [b, h, s, qk_nope_head_dim + v_head_dim]\nk_pass, value_states = torch.split(k_pass, [self.qk_nope_head_dim, self.v_head_dim], dim=-1)\n    # k_pass:       [b, h, s, qk_nope_head_dim]\n    # value_states: [b, h, s, v_head_dim]\n\nk_rot = k_rot.view(batch_size, 1, seq_length, self.qk_rope_head_dim)  # [b, 1, s, qk_rope_head_dim]\n\ncos, sin = position_embeddings\nif self.config.rope_interleave:  # support using interleaved weights for efficiency\n    q_rot, k_rot = apply_rotary_pos_emb_interleave(q_rot, k_rot, cos, sin)\nelse:\n    q_rot, k_rot = apply_rotary_pos_emb(q_rot, k_rot, cos, sin)\nk_rot = k_rot.expand(*k_pass.shape[:-1], -1)  # [b, h, s, qk_rope_head_dim]\n\nquery_states = torch.cat((q_pass, q_rot), dim=-1)  # [b, h, s, qk_head_dim]\nkey_states = torch.cat((k_pass, k_rot), dim=-1)  # [b, h, s, qk_head_dim]\n\n# Normal attention\nattn_output, attn_weights = attention_interface(\n    self,\n    query_states,\n    key_states,\n    value_states,\n    attention_mask,\n    dropout=0.0 if not self.training else self.attention_dropout,\n    scaling=self.scaling,\n    **kwargs,\n)\n</code></pre> <p>MLA \u7684 KV Cache \u91cc\u5b58\u7684\u662f <code>compressed_kv</code> \u548c <code>k_rot</code>\uff0c\u5927\u5927\u51cf\u5c11\u4e86 KV Cache \u7684\u6240\u9700\u7a7a\u95f4\u3002 \u4ee5 DeepSeekV3 \u4e3a\u4f8b\uff0c\u6bcf\u4e2a layer \u6bcf\u4e2a token \u4ec5\u9700\u4e00\u4e2a\u957f\u5ea6\u4e3a 192 \u7684 Cache\u3002</p>"},{"location":"notes/mla/#_1","title":"\u77e9\u9635\u5438\u6536","text":"<p>\u8fd9\u91cc\u77e9\u9635\u5438\u6536\u5e76\u975e\u76f4\u63a5\u5c06\u4e24\u4e2a\u8fde\u7eed\u7684\u77e9\u9635\u4e58\u5408\u5e76\u6210\u4e00\u4e2a\u77e9\u9635\u4e58\uff08\u8fd9\u6837\u505a\u4e27\u5931\u4e86 LoRA \u7684\u610f\u4e49\uff09\uff0c\u800c\u662f\u4ea4\u6362\u77e9\u9635\u4e58\u7684\u8ba1\u7b97\u987a\u5e8f\u3002</p>"},{"location":"notes/mla/#wuk-wuq","title":"\u5438\u6536 \\(W^{UK}\\) \u548c \\(W^{UQ}\\)","text":"\\[ q^\\top k = (W^{UQ} c_t^Q)^\\top (W^{UK} c_t^{KV}) = \\left({c^Q}^\\top {W^{UQ}}^\\top W^{UK}\\right) c^{KV} = \\left({W^{UK}}^\\top W^{UQ} c^Q \\right)^\\top c^{KV} \\] <p>\u5176\u4e2d \\(W^{UQ}\\) \u7684\u5f62\u72b6\u662f <code>[h * qk_head_dim (24576), q_lora_rank (1536)]</code>\uff0c \\(W^{UK}\\) \u7684\u5f62\u72b6\u662f <code>[h * qk_head_dim (24576), kv_lora_rank (512)]</code>\u3002</p> <p>\u77e9\u9635\u5438\u6536\u540e\uff0c\u53ef\u4ee5\u76f4\u63a5\u628a \\(c_t^{KV}\\) \u770b\u4f5c\u662f \\(K\\) \u8fdb\u884c Attention \u8ba1\u7b97\uff0c\u800c \\(c^{KV}\\) \u53c8\u662f\u6bcf\u4e2a\u5934\u5171\u7528\u7684\u3002 \u56e0\u6b64\u539f\u5148\u7684 128 heads 128+64 head_dim \u7684 MHA \u8f6c\u5316\u4e3a\u4e86 128 heads 512+64 head_dim \u7684 MQA\uff0c\u51cf\u5c0f\u4e86\u8bbf\u5b58\u91cf\uff0c\u4f46\u589e\u52a0\u4e86\u8ba1\u7b97\u91cf\u3002</p> <p>\u6211\u4eec\u5728 prefill \u65f6\u4f7f\u7528\u8ba1\u7b97\u5f3a\u5ea6\u8f83\u5c0f\uff0c\u8bbf\u5b58\u91cf\u66f4\u5927\u7684 MHA\u3002\u5728 decode \u65f6\u4f7f\u7528\u8ba1\u7b97\u5f3a\u5ea6\u8f83\u5927\uff0c\u8bbf\u5b58\u91cf\u66f4\u5c0f\u7684 MQA\u3002</p>"},{"location":"notes/mla/#wuv-wo","title":"\u5438\u6536 \\(W^{UV}\\) \u548c \\(W^O\\)","text":"<p>\u8fd9\u4e2a\u5438\u6536\u8fc7\u7a0b\u7a0d\u5fae\u590d\u6742\u4e00\u4e9b\u3002</p> <pre><code>v_t = einsum('hdc,blc-&gt;blhd', W_UV, c_t_KV) # (1)\no   = einsum('bqhl,blhd-&gt;bqhd', attn_weights, v_t)     # (2)\nu   = einsum('hdD,bhqd-&gt;bhD', W_o, o)       # (3)\n\n# \u5c06\u4e0a\u8ff0\u4e09\u5f0f\u5408\u5e76\uff0c\u5f97\u5230\u603b\u7684\u8ba1\u7b97\u8fc7\u7a0b\nu   = einsum('hdc,blc,bqhl,hdD-&gt;bhD', W_UV, c_t_KV, attn_weights, W_o)\n\n# \u5229\u7528\u7ed3\u5408\u5f8b\u6539\u53d8\u8ba1\u7b97\u987a\u5e8f\no_  = einsum('bhql,blc-&gt;bhqc', attn_weights, c_t_KV) # (4)\no   = einsum('bhqc,hdc-&gt;bhqd', o_, W_UV)  # (5)\nu   = einsum('hdD,bhqd-&gt;bqD', W_o, o)     # (6)\n</code></pre> <p>\u5982\u6b64\u901a\u8fc7\u8fd9\u4e2a\u77e9\u9635\u5438\u6536\uff0c\u6211\u4eec\u53ef\u4ee5\u76f4\u63a5\u628a \\(c_t^{KV}\\) \u770b\u4f5c\u662f V \u8fdb\u884c Attention \u8ba1\u7b97\u3002 \u8fd9\u6837\uff0c\u6211\u4eec\u7684 KV Cache \u53ea\u9700\u5b58\u50a8 \\(c_t^{KV}\\) \u548c \\(k_pe\\)\u3002 \\(K\\) \u7531 \\(c_t^{KV}\\) \u548c \\(k_pe\\) \u62fc\u63a5\u800c\u6210\u3002 \\(V\\) \u5373\u4e3a \\(c_t^{KV}\\)\u3002</p> <p>https://github.com/flashinfer-ai/flashinfer/pull/551</p> <p>https://zhuanlan.zhihu.com/p/700214123</p>"},{"location":"notes/mla/#rope","title":"RoPE","text":"<p>RoPE \u4f5c\u7528\u5728 \\(c^{KV}\\) \u548c \\(c^Q\\) \u4e0a\uff0c\u4f7f\u5f97 \\(W^{UK}\\) \u548c \\(W_{UQ}\\) \u4e0d\u80fd\u518d\u88ab\u5438\u6536\u3002 MLA \u7684\u65b9\u6848\u662f\u628a K \u5207\u6210\u4e24\u90e8\u5206\uff0c <code>pass</code> \u90e8\u5206\u4e0d\u7ecf\u8fc7 RoPE\uff0c\u4f7f\u4e24\u4e2a\u77e9\u9635\u80fd\u591f\u88ab\u5438\u6536\uff1b\u8ba9 <code>rot</code> \u90e8\u5206\u4e0d\u53c2\u4e0e\u77e9\u9635\u4e58\uff0c\u7ecf\u8fc7 RoPE \u540e\u76f4\u63a5\u4e0e <code>pass</code> \u90e8\u5206 concat\u3002</p>"},{"location":"notes/mla/#mla-in-vllm","title":"MLA in vLLM","text":"<pre><code>class MultiHeadLatentAttention(CustomOp):\n    ...\n\n    def forward_native(\n        self,\n        positions: torch.Tensor,\n        hidden_states: torch.Tensor,\n    ) -&gt; torch.Tensor:\n        q_c = None\n        kv_lora = None\n\n        if self.q_lora_rank is not None:\n            assert self.fused_qkv_a_proj is not None, \\\n                \"fused_qkv_a_proj is required when q_lora_rank is not None\"\n            assert self.q_a_layernorm is not None, \\\n                \"q_a_layernorm is required when q_lora_rank is not None\"\n            assert self.q_b_proj is not None, \\\n                \"q_b_proj is required when q_lora_rank is not None\"\n            qkv_lora = self.fused_qkv_a_proj(hidden_states)[0]\n            q_c, kv_lora = qkv_lora.split(\n                [self.q_lora_rank, self.kv_lora_rank + self.qk_rope_head_dim],\n                dim=-1,\n            )\n            q_c = self.q_a_layernorm(q_c)\n            q = self.q_b_proj(q_c)[0]\n        else:\n            assert self.kv_a_proj_with_mqa is not None, \\\n                \"kv_a_proj_with_mqa is required when q_lora_rank is None\"\n            assert self.q_proj is not None, \\\n                \"q_proj is required when q_lora_rank is None\"\n            kv_lora = self.kv_a_proj_with_mqa(hidden_states)[0]\n            q = self.q_proj(hidden_states)[0]\n\n        kv_c, k_pe = kv_lora.split([self.kv_lora_rank, self.qk_rope_head_dim],\n                                   dim=-1)\n        kv_c_normed = self.kv_a_layernorm(kv_c)\n\n        q = q.view(-1, self.num_heads, self.qk_head_dim)\n        # Add head dim of 1 to k_pe\n        k_pe = k_pe.unsqueeze(1)\n\n        q[..., self.qk_nope_head_dim:], k_pe = self.rotary_emb(\n            positions, q[..., self.qk_nope_head_dim:], k_pe)\n\n        attn_out = self.mla_attn(\n            q,\n            kv_c_normed,\n            k_pe,\n            output_shape=(hidden_states.shape[0],\n                          self.num_heads * self.v_head_dim))\n        return self.o_proj(attn_out)[0]\n</code></pre> <p>vLLM \u628a <code>q_a_proj</code> \u548c <code>kv_a_proj_with_mqa</code> \u4e24\u4e2a\u5bf9 <code>hidden_states</code> \u7684\u77e9\u9635\u4e58\u878d\u5408\u6210 <code>fused_qkv_a_proj</code>\u3002 \u5b9e\u73b0\u77e9\u9635\u5438\u6536\uff0c\u628a <code>kv_c_norm</code> \u4f20\u8fdb <code>mla_attn</code>\u3002 KV cache \u5b58\u653e\u7684\u662f <code>kv_c_normed</code> \u548c <code>k_pe</code>\u3002</p> <p>\u8c03\u7528\u94fe\uff1a</p> <pre><code>DeepseekV2MLAAttention.mla_attn = MultiHeadLatentAttention(...)\n-&gt;\nMultiHeadLatentAttention.mla_attn = Attention(..., use_mla=True, ...)\n-&gt;\nselector.py dispatch attention backend\n</code></pre> <p>\u5728 vLLM v1 \u4e2d\u6709 CutlassMLA, FlashattnMLA, FlashinferMLA, FlashMLA, TritonMLA \u540e\u7aef\uff08for CUDA\uff09\uff0c\u800c v0 \u53ea\u652f\u6301 FlashMLA \u548c Triton \u540e\u7aef\u3002 vLLM \u5b9a\u4e49\u4e86\u4e00\u4e2a\u901a\u7528\u63a5\u53e3 <code>MLACommonImpl</code>\uff0c\u5b9e\u73b0\u4e86 <code>forward</code> \u548c <code>_forward_prefill</code> \u7b49\u65b9\u6cd5\uff0c\u4f46\u662f\u6ca1\u6709\u5b9e\u73b0 <code>_forward_decode</code>\u3002 \u5404\u4e2a\u540e\u7aef\u7684 MLA \u5b9e\u73b0\u90fd\u7ee7\u627f <code>MLACommonImpl</code>\uff0c\u5404\u81ea\u5b9e\u73b0 <code>_forward_decode</code>\u3002</p> <p>\u6211\u4eec\u5173\u6ce8 <code>TritonMLAImpl._forward_decode</code> \u5b9e\u73b0\uff0c\u5176\u8c03\u7528\u4e86 <code>decode_attention_fwd</code>\u3002 \u7531\u4e8e\u77e9\u9635\u5438\u6536\u540e\u7b49\u4ef7\u4e8e MQA\uff0c\u63a5\u7740\u8c03\u7528 <code>decode_attention_fwd_grouped</code>\uff1a</p> <ol> <li><code>_decode_att_m_fwd</code></li> <li><code>_decode_softmax_reducev_fwd</code></li> </ol>"},{"location":"notes/nano-vllm/","title":"Nano-vLLM","text":"<p>https://github.com/GeeeekExplorer/nano-vllm</p>"},{"location":"notes/nano-vllm/#llm","title":"LLM","text":"<p><pre><code>class LLM(LLMEngine):\n    pass\n</code></pre> \u8fd9\u91cc LLM \u7c7b\u76f4\u63a5\u7a7f\u900f\u5230 LLMEngine \u7c7b\u3002 vLLM \u4e2d\u540c\u65f6\u6709\u540c\u6b65\u7684 LLM \u7c7b\u548c\u4e3a\u5f02\u6b65\u7684 AsyncLLMEngine \u7c7b\uff0c\u5b9e\u73b0\u66f4\u590d\u6742\u3002</p>"},{"location":"notes/nano-vllm/#llmengine","title":"LLMEngine","text":"<pre><code>class LLMEngine:\n\n    def __init__(self, model, **kwargs):\n        config_fields = {field.name for field in fields(Config)}\n        config_kwargs = {k: v for k, v in kwargs.items() if k in config_fields}\n        config = Config(model, **config_kwargs)\n        self.ps = []\n        self.events = []\n        ctx = mp.get_context(\"spawn\")\n        for i in range(1, config.tensor_parallel_size):\n            event = ctx.Event()\n            process = ctx.Process(target=ModelRunner, args=(config, i, event))\n            process.start()\n            self.ps.append(process)\n            self.events.append(event)\n        self.model_runner = ModelRunner(config, 0, self.events)\n        self.tokenizer = AutoTokenizer.from_pretrained(config.model, use_fast=True)\n        config.eos = self.tokenizer.eos_token_id\n        self.scheduler = Scheduler(config)\n        atexit.register(self.exit)\n</code></pre> <p>LLMEngine \u4e2d\u5305\u62ec\u4e86 model_runner, tokenizer, scheduler\uff0c\u8fd9\u4e09\u4e2a\u7c7b\u4e4b\u540e\u4f1a\u5c55\u5f00\u5206\u6790\u3002 \u6211\u4eec\u5148\u770b LLMEngine \u5bf9\u5916\u66b4\u9732\u7684\u63a5\u53e3 <code>LLMEngine.generate()</code>\u3002</p> <pre><code>def add_request(self, prompt: str | list[int], sampling_params: SamplingParams):\n    if isinstance(prompt, str):\n        prompt = self.tokenizer.encode(prompt)\n    seq = Sequence(prompt, sampling_params)\n    self.scheduler.add(seq)\n\ndef step(self):\n    seqs, is_prefill = self.scheduler.schedule()\n    token_ids = self.model_runner.call(\"run\", seqs, is_prefill)\n    self.scheduler.postprocess(seqs, token_ids)\n    outputs = [(seq.seq_id, seq.completion_token_ids) for seq in seqs if seq.is_finished]\n    num_tokens = sum(len(seq) for seq in seqs) if is_prefill else -len(seqs)\n    return outputs, num_tokens\n\ndef is_finished(self):\n    return self.scheduler.is_finished()\n\ndef generate(\n    self,\n    prompts: list[str] | list[list[int]],\n    sampling_params: SamplingParams | list[SamplingParams],\n    use_tqdm: bool = True,\n) -&gt; list[str]:\n    if use_tqdm:\n        pbar = tqdm(total=len(prompts), desc=\"Generating\", dynamic_ncols=True)\n    if not isinstance(sampling_params, list):\n        sampling_params = [sampling_params] * len(prompts)\n    for prompt, sp in zip(prompts, sampling_params):\n        self.add_request(prompt, sp)\n    outputs = {}\n    prefill_throughput = decode_throughput = 0.\n    while not self.is_finished():\n        t = perf_counter()\n        output, num_tokens = self.step()\n        if use_tqdm:\n            if num_tokens &gt; 0:\n                prefill_throughput = num_tokens / (perf_counter() - t)\n            else:\n                decode_throughput = -num_tokens / (perf_counter() - t)\n            pbar.set_postfix({\n                \"Prefill\": f\"{int(prefill_throughput)}tok/s\",\n                \"Decode\": f\"{int(decode_throughput)}tok/s\",\n            })\n        for seq_id, token_ids in output:\n            outputs[seq_id] = token_ids\n            if use_tqdm:\n                pbar.update(1)\n    outputs = [outputs[seq_id] for seq_id in sorted(outputs.keys())]\n    outputs = [{\"text\": self.tokenizer.decode(token_ids), \"token_ids\": token_ids} for token_ids in outputs]\n    if use_tqdm:\n        pbar.close()\n    return outputs\n</code></pre> <p>\u9996\u5148\uff0c<code>add_request</code> \u65b9\u6cd5\u4f1a\u5c06 prompt \u4f9d\u6b21\u52a0\u5165 scheduler\u3002 \u968f\u540e\u4e0d\u65ad\u8c03\u7528 <code>step</code> \u65b9\u6cd5\uff0c\u76f4\u5230 scheduler \u4e2d\u6ca1\u6709\u5c1a\u672a\u5b8c\u6210\u7684 sequence\u3002 \u5728 <code>step</code> \u65b9\u6cd5\u4e2d\uff0cscheduler \u51b3\u5b9a\u5f53\u524d\u8fdb\u884c prefill \u8fd8\u662f decode\uff0c\u5e76\u8fd4\u56de\u9700\u8981\u5904\u7406\u7684 sequence\uff0c\u4ea4\u7ed9 model_runner \u5904\u7406\u3002</p>"},{"location":"notes/nano-vllm/#scheduler","title":"Scheduler","text":"<p>\u5728\u6df1\u5165 Scheduler \u4e4b\u524d\uff0c\u6211\u4eec\u5148\u4ecb\u7ecd\u4e00\u4e0b Block \u7c7b\u548c BlockManager \u7c7b\u3002</p>"},{"location":"notes/nano-vllm/#block","title":"Block","text":"<pre><code>class Block:\n\n    def __init__(self, block_id):\n        self.block_id = block_id\n        self.ref_count = 0\n        self.hash = -1\n        self.token_ids = []\n\n    def update(self, hash: int, token_ids: list[int]):\n        self.hash = hash\n        self.token_ids = token_ids\n\n    def reset(self):\n        self.ref_count = 1\n        self.hash = -1\n        self.token_ids = []\n</code></pre> <p>Block \u7c7b\u5728\u521b\u5efa\u65f6\u65f6\u62ff\u5230\u4e00\u4e2a block_id\uff0c\u521d\u59cb\u5316\u5f15\u7528\u8ba1\u6570\uff0c\u54c8\u5e0c\uff0c\u548c\u5b58\u653e\u7684token Id\u3002 update \u65b9\u6cd5\u66f4\u65b0\u54c8\u5e0c\u503c\u548ctoken Id\u3002 reset \u65b9\u6cd5\u5c06\u5f15\u7528\u8ba1\u6570\u8bbe\u62101\uff0c\u54c8\u5e0c\u548c token Id \u90fd\u53d8\u4e3a\u521d\u59cb\u72b6\u6001\u3002</p>"},{"location":"notes/nano-vllm/#blockmanager","title":"BlockManager","text":"<pre><code>class BlockManager:\n\n    def __init__(self, num_blocks: int, block_size: int):\n        self.block_size = block_size\n        self.blocks: list[Block] = [Block(i) for i in range(num_blocks)]\n        self.hash_to_block_id: dict[int, int] = dict()\n        self.free_block_ids: deque[int] = deque(range(num_blocks))\n        self.used_block_ids: set[int] = set()\n</code></pre>"},{"location":"notes/nano-vllm/#allocate","title":"Allocate","text":"<pre><code>def allocate(self, seq: Sequence):\n    assert not seq.block_table\n    h = -1\n    cache_miss = False\n    for i in range(seq.num_blocks):\n        token_ids = seq.block(i)\n        h = self.compute_hash(token_ids, h) if len(token_ids) == self.block_size else -1\n        block_id = self.hash_to_block_id.get(h, -1)\n        if block_id == -1 or self.blocks[block_id].token_ids != token_ids:\n            cache_miss = True\n        if cache_miss:\n            block_id = self.free_block_ids[0]\n            block = self._allocate_block(block_id)\n        else:\n            seq.num_cached_tokens += self.block_size\n            if block_id in self.used_block_ids:\n                block = self.blocks[block_id]\n                block.ref_count += 1\n            else:\n                block = self._allocate_block(block_id)\n        if h != -1:\n            block.update(h, token_ids)\n            self.hash_to_block_id[h] = block_id\n        seq.block_table.append(block_id)\n</code></pre>"},{"location":"notes/papers/","title":"Papers","text":"<p>Helix Parallelism: Rethinking Sharding Strategies for Interactive Multi-Million-Token LLM Decoding</p> <p>Decode Context Parallelism in vLLM. </p> <p>\u6bcf\u4e2a DCP rank \u8fde\u7eed\u5b58\u653e\u82e5\u5e72\u4e2a\uff0816\uff09tokens \u7684 KV cache\u3002\u4e5f\u5c31\u662f\u8bf4\uff0c\u524d 16 \u4e2a decode steps \u628a\u65b0\u589e\u7684 KV cache \u90fd\u5b58\u5728 DCP rank 0 \u4e0a\uff0c\u540e 16 \u4e2a decode steps \u628a\u65b0\u589e\u7684 KV cache \u90fd\u5b58\u5728 DCP rank 1 \u4e0a\u3002</p> <p>https://arxiv.org/html/2507.07120v1 https://docs.vllm.ai/en/latest/serving/context_parallel_deployment.html#decode-context-parallel</p>"},{"location":"notes/quantization/","title":"Quantization","text":""},{"location":"notes/quantization/#data-formats","title":"Data Formats","text":""},{"location":"notes/quantization/#tensor-cores-supported-data-formats","title":"Tensor Cores Supported Data Formats","text":"Architecture Supported Formats Blackwell FP64, TF32, BF16, FP16, FP8, INT8, FP6, FP4 Hopper FP64, TF32, BF16, FP16, FP8, INT8 Ada FP64, TF32, BF16, FP16, FP8 Ampere FP64, TF32, BF16, FP16"},{"location":"notes/quantization/#low-bit-data-formats","title":"Low-bit Data Formats","text":"Format Description FP4 (E2M1) 4-bit floating point with 2 exponent bits and 1 mantissa bit. Values: 0, 0.5, 1, 1.5, 2, 3, 4, 6. MXFP8 FP8_E4M3 or FP8_E5M2. 1 FP8_E8M0 scale per 32 elements. MXFP6 FP6_E3M2 or FP6_E2M3. 1 FP8_E8M0 scale per 32 elements. MXFP4 FP4_E2M1. 1 FP8_E8M0 scale per 32 elements. NVFP4 FP4_E2M1. 1 FP8_E4M3 scale per 16 elements. 1 FP32 scale per tensor."},{"location":"notes/quantization/#quantization_1","title":"Quantization","text":"<p>Refer to (vLLM docs)[https://docs.vllm.ai/en/stable/features/quantization] and codebase.</p>"},{"location":"notes/quantization/#fp8-w8a8","title":"FP8 W8A8","text":"<p>Supported on Ada and forward for FP8 tensor cores.</p> <p>Targeting on all Linear layers:</p> <ul> <li>Weights: static, per-tensor / per-channel / per-block scaling.</li> <li>Activations: dynamic, per-token / per-block scaling.</li> </ul> <p>\u4ee5 per-block weight scaling \u4e3a\u4f8b\uff1aw13 \u548c w2 \u5728\u6bcf\u4e2a [block_n, block_k] \u5927\u5c0f\u7684 tile \u5171\u4eab\u4e00\u4e2a scale\uff0c\u8fd9\u65f6 hidden_states \u53ea\u80fd\u91c7\u7528 per-block \u7684 scaling\uff0c\u5728\u6bcf\u4e2a [1, block_k] \u5927\u5c0f\u7684 tile \u4e0a\u5171\u4eab\u4e00\u4e2a scale\u3002</p>"},{"location":"notes/rmsnorm/","title":"RMSNorm","text":"<p>\u8fd1\u65e5\u88ab\u95ee\u53ca RMSNorm\uff0c\u590d\u76d8\u65f6\u53d1\u89c9\u6709\u4e9b\u7ec6\u8282\u8fd8\u6709\u5f85\u6539\u8fdb\u3002Flashinfer \u4e2d\u6709\u5b9e\u73b0 RMSNorm \u8fd9\u4e00\u7b97\u5b50\uff0c\u73b0\u5728\u5b66\u4e60\u4e00\u4e0b\u3002</p>"},{"location":"notes/rmsnorm/#cpp","title":"CPP","text":"<p><code>norm.cu</code> \u4e2d\u5b9e\u73b0\u4e86\u57fa\u4e8e TensorView \u7684\u63a5\u53e3\uff0c\u63d0\u4f9b\u5bf9\u63a5\u5176\u4ed6\u6846\u67b6\uff08pytorch\uff09\u7684 binding\u3002</p> <p>rmsnorm \u652f\u6301\u4e8c\u7ef4\u548c\u4e09\u7ef4\u7684\u8f93\u5165\uff0c\u4e8c\u7ef4\u4f1a\u8c03\u7528 norm::RMSNorm\uff0c\u4e09\u7ef4\u4f1a\u8c03\u7528 norm::QKRMSNorm\u3002 \u8fd9\u91cc\u6211\u4eec\u5173\u6ce8\u4e8c\u7ef4\u7684 norm::RMSNorm\u3002</p> <pre><code>// norm.cu\nvoid rmsnorm(TensorView output, TensorView input, TensorView weight, double eps, bool enable_pdl) {\n  CHECK_LAST_DIM_CONTIGUOUS_INPUT(input);\n  CHECK_LAST_DIM_CONTIGUOUS_INPUT(output);\n  CHECK_LAST_DIM_CONTIGUOUS_INPUT(weight);\n  CHECK_DEVICE(input, weight);\n  CHECK_DIM(1, weight);  // weight: (hidden_size)\n\n  auto input_ndim = input.ndim();\n  if (input_ndim == 2) {\n    // Normal RMSNorm: [batch_size, hidden_size]\n    // Use CTA parallelization for better parallelism\n    CHECK_DIM(2, output);\n    TVM_FFI_ICHECK_EQ(input.size(1), weight.size(0));\n    unsigned int batch_size = input.size(0);\n    unsigned int hidden_size = input.size(1);\n    TVM_FFI_ICHECK_EQ(output.size(0), batch_size);\n    TVM_FFI_ICHECK_EQ(output.size(1), hidden_size);\n    ffi::CUDADeviceGuard device_guard(input.device().device_id);\n    const cudaStream_t stream = get_stream(input.device());\n\n    DISPATCH_DLPACK_DTYPE_TO_CTYPE_FP16(input.dtype(), c_type, [&amp;] {\n      cudaError_t status = norm::RMSNorm(\n          static_cast&lt;c_type*&gt;(input.data_ptr()), static_cast&lt;c_type*&gt;(weight.data_ptr()),\n          static_cast&lt;c_type*&gt;(output.data_ptr()), batch_size, hidden_size, input.stride(0),\n          output.stride(0), eps, enable_pdl, stream);\n      TVM_FFI_ICHECK(status == cudaSuccess)\n          &lt;&lt; \"RMSNorm failed with error code \" &lt;&lt; cudaGetErrorString(status);\n      return true;\n    });\n  } else if (input_ndim == 3) {\n    // QK RMSNorm: [batch_size, num_heads, head_dim]\n    // Use warp-level parallization\n    CHECK_DIM(3, output);  // output: (batch_size, num_heads, hidden_size)\n    TVM_FFI_ICHECK_EQ(input.size(2), weight.size(0));\n    unsigned int batch_size = input.size(0);\n    unsigned int num_heads = input.size(1);\n    unsigned int hidden_size = input.size(2);\n    TVM_FFI_ICHECK_EQ(output.size(0), batch_size);\n    TVM_FFI_ICHECK_EQ(output.size(1), num_heads);\n    TVM_FFI_ICHECK_EQ(output.size(2), hidden_size);\n\n    ffi::CUDADeviceGuard device_guard(input.device().device_id);\n    const cudaStream_t stream = get_stream(input.device());\n    DISPATCH_DLPACK_DTYPE_TO_CTYPE_FP16(input.dtype(), c_type, [&amp;] {\n      cudaError_t status = norm::QKRMSNorm(\n          static_cast&lt;c_type*&gt;(input.data_ptr()), static_cast&lt;c_type*&gt;(weight.data_ptr()),\n          static_cast&lt;c_type*&gt;(output.data_ptr()), batch_size, num_heads, hidden_size,\n          input.stride(0), input.stride(1), output.stride(0), output.stride(1), eps, enable_pdl,\n          stream);\n      TVM_FFI_ICHECK(status == cudaSuccess)\n          &lt;&lt; \"QKRMSNorm failed with error code \" &lt;&lt; cudaGetErrorString(status);\n      return true;\n    });\n  } else {\n    TVM_FFI_ICHECK(false) &lt;&lt; \"Unsupported input dimension: \" &lt;&lt; input_ndim;\n  }\n}\n</code></pre>"},{"location":"notes/rmsnorm/#host","title":"Host","text":"<p>RMSNorm \u5148\u8ba1\u7b97\u5411\u91cf\u5316\u8bbf\u5b58\u7684\u5411\u91cf\u957f\u5ea6\u3002\u6700\u957f\u662f 16 \u5b57\u8282\uff0c\u4f46\u8981\u786e\u4fdd\u548c d \u957f\u5ea6\u5bf9\u9f50\uff0c\u6240\u4ee5\u548c d \u53d6\u6700\u5927\u516c\u7ea6\u6570\u3002</p> <p>\u4ee4\u4eba\u611f\u53f9\uff0c\u624b\u6495\u65f6\u88ab\u95ee\u5230\u6709\u4f55\u4f18\u5316\u7a7a\u95f4\u65f6\uff0c\u7adf\u7136\u5fd8\u8bb0\u4e86\u5411\u91cf\u5316\u8bbf\u5b58\u3002\u590d\u76d8\u540e\uff0c\u6700\u7406\u60f3\u7684\u56de\u7b54\u5e94\u8be5\u662f\uff0c\u5148\u5224\u65ad\u8fd9\u662f\u4e00\u4e2a memory bound \u7684\u7b97\u5b50\uff0c\u6240\u4ee5\u4ece\u4f18\u5316\u8bbf\u5b58\u7684\u89d2\u5ea6\u4e0a\uff0c\u8054\u60f3\u5230\u5411\u91cf\u5316\u8bbf\u5b58\uff0c\u51cf\u5c11\u8bbf\u5b58\u7684\u603b\u6307\u4ee4\u6570\u3002</p> <p>\u6bcf\u4e2a\u7ebf\u7a0b\u5904\u7406\u6700\u957f 16 \u5b57\u8282\u7684\u5411\u91cf\uff0c\u5c31\u80fd\u8ba1\u7b97\u51fa block_size \u548c num_warps\u3002 \u8fd9\u91cc\uff0claunch kernel \u6240\u7528\u7684 block_size \u662f\u4e24\u7ef4\u7684 (32, num_warps)\uff0c\u65b9\u4fbf kernel \u5185\u505a warp-level reduce\u3002</p> <pre><code>// norm.cuh\ntemplate &lt;typename T&gt;\ncudaError_t RMSNorm(T* input, T* weight, T* output, uint32_t batch_size, uint32_t d,\n                    uint32_t stride_input, uint32_t stride_output, float eps = 1e-5,\n                    bool enable_pdl = false, cudaStream_t stream = 0) {\n  const uint32_t vec_size = std::gcd(16 / sizeof(T), d);\n\n  const uint32_t block_size = std::min&lt;uint32_t&gt;(1024, d / vec_size);\n  const uint32_t num_warps = ceil_div(block_size, 32);\n  dim3 nblks(batch_size);\n  dim3 nthrs(32, num_warps);\n  const uint32_t smem_size = num_warps * sizeof(float);\n  float weight_bias = 0.f;\n  void* args[] = {&amp;input, &amp;weight, &amp;output, &amp;d, &amp;stride_input, &amp;stride_output, &amp;weight_bias, &amp;eps};\n\n  cudaLaunchConfig_t config;\n  config.gridDim = nblks;\n  config.blockDim = nthrs;\n  config.dynamicSmemBytes = smem_size;\n  config.stream = stream;\n  cudaLaunchAttribute attrs[1];\n  attrs[0].id = cudaLaunchAttributeProgrammaticStreamSerialization;\n  attrs[0].val.programmaticStreamSerializationAllowed = enable_pdl;\n  config.numAttrs = 1;\n  config.attrs = attrs;\n\n  DISPATCH_ALIGNED_VEC_SIZE(vec_size, VEC_SIZE, {\n    auto kernel = RMSNormKernel&lt;VEC_SIZE, T&gt;;\n    FLASHINFER_CUDA_CALL(\n        cudaFuncSetAttribute(kernel, cudaFuncAttributeMaxDynamicSharedMemorySize, smem_size));\n    FLASHINFER_CUDA_CALL(cudaLaunchKernelEx(&amp;config, kernel, input, weight, output, d, stride_input,\n                                            stride_output, weight_bias, eps));\n  });\n  return cudaSuccess;\n}\n</code></pre>"},{"location":"notes/rmsnorm/#device","title":"Device","text":"<p>Kernel \u5b9e\u73b0\u4e5f\u5f88\u7b80\u5355\uff1a\u6bcf\u4e2a\u7ebf\u7a0b\u7b97\u5404\u81ea\u7684\u5e73\u65b9\u548c\uff0cBlock-level reduce\uff0c\u6700\u540e\u5c31\u662f element-wise \u64cd\u4f5c\u3002 \u6ce8\u610f\u51e0\u70b9\uff1a</p> <ol> <li>\u5411\u91cf\u5316\u8bbf\u5b58\uff1a\u4f7f\u7528\u4e86 <code>vec_t&lt;T, VEC_SIZE&gt;</code> \u7c7b\uff0c\u5c01\u88c5\u4e86 load \u548c store \u64cd\u4f5c\u3002</li> <li>Block-level reduce\uff1a\u5148\u505a warp-level reduce\uff0c\u6700\u540e\u4e00\u4e2a block \u4e2d\u7684\u6bcf\u4e2a warp \u518d\u505a\u4e00\u6b21 reduce\u3002\u663e\u7136\uff0c\u6709 warp_nums \u4e0d\u8d85\u8fc7 32\uff0c\u56e0\u6b64\u7b2c\u4e8c\u6b21 reduce \u4e5f\u80fd\u7528 warp-level reduce \u53bb\u505a\u3002</li> </ol> <p>\u96be\u7ef7\u7684\u662f\uff0c\u6211\u8bf4\u51fa\u4e86\u7b2c\u4e00\u6b21\u8981\u7528 warp-level reduce\uff0c\u4f46\u662f\u7b2c\u4e8c\u6b21\u6ca1\u60f3\u5230\u7528 warp-level reduce\uff0c\u624b\u5199\u4e86\u4e2a\u5728 SMEM \u4e2d\u7684\u8774\u8776\u76f8\u52a0\u3002</p> <pre><code>// norm.cuh\ntemplate &lt;uint32_t VEC_SIZE, typename T&gt;\n__global__ void RMSNormKernel(T* __restrict__ input, T* __restrict__ weight, T* __restrict__ output,\n                              const uint32_t d, const uint32_t stride_input,\n                              const uint32_t stride_output, float weight_bias, float eps) {\n  const uint32_t bx = blockIdx.x;\n  const uint32_t tx = threadIdx.x, ty = threadIdx.y;\n  constexpr uint32_t warp_size = 32;\n  const uint32_t num_warps = blockDim.y;\n  // NOTE(Zihao): it's guaranteed that num_warps should be smaller than 32\n  const uint32_t thread_id = tx + ty * warp_size;\n  const uint32_t num_threads = num_warps * warp_size;\n  const uint32_t rounds = ceil_div(d, VEC_SIZE * num_threads);\n  extern __shared__ float smem[];\n\n  float sum_sq = 0.f;\n\n#if (__CUDACC_VER_MAJOR__ &gt;= 12 &amp;&amp; defined(__CUDA_ARCH__) &amp;&amp; (__CUDA_ARCH__ &gt;= 900))\n  asm volatile(\"griddepcontrol.wait;\");\n#endif\n\n  for (uint32_t i = 0; i &lt; rounds; i++) {\n    vec_t&lt;T, VEC_SIZE&gt; input_vec;\n    input_vec.fill(0.f);\n    if ((i * num_threads + thread_id) * VEC_SIZE &lt; d) {\n      input_vec.load(input + bx * stride_input + i * num_threads * VEC_SIZE + thread_id * VEC_SIZE);\n    }\n#pragma unroll\n    for (uint32_t j = 0; j &lt; VEC_SIZE; j++) {\n      sum_sq += float(input_vec[j]) * float(input_vec[j]);\n    }\n  }\n\n  // first, warp reduce sum\n#pragma unroll\n  for (uint32_t offset = warp_size / 2; offset &gt; 0; offset /= 2) {\n    sum_sq += math::shfl_xor_sync(sum_sq, offset);\n  }\n\n  smem[ty] = sum_sq;\n  __syncthreads();\n  // then, cross warp reduce sum using only the first warp\n  if (ty == 0) {\n    sum_sq = (tx &lt; num_warps) ? smem[tx] : 0.f;\n#pragma unroll\n    for (uint32_t offset = warp_size / 2; offset &gt; 0; offset /= 2) {\n      sum_sq += math::shfl_xor_sync(sum_sq, offset);\n    }\n    smem[0] = sum_sq;\n  }\n  __syncthreads();\n\n  float rms_rcp = math::rsqrt(smem[0] / float(d) + eps);\n\n  for (uint32_t i = 0; i &lt; rounds; i++) {\n    vec_t&lt;T, VEC_SIZE&gt; input_vec;\n    vec_t&lt;T, VEC_SIZE&gt; weight_vec;\n    vec_t&lt;T, VEC_SIZE&gt; output_vec;\n    input_vec.fill(0.f);\n    weight_vec.fill(0.f);\n    if ((i * num_threads + thread_id) * VEC_SIZE &lt; d) {\n      input_vec.load(input + bx * stride_input + i * num_threads * VEC_SIZE + thread_id * VEC_SIZE);\n      weight_vec.load(weight + i * num_threads * VEC_SIZE + thread_id * VEC_SIZE);\n    }\n#pragma unroll\n    for (uint32_t j = 0; j &lt; VEC_SIZE; j++) {\n      output_vec[j] = float(input_vec[j]) * rms_rcp * (weight_bias + float(weight_vec[j]));\n    }\n    if ((i * num_threads + thread_id) * VEC_SIZE &lt; d) {\n      output_vec.store(output + bx * stride_output + i * num_threads * VEC_SIZE +\n                       thread_id * VEC_SIZE);\n    }\n  }\n#if (__CUDACC_VER_MAJOR__ &gt;= 12 &amp;&amp; defined(__CUDA_ARCH__) &amp;&amp; (__CUDA_ARCH__ &gt;= 900))\n  asm volatile(\"griddepcontrol.launch_dependents;\");\n#endif\n}\n</code></pre>"},{"location":"notes/rmsnorm/#epilogue","title":"Epilogue","text":"<p>\u5176\u5b9e\uff0cRMSNorm \u4f5c\u4e3a\u4e00\u4e2a\u7b80\u5355\u7b97\u5b50\uff0c\u9700\u8981\u6ce8\u610f\u7684\u70b9\u5e76\u4e0d\u591a\u3002 \u4f46\u5728\u7d27\u5f20\u523a\u6fc0\u7684\u73af\u5883\u4e2d\uff0c\u4e00\u65f6\u4e5f\u4f1a\u60f3\u4e0d\u8d77\u6765\u3002 \u8fd8\u9700\u591a\u8bfb\u4ee3\u7801\uff0c\u591a\u5b66\u591a\u7ec3\u3002</p>"},{"location":"notes/vllm/","title":"Experiment","text":""},{"location":"notes/vllm/#environment","title":"Environment","text":"<p>4 x NVIDIA L40 GPUs</p> <pre><code>$ nvidia-smi\nMon Nov  3 14:56:35 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 570.133.07             Driver Version: 570.133.07     CUDA Version: 12.8     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  NVIDIA L40                     Off |   00000000:21:00.0 Off |                    0 |\n| N/A   41C    P0             83W /  300W |     448MiB /  46068MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   1  NVIDIA L40                     Off |   00000000:61:00.0 Off |                    0 |\n| N/A   34C    P8             36W /  300W |      17MiB /  46068MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   2  NVIDIA L40                     Off |   00000000:81:00.0 Off |                    0 |\n| N/A   29C    P8             36W /  300W |      17MiB /  46068MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   3  NVIDIA L40                     Off |   00000000:E1:00.0 Off |                    0 |\n| N/A   31C    P8             37W /  300W |      17MiB /  46068MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n</code></pre> <p>Topology information:</p> <pre><code>$ nvidia-smi topo -m\n    GPU0    GPU1    GPU2    GPU3    NIC0    NIC1    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X  NODE    SYS SYS SYS SYS 0-127,256-383   0       N/A\nGPU1    NODE     X  SYS SYS SYS SYS 0-127,256-383   0       N/A\nGPU2    SYS SYS  X  NODE    NODE    NODE    128-255,384-511 1       N/A\nGPU3    SYS SYS NODE     X  NODE    NODE    128-255,384-511 1       N/A\nNIC0    SYS SYS NODE    NODE     X  PIX             \nNIC1    SYS SYS NODE    NODE    PIX  X              \n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0: mlx5_0\n  NIC1: mlx5_1\n</code></pre> <p>GPU0 \u548c GPU1\uff0c\u4ee5\u53ca GPU2 \u548c GPU3 \u4e4b\u95f4\u7684\u901a\u4fe1\u65b9\u5f0f\u662f NODE\uff0c\u9700\u8981\u8de8\u8fc7 CPU cluster \u4f46\u662f\u4e0d\u8de8\u8fc7 NUMA node\u3002 GPU0/1 \u548c GPU2/3 \u4e4b\u95f4\u7684\u901a\u4fe1\u65b9\u5f0f\u662f SYS\uff0c\u9700\u8981\u8de8\u8fc7 NUMA \u8282\u70b9\u95f4\u7684 QPI/UPI \u603b\u7ebf\u3002</p>"},{"location":"notes/vllm/#gpt-oss-120b-mxfp4","title":"gpt-oss-120b mxfp4","text":""},{"location":"notes/vllm/#test-tp2","title":"Test TP2","text":"<p>\u8fd0\u884c server \u811a\u672c\uff1a</p> <pre><code>vllm serve ../models/gpt-oss-120b \\\n    -tp 2\n</code></pre> <p>\u8fd0\u884c client benchmark \u811a\u672c\uff1a</p> <pre><code>vllm bench serve --model ../models/gpt-oss-120b\n</code></pre> <p>\u9ed8\u8ba4\u914d\u7f6e\u4e0b\uff0crequest rate \u4e3a\u65e0\u9650\u5236\uff0cburstiness \u4e3a 1.0\u3002 burstiness \u4e3a 1.0 \u8868\u793a\u8bf7\u6c42\u5230\u8fbe\u670d\u4ece\u6cca\u677e\u5206\u5e03\uff0c\u5426\u5219\u670d\u4ece Gamma \u5206\u5e03\uff0cburstiness \u8d8a\u5927\u5219\u8bf7\u6c42\u8d8a\u5747\u5300\uff0c\u8d8a\u5c0f\u5219\u8bf7\u6c42\u8d8a\u96c6\u4e2d\u3002 \u5171 1000 \u4e2a\u8bf7\u6c42\uff0c\u6bcf\u4e2a\u8bf7\u6c42\u8f93\u5165\u957f\u5ea6 1024\uff0c\u8f93\u51fa\u957f\u5ea6 128\u3002 \u7ed3\u679c\u5982\u4e0b\uff1a</p> <pre><code>./bench_server.sh \nINFO 11-03 16:49:51 [__init__.py:216] Automatically detected platform cuda.\nNamespace(subparser='bench', bench_type='serve', dispatch_function=&lt;function BenchmarkServingSubcommand.cmd at 0x77995f557380&gt;, seed=0, num_prompts=1000, dataset_name='random', no_stream=False, dataset_path=None, no_oversample=False, custom_output_len=256, custom_skip_chat_template=False, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 0}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', endpoint_type=None, base_url=None, host='127.0.0.1', port=8000, endpoint='/v1/completions', header=None, max_concurrency=None, model='../models/gpt-oss-120b', tokenizer=None, use_beam_search=False, logprobs=None, request_rate=inf, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, request_id_prefix='benchmark-serving', top_p=None, top_k=None, min_p=None, temperature=None, tokenizer_mode='auto', served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600)\nINFO 11-03 16:49:55 [datasets.py:507] Sampling input_len from [1024, 1024] and output_len from [128, 128]\nStarting initial single prompt test run...\nWaiting for endpoint to become up in 600 seconds\n |                                                                                                                  | 00:01 elapsed, 140:06:18 remaining\nInitial test run completed. Starting main benchmark run...\nTraffic request rate: inf\nBurstiness factor: 1.0 (Poisson process)\nMaximum request concurrency: None\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [01:32&lt;00:00, 10.79it/s]\ntip: install termplotlib and gnuplot to plot the metrics\n============ Serving Benchmark Result ============\nSuccessful requests:                     1000      \nBenchmark duration (s):                  92.72     \nTotal input tokens:                      1022592   \nTotal generated tokens:                  52897     \nRequest throughput (req/s):              10.79     \nOutput token throughput (tok/s):         570.52    \nPeak output token throughput (tok/s):    1799.00   \nPeak concurrent requests:                1000.00   \nTotal Token throughput (tok/s):          11599.66  \n---------------Time to First Token----------------\nMean TTFT (ms):                          44668.95  \nMedian TTFT (ms):                        44620.76  \nP99 TTFT (ms):                           88297.92  \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          162.63    \nMedian TPOT (ms):                        170.06    \nP99 TPOT (ms):                           172.12    \n---------------Inter-token Latency----------------\nMean ITL (ms):                           156.00    \nMedian ITL (ms):                         170.00    \nP99 ITL (ms):                            173.31    \n==================================================\n</code></pre>"}]}