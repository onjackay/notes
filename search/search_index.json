{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to My Notes","text":"<p>This is a collection of personal notes and documentation organized by topics.</p> <p>\u300c\u4e00\u5e74\u592a\u957f\uff0c\u5341\u4e8c\u4e2a\u6708\u592a\u77ed\u300d</p>"},{"location":"#navigation","title":"Navigation","text":"<p>Use the sidebar to navigate through different topics and notes. Feel free to explore the content using the search functionality.</p>"},{"location":"cpp/crtp/","title":"Curiously Recurring Template Pattern (CRTP)","text":"<p>CRTP\uff1a\u57fa\u7c7b\u6a21\u677f\u4ee5\u6d3e\u751f\u7c7b\u4f5c\u4e3a\u6a21\u677f\u53c2\u6570\uff0c\u4ece\u800c\u5728\u7f16\u8bd1\u671f\u5b9e\u73b0\u9759\u6001\u591a\u6001\uff08static polymorphism\uff09\u3002</p>"},{"location":"cpp/crtp/#_1","title":"\u5b9e\u73b0\u9759\u6001\u591a\u6001","text":"<p>CRTP \u901a\u8fc7\u5728\u57fa\u7c7b\u516c\u5f00\u63a5\u53e3\uff0c\u5e76\u5728\u6d3e\u751f\u7c7b\u4e2d\u5b9e\u73b0\u8be5\u63a5\u53e3\uff0c\u4ece\u800c\u5728\u7f16\u8bd1\u671f\u7ed1\u5b9a\u51fd\u6570\u8c03\u7528\uff0c\u5b9e\u73b0\u9759\u6001\u591a\u6001\uff0c\u907f\u514d\u865a\u51fd\u6570\u7684\u8fd0\u884c\u65f6\u5f00\u9500\u3002</p> <pre><code>template&lt;typename Derived&gt;\nclass Shape {\npublic:\n    float area() const {\n        return static_cast&lt;const Derived*&gt;(this)-&gt;area();\n    }\n};\n\nclass Circle : public Shape&lt;Circle&gt; {\npublic:\n    Circle(float radius) : radius(radius) {}\n    float area() const {\n        return 3.14159f * radius * radius;\n    }\nprivate:\n    float radius;\n};\n\nclass Square : public Shape&lt;Square&gt; {\npublic:\n    Square(float side) : side(side) {}\n    float area() const {\n        return side * side;\n    }\nprivate:\n    float side;\n};\n\n// \u4f7f\u7528\uff1a\nCircle c(5.0f);\nSquare s(4.0f);\nfloat circleArea = c.area(); // 78.5398\nfloat squareArea = s.area(); // 16.0\n</code></pre>"},{"location":"cpp/crtp/#_2","title":"\u81ea\u5b9a\u4e49\u6d3e\u751f\u7c7b\u7684\u884c\u4e3a","text":"<p>CRTP \u53ef\u4ee5\u7ed9\u57fa\u7c7b\u63d0\u4f9b\u901a\u7528\u63a5\u53e3\uff0c\u6d3e\u751f\u7c7b\u5b9e\u73b0\u5177\u4f53\u884c\u4e3a\u3002\u53ef\u4ee5\u628a\u6d3e\u751f\u7c7b\u7684\u516c\u5171\u903b\u8f91\u653e\u5728\u57fa\u7c7b\u4e2d\uff0c\u6d3e\u751f\u7c7b\u53ea\u9700\u5b9e\u73b0\u7279\u5b9a\u884c\u4e3a\uff0c\u51cf\u5c11\u4ee3\u7801\u91cd\u590d\u3002</p> <pre><code>template &lt;typename Derived&gt;\nclass EqualityComparable {\npublic:\n    bool operator==(const Derived&amp; other) const {\n        const Derived&amp; self = static_cast&lt;const Derived&amp;&gt;(*this);\n        return self.is_equal(other);\n    }\n\n    bool operator!=(const Derived&amp; other) const {\n        return !(*this == other);\n    }\n};\n\nclass MyClass : public EqualityComparable&lt;MyClass&gt; {\n    int value;\npublic:\n    MyClass(int v) : value(v) {}\n\n    bool is_equal(const MyClass&amp; other) const {\n        return value == other.value;\n    }\n};\n\n// \u4f7f\u7528\uff1a\nMyClass a(1), b(2);\nif (a == b) { ... } // \u81ea\u52a8\u53ef\u7528\uff01\n</code></pre> <ol> <li>\u5b9e\u73b0\u7f16\u8bd1\u671f\u9759\u6001\u63a5\u53e3\u7ea6\u675f\uff0c\u7c7b\u4f3c C++20 concepts\uff0c\u7528\u4e8e\u8981\u6c42\u6d3e\u751f\u7c7b\u5fc5\u987b\u5b9e\u73b0\u67d0\u4e2a\u51fd\u6570</li> </ol> <pre><code>template &lt;typename Derived&gt;\nclass Drawable {\npublic:\n    void draw() {\n        static_cast&lt;Derived*&gt;(this)-&gt;do_draw();\n    }\n};\n// \u5982\u679c Derived \u6ca1\u6709 do_draw()\uff0c\u7f16\u8bd1\u65f6\u62a5\u9519\n</code></pre> <p>\u5728 C++20 \u540e\u7684\u7248\u672c\u4e2d\uff0c\u53ef\u4ee5\u4f7f\u7528 concepts \u6765\u5b9e\u73b0\u7c7b\u4f3c\u7684\u529f\u80fd\uff1a</p> <pre><code>template &lt;typename T&gt;\nconcept DrawableConcept = requires(T t) {\n    { t.do_draw() };\n};\n\ntemplate &lt;DrawableConcept T&gt;\nclass Drawable {\npublic:\n    void draw() {\n        static_cast&lt;T*&gt;(this)-&gt;do_draw();\n    }\n};\n</code></pre>"},{"location":"cpp/crtp/#_3","title":"\u6ce8\u610f\u4e8b\u9879","text":"<ul> <li>CRTP \u9700\u8981\u6d3e\u751f\u7c7b\u5728\u7f16\u8bd1\u671f\u5df2\u77e5\uff0c\u56e0\u6b64\u4e0d\u80fd\u7528\u4e8e\u8fd0\u884c\u65f6\u591a\u6001\uff1a\u4e0d\u80fd\u4f7f\u7528\u6307\u9488\u6216\u5f15\u7528\u6765\u5b58\u50a8\u57fa\u7c7b\u7c7b\u578b\u3002</li> </ul>"},{"location":"cpp/crtp/#_4","title":"\u7528\u4f8b","text":"<p><code>std::enable_shared_from_this</code> \u662f CRTP \u7684\u4e00\u4e2a\u5e38\u89c1\u7528\u4f8b\uff0c\u5b83\u5141\u8bb8\u7c7b\u5b89\u5168\u5730\u4ece <code>shared_from_this</code> \u83b7\u53d6\u81ea\u8eab\u7684 <code>shared_ptr</code>\u3002</p> <pre><code>class Widget: public std::enable_shared_from_this&lt;Widget&gt; {\npublic:\n\u2026\nvoid process();\n\u2026\n}\n</code></pre> <p>\u7b80\u5316\u7248\u5b9e\u73b0\uff1a</p> <pre><code>template &lt;typename T&gt;\nclass enable_shared_from_this {\nprotected:\n    weak_ptr&lt;T&gt; __weak_this; // \u5185\u90e8\u4fdd\u5b58\u4e00\u4e2a\u5f31\u5f15\u7528\uff0c\u6307\u5411\u7ba1\u7406 this \u7684 shared_ptr\n\npublic:\n    shared_ptr&lt;T&gt; shared_from_this() {\n        return shared_ptr&lt;T&gt;(__weak_this); // \u4ece\u5f31\u5f15\u7528\u5347\u7ea7\u4e3a shared_ptr\n    }\n};\n</code></pre> <p>\u5f53\u5bf9\u8c61\u88ab\u7b2c\u4e00\u4e2a <code>shared_ptr</code> \u6784\u9020\u65f6\uff0c\u6807\u51c6\u5e93\u4f1a\u68c0\u6d4b\u5b83\u662f\u5426\u7ee7\u627f\u81ea<code>enable_shared_from_this&lt;T&gt;</code>\uff0c\u5982\u679c\u662f\uff0c\u5c31\u81ea\u52a8\u8bbe\u7f6e <code>__weak_this</code> \u6307\u5411\u8fd9\u4e2a\u63a7\u5236\u5757\u3002</p>"},{"location":"notes/flash-attention/","title":"From Online Softmax to Flash Attention","text":""},{"location":"notes/flash-attention/#online-softmax","title":"Online Softmax","text":""},{"location":"notes/flash-attention/#3-pass-safe-softmax","title":"3-Pass Safe Softmax","text":"<p>Softmax \u7684\u6807\u51c6\u5b9a\u4e49\u662f $$ y_i = \\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)} $$ \u4f46\u662f\u5f53 \\(x_i\\) \u5f88\u5927\u65f6\uff0c\\(\\exp(x_i)\\) \u4f1a\u6ea2\u51fa\u3002\u4e3a\u4e86\u907f\u514d\u8fd9\u79cd\u60c5\u51b5\uff0c\u901a\u5e38\u4f1a\u5148\u8ba1\u7b97\u8f93\u5165\u7684\u6700\u5927\u503c \\(m = \\max_i (x_i)\\)\uff0c\u7136\u540e\u4f7f\u7528 $$ y_i = \\frac{\\exp(x_i - m)}{\\sum_j \\exp(x_j - m)} $$ \u6765\u8ba1\u7b97 softmax\uff0c\u786e\u4fdd \\(x_i - m \\le 0\\)\uff0c\u4ece\u800c\u907f\u514d\u6ea2\u51fa\u3002 \u6734\u7d20\u7684\u5b9e\u73b0\u8fd9\u79cd\u65b9\u6cd5\u9700\u8981\u904d\u5386\u4e09\u6b21\u8f93\u5165\uff1a</p> <ol> <li>\u8ba1\u7b97\u6700\u5927\u503c \\(m_i = \\max(m_{i-1}, x_i)\\)\u3002</li> <li>\u8ba1\u7b97\u5f52\u4e00\u5316\u56e0\u5b50 \\(d_i = d_{i-1} + \\exp(x_j - m_N)\\)\u3002</li> <li>\u8ba1\u7b97\u6700\u7ec8\u7684 softmax \u8f93\u51fa \\(y_i = \\exp(x_i - m_N) / d_N\\)\u3002</li> </ol>"},{"location":"notes/flash-attention/#2-pass-safe-online-softmax","title":"2-Pass Safe Online Softmax","text":"<p>\u6211\u4eec\u53ef\u4ee5\u628a\u524d\u4e24\u6b21\u904d\u5386\u878d\u5408\u6210\u4e00\u6b21\uff0c\u5728\u4e00\u6b21\u904d\u5386\u4e2d\u540c\u65f6\u66f4\u65b0\u6700\u5927\u503c\u548c\u5f52\u4e00\u5316\u56e0\u5b50\uff0c\u5b9e\u73b0 2-Pass Safe Softmax\uff1a</p> <ol> <li>\u8ba1\u7b97\u6700\u5927\u503c\u548c\u5f52\u4e00\u5316\u56e0\u5b50<ul> <li>\\(m_i = \\max(m_{i-1}, x_i)\\)</li> <li>\\(d_i = d_{i-1} \\cdot \\exp(m_{i-1} - m_i) + \\exp(x_i - m_i)\\)</li> </ul> </li> <li>\u8ba1\u7b97\u6700\u7ec8\u7684 softmax \u8f93\u51fa \\(y_i = \\exp(x_i - m_N) / d_N\\)\u3002</li> </ol> <p>\u628a 3-Pass \u4f18\u5316\u6210 2-Pass \u6709\u4ec0\u4e48\u6536\u76ca\u5417\uff1f\u4ece\u8ba1\u7b97\u91cf\u4e0a\u770b\uff0c2-Pass \u751a\u81f3\u8fd8\u8981\u6bd4 3-Pass \u591a\u4e00\u4e9b\u8ba1\u7b97\u3002\u4f46\u4ece\u5168\u5c40\u5185\u5b58\u8bbf\u95ee\u7684\u89d2\u5ea6\u6765\u770b\uff0c2-Pass \u53ea\u9700\u8981\u904d\u5386\u4e24\u6b21\u8f93\u5165\uff0c\u800c 3-Pass \u9700\u8981\u904d\u5386\u4e09\u6b21\uff0c\u51cf\u5c11\u4e86\u4e00\u6b21\u5185\u5b58\u8bbf\u95ee\u3002\u5bf9\u4e8e softmax \u8fd9\u79cd memory-bound \u7684\u64cd\u4f5c\u6765\u8bf4\uff0c\u51cf\u5c11\u5185\u5b58\u8bbf\u95ee\u5f80\u5f80\u80fd\u5e26\u6765\u66f4\u5927\u7684\u6027\u80fd\u63d0\u5347\u3002</p>"},{"location":"notes/flash-attention/#flashattention-v1","title":"FlashAttention V1","text":""},{"location":"notes/flash-attention/#1-pass-attention","title":"1-Pass Attention","text":"<p>\u5ffd\u7565\u7f29\u653e\u5e38\u6570 \\(\\sqrt{d}\\)\uff0c\u6807\u51c6 Attention \u7684\u8ba1\u7b97\u516c\u5f0f\u662f $$ O = \\text{Softmax}(QK^T)V $$ \u5176\u4e2d \\(Q, K, V\\) \u7684\u5f62\u72b6\u5206\u522b\u662f \\((M, D)\\)\uff0c\\((N, D)\\)\uff0c\\((N, D)\\)\u3002 \u90a3\u4e48\u5728 2-Pass Softmax \u7684\u57fa\u7840\u4e0a\uff0c\u6bcf\u4e00\u884c \\(O_i\\) \u7684\u8ba1\u7b97\u53ef\u4ee5\u5206\u89e3\u6210\u4ee5\u4e0b\u4e24\u6b65\uff1a</p> <ol> <li>\u8ba1\u7b97 \\(QK^T\\) \u7684\u6700\u5927\u503c\u548c\u5f52\u4e00\u5316\u56e0\u5b50<ul> <li>\\(x_j = Q_i K_j^T\\)</li> <li>\\(m_j = \\max(m_{j-1}, x_j)\\)</li> <li>\\(d_j = d_j \\cdot \\exp(m_{j-1} - m_j) + \\exp(x_j - m_j)\\)</li> </ul> </li> <li>\u8ba1\u7b97 \\(O\\)<ul> <li>\\(O_j = O_{j-1} + \\frac{\\exp(x_j - m_D)}{d_N} V_j\\)</li> </ul> </li> </ol> <p>\u867d\u7136 Softmax \u4e0d\u80fd\u88ab\u8fdb\u4e00\u6b65\u4f18\u5316\u6210 1-Pass\uff0c\u4f46\u662f Attention \u53ea\u9700\u8981\u5f97\u5230\u6700\u7ec8\u7684\u8f93\u51fa \\(O\\)\uff0c\u800c\u4e0d\u9700\u8981\u4e2d\u95f4\u7684 Softmax \u7ed3\u679c\u3002\u5b9e\u9645\u4e0a Attention \u662f\u53ef\u4ee5\u88ab\u4f18\u5316\u6210 1-Pass \u7684\u3002 \u5b9a\u4e49 \\(O'_j\\)</p> \\[ O'_j = \\sum_{k=1}^{j} \\frac{\\exp(x_k-m_j)}{d_j} V_k \\] <p>\u53ef\u4ee5\u5f97\u5230\u9012\u63a8\u5f0f\uff1a</p> \\[ O'_j = O'_{j-1} \\cdot \\exp(m_{j-1}-m_j) \\cdot \\frac{d_{j-1}}{d_j} + \\frac{\\exp(x_j - m_j)}{d_j} V_j \\] <p>\u7531\u6b64\uff0c\u6211\u4eec\u53ef\u4ee5\u5728\u4e00\u6b21\u904d\u5386\u4e2d\u540c\u65f6\u8ba1\u7b97 \\(m_j, d_j, O'_j\\)\uff0c\u4ece\u800c\u5b9e\u73b0 1-Pass Attention\uff1a</p> <ol> <li>\u8ba1\u7b97 \\(QK^T\\) \u7684\u6700\u5927\u503c\u3001\u5f52\u4e00\u5316\u56e0\u5b50\u548c\u8f93\u51fa<ul> <li>\\(x_j = Q_i K_j^T\\)</li> <li>\\(m_j = \\max(m_{j-1}, x_j)\\)</li> <li>\\(d_j = d_j \\cdot \\exp(m_{j-1} - m_j) + \\exp(x_j - m_j)\\)</li> <li>\\(O'_j = O'_{j-1} \\cdot \\exp(m_{j-1}-m_j) \\cdot \\frac{d_{j-1}}{d_j} + \\frac{\\exp(x_j - m_j)}{d_j} V_j\\)</li> </ul> </li> </ol>"},{"location":"notes/flash-attention/#tiling","title":"Tiling","text":"<p>\u6211\u4eec\u628a K \u548c V \u5206\u5757\uff0c\u6bcf\u5757\u5927\u5c0f\u4e3a \\((n, D)\\)\uff0c\u518d\u628a Q \u6bcf\u5757\u5206\u4e3a \\((m, D)\\)\u3002 \u6211\u4eec\u5148\u5206\u6790\u5bf9\u4e8e \\(Q_i, K_j, V_j\\) \u5206\u5757\u7684\u8ba1\u7b97\u8fc7\u7a0b\uff1a</p> <ol> <li>\u8ba1\u7b97 \\(QK^T\\)<ul> <li>\\(X = Q_i K_j^T \\in \\R^{(m,n)}\\)</li> </ul> </li> <li>\u5bf9\u6bcf\u884c\u6c42<ul> <li>\\(\\tilde{m_i}[k] = \\max_l(X_{k,l}) \\in \\R^m\\)</li> <li>\\(P_{k,l} = \\exp(X_{k,l} - \\tilde{m_i}) \\in \\R^{(m,n)}\\)</li> <li>\\(d_i[k] = \\sum_l P_{k,l} \\in \\R^m\\)</li> </ul> </li> <li>\u5bf9\u6bcf\u884c\u66f4\u65b0\u6700\u5927\u503c\u548c\u5f52\u4e00\u5316\u56e0\u5b50<ul> <li>\\(m_i[k] = \\max(m'_i[k], \\tilde{m_i}[k])\\)</li> <li>\\(d_i[k] = d'_i[k] \\cdot \\exp(m'_i[k] - m_i[k]) + d_i[k] \\cdot \\exp(\\tilde{m_i}[k] - m_i[k])\\)</li> </ul> </li> <li>\u5bf9\u6bcf\u884c\u8ba1\u7b97\u8f93\u51fa<ul> <li>$O_i[k] = d_i^{-1} ((d'_i \\cdot \\exp(m'_i - m_i)) \\cdot O_i[k] + \\exp(\\tilde{m_i} - m_i) \\cdot PV) $</li> </ul> </li> <li>\u7ed9\u4e0b\u4e2a\u5757\u66f4\u65b0\u8fd9\u4e2a\u5757\u7684\u6700\u5927\u503c\u548c\u5f52\u4e00\u5316\u56e0\u5b50<ul> <li>\\(m'_i = m\\)</li> <li>\\(d'_i = d\\)</li> </ul> </li> </ol> <p>FlashAttention V1 \u628a K \u548c V \u7684\u5206\u5757\u904d\u5386\u653e\u5728\u5916\u5c42\u5faa\u73af\uff0c\u628a Q \u7684\u5206\u5757\u904d\u5386\u653e\u5728\u5185\u5c42\u5faa\u73af\u3002</p>"},{"location":"notes/flash-attention/#flashattention-v2","title":"FlashAttention V2","text":"<ol> <li> <p>\u51cf\u5c11 Cuda Core \u8ba1\u7b97</p> <p>\u5728 V1\uff0c\u6bcf\u4e2a tile \u4e2d \\(O_i\\) \u7684\u8ba1\u7b97\u90fd\u5305\u542b\u4e24\u6b21 rescale \u64cd\u4f5c\uff1a\u4e58 \\(d'\\) \u548c\u9664 \\(d\\)\u3002 V2 \u5728\u6bcf\u4e2a tile \u4e2d\u4ec5\u4fdd\u7559\u4e58 \\(d'\\) \u7684 rescale\uff0c\u628a\u6240\u6709\u7684 \\(d\\) \u7684 rescale \u4e00\u8d77\u653e\u5728\u6700\u540e\u3002</p> </li> <li> <p>\u8c03\u6362\u5faa\u73af\u987a\u5e8f</p> <p>V1 \u5148\u5faa\u73af K \u548c V\uff0c\u540e\u5faa\u73af Q \u7684\u505a\u6cd5\u4f7f\u5f97\u6bcf\u6b21\u5185\u5faa\u73af\u90fd\u9700\u8981\u53cd\u590d\u5411 HBM \u8bbf\u5b58 \\(O_i, m'_i, d'_i\\)\u3002 V2 \u8c03\u6362\u4e86\u5185\u5916\u5faa\u73af\u7684\u987a\u5e8f\uff0c\u5148\u5faa\u73af Q\uff0c\u518d\u5faa\u73af K \u548c V\uff0c\u53ea\u5728\u6bcf\u6b21\u5916\u5faa\u73af\u4e2d\u5411 HBM \u8bbf\u5b58 \\(O_i, m'_i, d'_i\\)\u3002</p> </li> <li> <p>\u589e\u52a0 Sequence Length \u7ef4\u5ea6\u5e76\u884c</p> <p>V1 \u53ea\u5728 Batch Size \u548c Head Num \u7ef4\u5ea6\u4e0a\u5e76\u884c\uff0cV2 \u589e\u52a0\u4e86 Sequence Length \u7ef4\u5ea6\u7684\u5e76\u884c</p> </li> </ol>"},{"location":"notes/flash-attention/#flashattention-v3","title":"FlashAttention V3","text":"<p>TODO</p>"},{"location":"notes/flash-attention/#flashattention-vs-sdpa","title":"FlashAttention vs SDPA","text":"<p>FlashAttention \u662f\u5426\u4e00\u5b9a\u6bd4\u6807\u51c6\u7684 SDPA \u5feb\uff1f\u7b54\u6848\u662f\u5426\u5b9a\u7684\u3002 FlashAttention \u5e0c\u671b\u901a\u8fc7\u51cf\u5c11\u5168\u5c40\u5185\u5b58\u8bbf\u95ee\u6765\u63d0\u5347\u6027\u80fd\uff0cFA1 \u8bba\u6587\u4e2d\u5176\u5b9e\u7ed9\u51fa\u4e86 FlashAttention \u548c SDPA \u8bbf\u5b58\u91cf\u7684\u6e10\u8fdb\u5206\u6790\uff1a</p> Method Memory Access SDPA \\(\\Theta(Nd + N^2)\\) FlashAttention \\(\\Theta(N^2 d^2 M^{-1})\\) <p>\u5176\u4e2d \\(N\\) \u662f <code>seq_len</code>\uff0c\\(d\\) \u662f <code>head_dim</code>\uff0c\\(M\\) \u662f GPU \u7684 SRAM \u5927\u5c0f\u3002 \u5bf9\u4e8e\u4e3b\u6d41\u7684\u6a21\u578b\u548c\u63a8\u7406\u6846\u67b6\uff0c<code>seq_len</code> \u4e00\u822c\u5728\u51e0\u5343\u4ee5\u5185\uff088192\uff09\uff0c<code>head_dim</code> \u662f 64/128\uff0cGPU \u7684 SRAM \u5927\u6982\u662f 200kB\uff08A100: 192kB, H100: 228kB\uff09\u3002 \u8fd9\u65f6 \\(d^2 M^{-1}\\) \u662f\u8fdc\u5c0f\u4e8e 1 \u7684\uff0c\u6240\u4ee5 FlashAttention \u7684\u8bbf\u5b58\u91cf\u8981\u8fdc\u5c0f\u4e8e SDPA\u3002</p> <p>\u4f46\u662f\u5982\u679c <code>head_dim</code> \u6bd4\u8f83\u5927\uff0c\u6bd4\u5982 256 \u6216 512\uff0c\u6216\u8005 SRAM \u6bd4\u8f83\u5c0f\uff0c\u6bd4\u5982\u5728\u65e7\u67b6\u6784\u7684 GPU \u4e0a\u8fd0\u884c\uff0cFlashAttention \u7684\u8bbf\u5b58\u91cf\u53ef\u80fd\u4f1a\u5927\u4e8e SDPA\u3002</p>"},{"location":"notes/flash-attention/#flashattention-v4","title":"FlashAttention V4","text":"<p>Tri Dao \u5df2\u7ecf\u9884\u544a FlashAttention V4 \u4e86\uff0c\u4f46\u8fd8\u672a\u6b63\u5f0f\u53d1\u5e03\u3002</p>"},{"location":"notes/mla/","title":"Multi-head Latent Attention (MLA)","text":""},{"location":"notes/mla/#transformers","title":"Transformers","text":""},{"location":"notes/mla/#deepseekv3-configuration","title":"DeepSeekV3 Configuration","text":"<pre><code>hidden_size = 7168\nq_lora_rank = 1536\nnum_heads   = 128\nqk_nope_head_dim = 128\nqk_rope_head_dim = 64\nqk_head_dim = qk_nope_head_dim + qk_rope_head_dim = 192\nv_head_dim = 128\n</code></pre>"},{"location":"notes/mla/#deepseekv3-attention","title":"DeepSeekV3 Attention","text":"<pre><code>query_shape = (batch_size, seq_length, -1, self.qk_head_dim)\nkey_shape = (batch_size, seq_length, -1, self.qk_nope_head_dim + self.v_head_dim)\n\nif self.q_lora_rank is None:\n    q_states = self.q_proj(hidden_states)  # [b, s, h * qk_head_dim]\nelse:\n    q_states = self.q_b_proj(self.q_a_layernorm(self.q_a_proj(hidden_states)))\n        # q_a_proj: [b, s, d] -&gt; [b, s, q_lora_rank]   (16x down_proj)\n        # q_b_proj: [b, s, q_lora_rank] -&gt; [b, s, h * qk_head_dim]  (16x up_proj)\n\nq_states = q_states.view(query_shape).transpose(1, 2)  # [b, h, s, qk_head_dim]\nq_pass, q_rot = torch.split(q_states, [self.qk_nope_head_dim, self.qk_rope_head_dim], dim=-1)\n    # q_pass: [b, h, s, qk_nope_head_dim]\n    # q_rot:  [b, h, s, qk_rope_head_dim]\n\ncompressed_kv = self.kv_a_proj_with_mqa(hidden_states)  # [b, s, kv_lora_rank + qk_rope_head_dim]\nk_pass, k_rot = torch.split(compressed_kv, [self.kv_lora_rank, self.qk_rope_head_dim], dim=-1)\n    # k_pass: [b, s, kv_lora_rank]\n    # k_rot:  [b, s, qk_rope_head_dim]\n\nk_pass = self.kv_b_proj(self.kv_a_layernorm(k_pass)).view(key_shape).transpose(1, 2)\n    # k_pass: [b, h, s, qk_nope_head_dim + v_head_dim]\nk_pass, value_states = torch.split(k_pass, [self.qk_nope_head_dim, self.v_head_dim], dim=-1)\n    # k_pass:       [b, h, s, qk_nope_head_dim]\n    # value_states: [b, h, s, v_head_dim]\n\nk_rot = k_rot.view(batch_size, 1, seq_length, self.qk_rope_head_dim)  # [b, 1, s, qk_rope_head_dim]\n\ncos, sin = position_embeddings\nif self.config.rope_interleave:  # support using interleaved weights for efficiency\n    q_rot, k_rot = apply_rotary_pos_emb_interleave(q_rot, k_rot, cos, sin)\nelse:\n    q_rot, k_rot = apply_rotary_pos_emb(q_rot, k_rot, cos, sin)\nk_rot = k_rot.expand(*k_pass.shape[:-1], -1)  # [b, h, s, qk_rope_head_dim]\n\nquery_states = torch.cat((q_pass, q_rot), dim=-1)  # [b, h, s, qk_head_dim]\nkey_states = torch.cat((k_pass, k_rot), dim=-1)  # [b, h, s, qk_head_dim]\n\n# Normal attention\nattn_output, attn_weights = attention_interface(\n    self,\n    query_states,\n    key_states,\n    value_states,\n    attention_mask,\n    dropout=0.0 if not self.training else self.attention_dropout,\n    scaling=self.scaling,\n    **kwargs,\n)\n</code></pre> <p>MLA \u7684 KV Cache \u91cc\u5b58\u7684\u662f <code>compressed_kv</code> \u548c <code>k_rot</code>\uff0c\u5927\u5927\u51cf\u5c11\u4e86 KV Cache \u7684\u6240\u9700\u7a7a\u95f4\u3002 \u4ee5 DeepSeekV3 \u4e3a\u4f8b\uff0c\u6bcf\u4e2a layer \u6bcf\u4e2a token \u4ec5\u9700\u4e00\u4e2a\u957f\u5ea6\u4e3a 192 \u7684 Cache\u3002</p>"},{"location":"notes/mla/#_1","title":"\u77e9\u9635\u5438\u6536","text":"<p>\u8fd9\u91cc\u77e9\u9635\u5438\u6536\u5e76\u975e\u76f4\u63a5\u5c06\u4e24\u4e2a\u8fde\u7eed\u7684\u77e9\u9635\u4e58\u5408\u5e76\u6210\u4e00\u4e2a\u77e9\u9635\u4e58\uff08\u8fd9\u6837\u505a\u4e27\u5931\u4e86 LoRA \u7684\u610f\u4e49\uff09\uff0c\u800c\u662f\u4ea4\u6362\u77e9\u9635\u4e58\u7684\u8ba1\u7b97\u987a\u5e8f\u3002</p>"},{"location":"notes/mla/#wuk-w_uq","title":"\u5438\u6536 \\(W^{UK}\\) \u548c \\(W_{UQ}\\)","text":"\\[ q^\\top k = (W^{UQ} c_t^Q)^\\top (W^{UK} c_t^{KV}) = \\left({c^Q}^\\top {W^{UQ}}^\\top W^{UK}\\right) c^{KV} \\] <p>\u5176\u4e2d \\(W^{UQ}\\) \u7684\u5f62\u72b6\u662f <code>[h * qk_head_dim (24576), q_lora_rank (1536)]</code>\uff0c \\(W^{UK}\\) \u7684\u5f62\u72b6\u662f <code>[h * qk_head_dim (24576), kv_lora_rank (512)]</code>\u3002</p> <p>\u77e9\u9635\u5438\u6536\u540e\uff0c\u53ef\u4ee5\u76f4\u63a5\u628a \\(c_t^{KV}\\) \u770b\u4f5c\u662f \\(K\\) \u8fdb\u884c Attention \u8ba1\u7b97\uff0c\u800c \\(c^{KV}\\) \u53c8\u662f\u6bcf\u4e2a\u5934\u5171\u7528\u7684\u3002 \u56e0\u6b64\u539f\u5148\u7684 128 heads 128+64 head_dim \u7684 MHA \u8f6c\u5316\u4e3a\u4e86 128 heads 512+64 head_dim \u7684 MQA\uff0c\u51cf\u5c0f\u4e86\u8bbf\u5b58\u91cf\uff0c\u4f46\u589e\u52a0\u4e86\u8ba1\u7b97\u91cf\u3002</p> <p>\u6211\u4eec\u5728 prefill \u65f6\u4f7f\u7528\u8ba1\u7b97\u5f3a\u5ea6\u8f83\u5c0f\uff0c\u8bbf\u5b58\u91cf\u66f4\u5927\u7684 MHA\u3002\u5728 decode \u65f6\u4f7f\u7528\u8ba1\u7b97\u5f3a\u5ea6\u8f83\u5927\uff0c\u8bbf\u5b58\u91cf\u66f4\u5c0f\u7684 MQA\u3002</p>"},{"location":"notes/mla/#wuv-wo","title":"\u5438\u6536 \\(W^{UV}\\) \u548c \\(W^O\\)","text":"<pre><code>v_t = einsum('hdc,blc-&gt;blhd', W_UV, c_t_KV) # (1)\no   = einsum('bqhl,blhd-&gt;bqhd', attn_weights, v_t)     # (2)\nu   = einsum('hdD,bhqd-&gt;bhD', W_o, o)       # (3)\n\n# \u5c06\u4e0a\u8ff0\u4e09\u5f0f\u5408\u5e76\uff0c\u5f97\u5230\u603b\u7684\u8ba1\u7b97\u8fc7\u7a0b\nu   = einsum('hdc,blc,bqhl,hdD-&gt;bhD', W_UV, c_t_KV, attn_weights, W_o)\n\n# \u5229\u7528\u7ed3\u5408\u5f8b\u6539\u53d8\u8ba1\u7b97\u987a\u5e8f\no_  = einsum('bhql,blc-&gt;bhqc', attn_weights, c_t_KV) # (4)\no   = einsum('bhqc,hdc-&gt;bhqd', o_, W_UV)  # (5)\nu   = einsum('hdD,bhqd-&gt;bqD', W_o, o)     # (6)\n</code></pre> <p>https://github.com/flashinfer-ai/flashinfer/pull/551</p> <p>https://zhuanlan.zhihu.com/p/700214123</p>"},{"location":"notes/mla/#rope","title":"RoPE","text":"<p>RoPE \u4f5c\u7528\u5728 \\(c^{KV}\\) \u548c \\(c^Q\\) \u4e0a\uff0c\u4f7f\u5f97 \\(W^{UK}\\) \u548c \\(W_{UQ}\\) \u4e0d\u80fd\u518d\u88ab\u5438\u6536\u3002 MLA \u7684\u65b9\u6848\u662f\u628a K \u5207\u6210\u4e24\u90e8\u5206\uff0c <code>pass</code> \u90e8\u5206\u4e0d\u7ecf\u8fc7 RoPE\uff0c\u4f7f\u4e24\u4e2a\u77e9\u9635\u80fd\u591f\u88ab\u5438\u6536\uff1b\u8ba9 <code>rot</code> \u90e8\u5206\u4e0d\u53c2\u4e0e\u77e9\u9635\u4e58\uff0c\u7ecf\u8fc7 RoPE \u540e\u76f4\u63a5\u4e0e <code>pass</code> \u90e8\u5206 concat\u3002</p>"},{"location":"notes/mla/#mla-in-vllm","title":"MLA in vLLM","text":"<pre><code>class MultiHeadLatentAttention(CustomOp):\n    ...\n\n    def forward_native(\n        self,\n        positions: torch.Tensor,\n        hidden_states: torch.Tensor,\n    ) -&gt; torch.Tensor:\n        q_c = None\n        kv_lora = None\n\n        if self.q_lora_rank is not None:\n            assert self.fused_qkv_a_proj is not None, \\\n                \"fused_qkv_a_proj is required when q_lora_rank is not None\"\n            assert self.q_a_layernorm is not None, \\\n                \"q_a_layernorm is required when q_lora_rank is not None\"\n            assert self.q_b_proj is not None, \\\n                \"q_b_proj is required when q_lora_rank is not None\"\n            qkv_lora = self.fused_qkv_a_proj(hidden_states)[0]\n            q_c, kv_lora = qkv_lora.split(\n                [self.q_lora_rank, self.kv_lora_rank + self.qk_rope_head_dim],\n                dim=-1,\n            )\n            q_c = self.q_a_layernorm(q_c)\n            q = self.q_b_proj(q_c)[0]\n        else:\n            assert self.kv_a_proj_with_mqa is not None, \\\n                \"kv_a_proj_with_mqa is required when q_lora_rank is None\"\n            assert self.q_proj is not None, \\\n                \"q_proj is required when q_lora_rank is None\"\n            kv_lora = self.kv_a_proj_with_mqa(hidden_states)[0]\n            q = self.q_proj(hidden_states)[0]\n\n        kv_c, k_pe = kv_lora.split([self.kv_lora_rank, self.qk_rope_head_dim],\n                                   dim=-1)\n        kv_c_normed = self.kv_a_layernorm(kv_c)\n\n        q = q.view(-1, self.num_heads, self.qk_head_dim)\n        # Add head dim of 1 to k_pe\n        k_pe = k_pe.unsqueeze(1)\n\n        q[..., self.qk_nope_head_dim:], k_pe = self.rotary_emb(\n            positions, q[..., self.qk_nope_head_dim:], k_pe)\n\n        attn_out = self.mla_attn(\n            q,\n            kv_c_normed,\n            k_pe,\n            output_shape=(hidden_states.shape[0],\n                          self.num_heads * self.v_head_dim))\n        return self.o_proj(attn_out)[0]\n</code></pre> <p>vLLM \u628a <code>q_a_proj</code> \u548c <code>kv_a_proj_with_mqa</code> \u4e24\u4e2a\u5bf9 <code>hidden_states</code> \u7684\u77e9\u9635\u4e58\u878d\u5408\u6210 <code>fused_qkv_a_proj</code>\u3002 \u5b9e\u73b0\u77e9\u9635\u5438\u6536\uff0c\u628a <code>kv_c_norm</code> \u4f20\u8fdb <code>mla_attn</code>\u3002 KV cache \u5b58\u653e\u7684\u662f <code>kv_c_normed</code> \u548c <code>k_pe</code>\u3002</p> <p>\u8c03\u7528\u94fe\uff1a</p> <pre><code>DeepseekV2MLAAttention.mla_attn = MultiHeadLatentAttention(...)\n-&gt;\nMultiHeadLatentAttention.mla_attn = Attention(..., use_mla=True, ...)\n-&gt;\nselector.py dispatch attention backend\n</code></pre> <p>\u5728 vLLM v1 \u4e2d\u6709 CutlassMLA, FlashattnMLA, FlashinferMLA, FlashMLA, TritonMLA \u540e\u7aef\uff08for CUDA\uff09\uff0c\u800c v0 \u53ea\u652f\u6301 FlashMLA \u548c Triton \u540e\u7aef\u3002 vLLM \u5b9a\u4e49\u4e86\u4e00\u4e2a\u901a\u7528\u63a5\u53e3 <code>MLACommonImpl</code>\uff0c\u5b9e\u73b0\u4e86 <code>forward</code> \u548c <code>_forward_prefill</code> \u7b49\u65b9\u6cd5\uff0c\u4f46\u662f\u6ca1\u6709\u5b9e\u73b0 <code>_forward_decode</code>\u3002 \u5404\u4e2a\u540e\u7aef\u7684 MLA \u5b9e\u73b0\u90fd\u7ee7\u627f <code>MLACommonImpl</code>\uff0c\u5404\u81ea\u5b9e\u73b0 <code>_forward_decode</code>\u3002</p>"},{"location":"vllm/1/","title":"Test Title","text":"<p>Test content.</p>"}]}